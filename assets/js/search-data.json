{
  
    
        "post0": {
            "title": "A PyTorch Refresher",
            "content": "PyTorch is a great library for deep learning for many reasons; including but not limited to: . It&#39;s easy to learn and use. | Very well documented with many great tutorials, official and unofficial. | Very popular, with a strong community and officially backed by Facebook. | Neat and ordered API. It&#39;s great for getting used to Python classes. | . It&#39;s been a little while since I dipped my toes into the PyTorch water, so I want to take this opportunity to do a complete refresher covering the basics of PyTorch. Maybe you will find it useful too. If you want to run this notebook too you can click the Colab button, be sure to set the runtime to GPU. There is nothing that revolutionary in this post but we will go from an image dataset to a trained image classification model. . We need three things to do deep learning: . Data - input data and output labels/targets. | A model - something that takes in the inputs and produces a probability or value that can be compared to the targets. | A loss function - something that takes our predictions and the actual targets and tells us how well the model is doing. | Data . As PyTorch is a machine learning library we are going to need some data, something that we can input into the model with corresponding labels that we can attempt to predict. I am going to use some of the functionality of the fastai library to quickly get a dataset we can play with: Imagenette . Imagenette is a much smaller subset of the famous Imagenet dataset and consists of 10 classes of images: . tench | English springer | cassette player | chain saw | church | French horn | garbage truck | gas pump | golf ball | parachute | . Let&#39;s have a look at some of the data. . from fastai.data.external import untar_data, URLs path = untar_data(URLs.IMAGENETTE_160) . This downloads the data and returns a fastai verion of a pathlib object. Here I can call ls on this object to list the files and directories in the path where the data was downloaded. Among these you can see that we get a /train and /val directory. . path.ls() . (#4) [Path(&#39;/root/.fastai/data/imagenette2-160/.DS_Store&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/val&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/noisy_imagenette.csv&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train&#39;)] . Let&#39;s have a look in the /train directory to see what we have. Here we can see there are 11 items in this list, one folder for each class and an extra .DS_Store file (we don&#39;t need this). . train_dir = [p for p in path.ls() if p.name == &#39;train&#39;][0] val_dir = [p for p in path.ls() if p.name == &#39;val&#39;][0] train_dir.ls() . (#11) [Path(&#39;/root/.fastai/data/imagenette2-160/train/n03445777&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03394916&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/.DS_Store&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n02102040&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03000684&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n01440764&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03888257&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03417042&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03425413&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03028079&#39;)...] . Let&#39;s check that assumption that the images of a given class are separated into specific folders and have a peek at some images in these folders. Here I am using another utility function from fastai show_image that just helps with showing images with matplotlib. . from fastai.vision.all import show_image from PIL import Image import matplotlib.pyplot as plt train_folder = train_dir.ls()[1] print(f&#39;Train folder name={train_folder.name}&#39;) first_img, second_img = train_folder.ls()[:2] f, ax = plt.subplots(1, 2) img1 = Image.open(first_img) img2 = Image.open(second_img) _ = show_image(img1, ctx=ax[0]) _ = show_image(img2, ctx=ax[1]) . Train folder name=n03394916 . It certainly looks like the classes are given by the directory name of the folder that contains the images. We want to confirm that the same classes are in the same named folder in the val/. Let&#39;s have a look at that. . val_folder = val_dir.ls()[1] print(f&#39;Folder name={val_folder.name}&#39;) first_img, second_img = val_folder.ls()[:2] f, ax = plt.subplots(1, 2) img1 = Image.open(first_img) img2 = Image.open(second_img) _ = show_image(img1, ctx=ax[0]) _ = show_image(img2, ctx=ax[1]) . Folder name=n03394916 . That&#39;s some fine tootin&#39;. Great it looks like the second folder of both train/ and val/ contain French horns, and both have the same directory name in their respective locations. . Tensors . Right, that&#39;s all well and good but how do we use this data and what are images anyway? Well, digital images can be represented as arrays so if you have a colour image you have a 3-dimensional array that has the shape height x width x channels where red pixel intensity is represented in one channel, green in the next channel and blue in the final channel for RGB images. . Let&#39;s look at this more concretely. Below we import NumPy and transform the loaded img1 from earlier into an array. NumPy knows how to do this with a PIL Image. . import numpy as np arr = np.array(img1) print(&#39;The image array has the shape: &#39;, arr.shape) print(&#39;The top 10x10px in the red channel have the values: n&#39;, arr[:10, :10, 0]) . The image array has the shape: (160, 185, 3) The top 10x10px in the red channel have the values: [[233 233 233 233 233 233 233 233 234 234] [234 234 234 234 234 234 234 234 234 234] [234 234 234 234 234 234 234 234 234 234] [235 235 235 235 235 235 235 235 234 234] [235 235 235 235 235 235 235 235 234 234] [234 234 234 234 234 234 234 234 234 234] [234 234 234 234 234 234 234 234 234 234] [233 233 233 233 233 233 233 233 234 234] [233 234 234 234 234 233 232 231 235 234] [233 234 234 234 234 233 232 231 234 233]] . . As you can see the image is 160px by 213px and has 3 channels. Let&#39;s show this image, and each channel isolated. Here I have created a simple function isolate_channel which sets all pixel values in all channels except the selected one to 0, and plot_img to help with showing these. . def isolate_channel(arr, chan=0): &quot;&quot;&quot;Sets all channels which are not chan to 0 intensity&quot;&quot;&quot; chans = [i for i in range(3) if i != chan] img_copy = arr.copy() img_copy[:, :, chans] = 0 return img_copy def plot_img(arr, ax, title): &quot;&quot;&quot;Plots image with given axes and title&quot;&quot;&quot; ax.imshow(arr) ax.axis(&#39;off&#39;) ax.set_title(title) f, ax = plt.subplots(2, 2) ax = ax.flatten() plot_img(arr, ax=ax[0], title=&#39;Original Image&#39;) for i, title in enumerate([&#39;Red Channel&#39;, &#39;Green Channel&#39;, &#39;Blue Channel&#39;]): plot_img(isolate_channel(arr, i), ax=ax[i+1], title=title) f.set_size_inches(8, 4) plt.tight_layout() . So how does this relate to tensors? Well, tensors are essentially the same thing, if you are familiar with NumPy&#39;s vectors and arrays then you will know how to work with tensors. Tensors tend to be described by rank with the following relation to other data structures: . Rank Structure . 0 | Scalar | . 1 | Vector | . 2 | Matrix | . 3 | 3-Dimensional Array | . Let&#39;s convert our NumPy array to a torch tensor. See how its the same to index into as the NumPy array and has the same size/shape . import torch ten = torch.tensor(arr) print(&#39;Top 10x10px of the red channel: n&#39;, ten[:10, :10, 0]) print(&#39;Tensor shape is: &#39;, ten.size(), &#39; | &#39;, ten.shape) . Top 10x10px of the red channel: tensor([[233, 233, 233, 233, 233, 233, 233, 233, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [235, 235, 235, 235, 235, 235, 235, 235, 234, 234], [235, 235, 235, 235, 235, 235, 235, 235, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [233, 233, 233, 233, 233, 233, 233, 233, 234, 234], [233, 234, 234, 234, 234, 233, 232, 231, 235, 234], [233, 234, 234, 234, 234, 233, 232, 231, 234, 233]], dtype=torch.uint8) Tensor shape is: torch.Size([160, 185, 3]) | torch.Size([160, 185, 3]) . The cool thing about torch tensors is that they are aware of the actions (matrix multiplication, additions, etc) that happen to them and any chain of events subsequently so that gradients can be calculated automatically. Remember the gradient of a function $f$ just tells you what you need to change about $x$ to make $f(x)$ lower (or higher if you do the opposite). . This is not that useful for your input data represented as a tensor; you don&#39;t want to use any gradients to change your input data. But you can store the parameters of your models as tensors, and you do want to change these during training to make your model better. This you can do by using the gradients after every forward pass. See here for a really good overview. . Let&#39;s see this in action. Starting with an easy one, if you are familiar with the power rule you will know that the gradient of the square function $x^2$ is $2x$. Below we have a tensor [1., 2., 3.] which we have asked PyTorch to calculate gradients by using requires_grad=True. We pass this tensor through a simple square function which returns the square of each element in the tensor. . def square(x): return x**2 ten = torch.tensor([1., 2. , 3.], requires_grad=True) print(&#39;Tensor is: &#39;, ten) out = square(ten) print(&#39;The result of squaring the tensor is: &#39;, out) out = out.sum() out.backward() print(&#39; nComputing gradients:&#39;) print(ten.grad) . Tensor is: tensor([1., 2., 3.], requires_grad=True) The result of squaring the tensor is: tensor([1., 4., 9.], grad_fn=&lt;PowBackward0&gt;) Computing gradients: tensor([2., 4., 6.]) . As you can see calling .backward() on the summed output of the square() function automatically calculates the gradients for the tensor ten. . Why is this useful then? Well, generally for a supervised machine learning task we want: . Input data | A model to convert our input data to predictions. The model has parameters that we want to update by computing their gradients after each forward pass | A loss function that takes in the predictions and the target that we want the prediction to match. The lower the value returned the better we are doing. Calling backward on the output of the loss function allows us to get gradients so that the parameters of the model can be modified in a manner that should hopefully make the loss value returned lower on the next pass of the data. | . Let&#39;s make a small contrived example with that setup. We will have some input data, that we will put through a &quot;model&quot; and then compute the loss using the output of that &quot;model&quot;. With that loss, we can compute the gradients and modify the weights of our &quot;model&quot; with them. For this we have the following: . Input data which is always an array of [1, 2, 3, 4] | A model with some randomly initialized parameters. In this case just a tensor of shape 4x1 | A loss function. In this case, we will just use $x^2$ as it is a differentiable function, and we can nicely plot the loss value at a range of $x$ points. In reality, we want a loss function that takes in model outputs and targets. The output of the loss function should be lower when the model is better at predicting the given target and should also be a smooth and differentiable function. | . from matplotlib.animation import FuncAnimation from IPython.display import HTML inp = torch.tensor([1., 2., 3., 4.]) weights = torch.tensor([[0.5945], [0.8308], [0.1952], [0.9057]], requires_grad=True) def loss_func(x): return x**2 def one_forward_pass(inp=inp, weights=weights): out = inp @ weights loss = loss_func(out) loss.backward() weights.data = weights.data - 0.001*weights.grad weights.grad = None return out, weights, loss . def animate(i): o, w, loss = one_forward_pass() scatter.set_offsets([o.item(), loss.item()]) text.set_text(f&#39;Forward Pass {i + 1} nLoss={loss.item():.2f} nWeights={w.data.squeeze()}&#39;) return scatter, anim = FuncAnimation(fig, animate, frames=25, interval=500, blit=True) HTML(anim.to_jshtml()) . &lt;/input&gt; Once Loop Reflect If you follow the red dot you can see after each iteration the parameters are changed by using their gradients and the subsequent loss value is a bit lower. You can see how we are moving down the slope towards the minimum value of the loss which is when the &quot;model&quot; outputs 0. Our input data is staying the same on each iteration but the change in parameters is helping us to get there. This is gradient descent - using gradients to minimize the output of a function. . So we can see how we can use tensors and automatic gradient calculation to optimise functions, so we could just use torch.tensor to get out data into shape for modelling, construct a model, calculate the loss and compute gradients to do some deep learning. You could do that but then you are missing out on a lot of the functionality Pytorch has to offer to make our lives easier. . Datasets . First, up Datasets, and by this, I don&#39;t just mean the images we already have. Pytorch has an abstract Dataset class which you can use to represent your data and get the collection of input and targets of whatever items you wish to train with. Here I am demonstrating this with the images from Imagenette, but it could be text, sound, tabular data; anything really. . Datasets must inherit from the parent class torch.utils.data.Dataset and implement two methods: . __get__ so that we can index into a dataset and the item at the $ith$ index | __len__ so that calling len on a Dataset returns the number of items in it. | . I am going to use some of the transforms from the torchvision library, so it&#39;s worth pointing out here that when working with images that torchvision already has useful functions for creating datasets, so creating a custom Dataset here is unnecessary but is useful for understanding how to implement your own Dataset class if you ever want to. I finding implementing your own Dataset is also a good way to get to know your data. The same is true of torchtext if you are working with text data. . from torch.utils.data import Dataset class ImageDataset: &quot;&quot;&quot;Custom Dataset class for working with images&quot;&quot;&quot; def __init__(self, files, label_func=None, transforms=None): &quot;&quot;&quot;Pass in a list of image files and a way to label them :files (list): list of image file filepaths :label_func (callable): A function to get the label from image name :transforms (callable): Optional transforms to apply to the images &quot;&quot;&quot; if label_func is None: raise Exception(&#39;label_func is None. A way to label the files must be passed&#39;) self.files = files self.label_func = label_func self.transforms = transforms def __len__(self): return len(self.files) def __getitem__(self, i): img_name = self.files[i] label = self.label_func(img_name) image = Image.open(img_name).convert(&#39;RGB&#39;) if self.transforms: image = self.transforms(image) return {&#39;image&#39;: image, &#39;label&#39;: label} . To use this Dataset class we need to pass in a list of Pathlib files, and a way to label them. We can use the .glob method of the Pathlib to find all files that match the .JPEG pattern like below: . train_files = [i for i in train_dir.glob(&#39;**/*.JPEG&#39;)] train_files[0] . Path(&#39;/root/.fastai/data/imagenette2-160/train/n03445777/n03445777_6465.JPEG&#39;) . To label the image we need to know the set of unique labels of which there are 10 in total, and have a dictionary that converts the string to an index, and then sets the corresponding index in a tensor to 1 for that image, label pair. . unique_labels = [d.name for d in train_dir.ls() if d.is_dir()] label_dict = {lab: i for i, lab in enumerate(unique_labels)} def get_label(filename): label_ten = torch.zeros((10, 1)) label_str = filename.parent.name label_ten[label_dict[label_str]] = 1.0 return label_ten get_label(train_files[0]) . tensor([[1.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.]]) . Let&#39;s see this in action. I am passing the list of train image files and the get_label as the label function to convert the label to a tensor. Indexing into the train_ds returns the image and label at the i-th index as a dictionary. . train_ds = ImageDataset(files=train_files, label_func=get_label) first_example = train_ds[0] first_example[&#39;label&#39;] first_example[&#39;image&#39;] . Nice! Seems to be working, but let&#39;s have a look at a couple of image sizes. . print(&#39;Image is size &#39;, train_ds[178][&#39;image&#39;].size) print(&#39;Image is size &#39;, train_ds[-150][&#39;image&#39;].size) . Image is size (181, 160) Image is size (160, 204) . Data Augmentation . As you can see these images are different sizes. For computer vision models to work the input tensors have to be of the same shape (height x width). There are several different ways we can do this. The simplest is to just reshape every image to a square, 124 x 124 for instance, regardless of the original size. The problem with this is that we will be squishing and stretching the image in ways that will distort the shape of anything contained in the image. . This simple technique also misses out on a key thing we can do to maximise what we can get out of the training data - namely data augmentation. Data augmentation is a technique where we apply transforms to the input such that each time we get the i-th example from our ImageDataset there is some randomness in the transform and the image we get back is not always exactly the same. In this way, it effectively increases the size of the training set. . For the training dataset, I am going to make use of some transforms from the torchvision library to do this. . from torchvision.transforms import Resize, RandomCrop, RandomHorizontalFlip, Compose # resizes smallest edge to the given size - this should already be 160, but to be safe resize = Resize(size=160) # takes a random crop of size (h, w) random_crop = RandomCrop(size=(124, 124)) # randomly flip the image on the along y axis with probablity = p random_flip = RandomHorizontalFlip(p=0.5) # compose these all together train_transforms = Compose( [ resize, random_crop, random_flip ] ) train_ds = ImageDataset(files=train_files, label_func=get_label, transforms=train_transforms) . Let&#39;s get an example from this dataset now and have a look. . train_ds[3456][&#39;image&#39;] . And again... As you can see these are now not exactly the same now that we have thrown some randomness into the mix. . train_ds[3456][&#39;image&#39;] . Currently what we get back from ImageDataset is a PIL image and a tensor representing the labels but for training we want both the image and the labels to be converted to a tensor. For this, we just need to add in a ToTensor transform, which will do this and convert the pixel values to between 0-1. Notice how when we call shape it is slightly different to what we get when the image array shape is displayed with numpy. Pytorch expects images to have shape Channel, Height, Width as opposed to Height, Width, Channel that we get with numpy. . Finally, we add a normalisation transform to get tensors in the range -1 to 1. Here I am just going to use the Imagenet stats for the mean and standard deviation of each channel, as Imagenette comes from Imagenet data. What you do here depends on what you are using for your model as well. If you are going to use transfer learning and the pre-trained model that you start from has learnt from Imagenet you will also want to use Imagenet stats. If you are starting with a new model, and a new dataset then you want to compute the channels means and standard deviation on your training set images and pass these stats to the normalisation transform. . from torchvision.transforms import ToTensor, Normalize to_tensor = ToTensor() # normalisation [Red channel mean, B mean, G mean], [R std dev, G std dev, B std dev] norm = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) train_transforms = Compose( [ resize, random_crop, random_flip, to_tensor, norm ] ) train_dataset = ImageDataset(files=train_files, label_func=get_label, transforms=train_transforms) first_example = train_dataset[0] print(first_example[&#39;image&#39;][0, :10, :10])# 10 x 10 pixels from the first channel (red) print(first_example[&#39;label&#39;]) print(first_example[&#39;image&#39;].shape) . tensor([[-0.8335, -0.5253, -0.3027, -0.4739, -0.2513, -0.1143, -0.1828, -0.1657, -0.0801, -0.1314], [-0.1657, -0.0972, -0.2684, -0.3027, -0.2171, -0.1143, -0.0972, -0.2342, -0.4054, -0.4568], [-0.1999, -0.1828, -0.2342, -0.1486, -0.1828, -0.1828, -0.2171, -0.4568, -0.7479, -0.7993], [-0.5767, -0.5424, -0.1657, -0.1657, -0.1314, -0.1657, -0.3369, -0.5253, -0.6109, -0.5767], [-0.3198, -0.4054, 0.0569, -0.1999, 0.0056, -0.0116, -0.2684, -0.3541, -0.1314, 0.0227], [-0.5253, -0.2513, 0.0227, -0.0629, -0.0629, 0.0741, -0.0801, -0.3541, -0.0458, 0.3652], [-0.6281, -0.4739, -0.4397, -0.5424, -0.4226, -0.1657, -0.0629, -0.2171, -0.1999, 0.0569], [-0.3369, -0.3198, -0.4911, -0.1999, -0.3541, -0.4054, -0.0972, 0.2282, 0.1426, -0.0458], [-0.3541, -0.2342, -0.2342, -0.0458, -0.1999, -0.5596, -0.4739, -0.0629, -0.1314, -0.4226], [-0.8164, -0.5082, -0.2171, -0.3541, -0.0287, -0.1828, -0.5082, -0.5253, -0.5596, -0.5424]]) tensor([[1.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.]]) torch.Size([3, 124, 124]) . Great so now we have our training dataset in our ImageDataset class that we can index into and get the image as a tensor with random cropping and mirroring, and the label as a tensor. . We now need the same for validation images. Luckily this is quite easy we just reuse the same custom class with a few key differences. We don&#39;t want our transforms to be random here. This is the validation set and we want the same image/tensor to be returned each time we pass through this dataset, otherwise, the accuracy metrics won&#39;t be consistent. To achieve this we will resize the smallest edge to the same size as the output of the training dataset, and then take a central square crop of the same size each time. . from torchvision.transforms import CenterCrop val_resize = Resize(124) val_crop = CenterCrop(124) valid_transforms = Compose( [ val_resize, val_crop ] ) val_files = [i for i in val_dir.glob(&#39;**/*.JPEG&#39;)] val_dataset = ImageDataset(files=val_files, label_func=get_label, transforms=valid_transforms) . Let&#39;s have a look at the first image in the validation set. . val_dataset[0][&#39;image&#39;] . And again. No randomness here, it&#39;s the same again. . img = val_dataset[0][&#39;image&#39;] print(img.shape) img . (124, 124) . This looks good so now we again want to make sure the image is converted to tensor and is normalised, so let&#39;s add those to the transforms. . val_to_tensor = ToTensor() # its important we use the same normalisation stats as the train, i.e don&#39;t recompute for validation set ever. val_normalise = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) valid_transforms = Compose( [ val_resize, val_crop, val_to_tensor, val_normalise ] ) val_dataset = ImageDataset(files=val_files, label_func=get_label, transforms=valid_transforms) val_dataset[-1000] . {&#39;image&#39;: tensor([[[ 0.4851, 0.5878, 0.7762, ..., 1.8722, 1.8550, 1.8550], [-0.1314, 0.0398, 0.2624, ..., 2.2489, 2.2489, 2.2318], [-0.0972, 0.1254, 0.2967, ..., 2.2489, 2.2489, 2.2489], ..., [-1.4329, -1.0390, -1.0562, ..., 0.6221, 0.6563, 0.6563], [-1.6727, -1.4843, -1.0904, ..., 0.5707, 0.6049, 0.6049], [-0.4911, -0.3883, -0.2856, ..., 1.0502, 1.0844, 1.0844]], [[ 0.1527, 0.2402, 0.3978, ..., 2.0434, 2.0434, 2.0259], [-0.4776, -0.3375, -0.1450, ..., 2.4286, 2.4286, 2.4111], [-0.4426, -0.2675, -0.1099, ..., 2.4111, 2.3936, 2.3936], ..., [-1.6331, -1.2654, -1.2829, ..., 0.1001, 0.1352, 0.1527], [-1.8782, -1.7031, -1.3354, ..., 0.0651, 0.1001, 0.1001], [-0.6877, -0.6001, -0.5126, ..., 0.5728, 0.6078, 0.6078]], [[ 0.0779, 0.1476, 0.2871, ..., 2.3088, 2.2914, 2.2566], [-0.5844, -0.4275, -0.2532, ..., 2.6226, 2.6226, 2.6051], [-0.5495, -0.3753, -0.2358, ..., 2.5877, 2.5703, 2.5529], ..., [-1.5779, -1.2467, -1.2467, ..., -0.0441, -0.0092, 0.0082], [-1.7870, -1.6476, -1.3164, ..., -0.0615, -0.0267, -0.0267], [-0.6367, -0.5844, -0.4973, ..., 0.4439, 0.4788, 0.4788]]]), &#39;label&#39;: tensor([[0.], [0.], [0.], [0.], [0.], [0.], [0.], [1.], [0.], [0.]])} . Dataloaders . Datasets get our image, label at the i-th index for us but to be able to train efficiently we want to stack several items together into batches. That&#39;s where the DataLoader class from torch.utils.data comes in. . We can pass our datasets into this class and quite easily get something that returns our image tensors batched together. The default dataloader will most often do the job for us but we can inherit from this class and create a more custom DataLoader if needed. See a more custom DataLoader I have created in the past to get an idea of how we can do this. . The key parameters here are dataset - the dataset we have previously implemented, batch_size - how many items we want to stack together into a batch and shuffle - whether to randomise the order of items before batching them up. . One thing you might want to change is how the DataLoader collates your items into batches and you can do that by passing in a function with the collate_fn parameter. Here this isn&#39;t necessary as batching these images and labels together is quite easy. You might want to do this if you are implementing padding in a text DataLoader though. See here and here for examples of where I have done this before. . from torch.utils.data import DataLoader train_dl = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) . A DataLoader returns an iterable so that we can iterate through batches. We can use next and iter to grab one batch for inspection. Let&#39;s check the size of the image batch and the label batch. As you can see images are batched together with 64 in total and all have 3 channels, and a height and width of 124 pixels. . The label batch again has 64 items, each one with a shape of 10 rows by 1 column relating to the 10 possible options for each label. Only one row of the 10 will be equal to 1 and the rest as 0s as this is a single label classification task. . batch = next(iter(train_dl)) print(&#39;Images have been batched into a batch of size&#39;, batch[&#39;image&#39;].shape) print(&#39;Labels have been batched into a batch of size&#39;, batch[&#39;label&#39;].shape) batch[&#39;label&#39;][0] . Images have been batched into a batch of size torch.Size([64, 3, 124, 124]) Labels have been batched into a batch of size torch.Size([64, 10, 1]) . tensor([[1.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.]]) . Again with the validation DataLoader, it&#39;s simple enough to do the same thing as you do with the training set. One thing we can do differently is to use a higher batch size. With the validation data, we won&#39;t be tracking forward passes and accumulating gradients so larger batches won&#39;t have as large a footprint on GPU usage compared to the same batch size for the training set. It&#39;s also not necessary to shuffle the validation data. . val_dl = DataLoader(dataset=val_dataset, batch_size=124) . Models . With our ImageDataset now passed into a DataLoader class, we have a nice way to iterate through batches of the training data and the validation data. . We now need a way to go from the input data - the batch[&#39;image&#39;] - to predictions that can be compared to the correct label - the batch[&#39;label&#39;]. For this, we need a model, and here we only have 3 constraints: . The model must be able to take in input tensors of the size we have specified (3 x 124 x 124). | Have weights that can be updated during training to lower the output of the loss function, and hopefully make the model better at prediction. | Produce outputs that have the same shape as the labels (10 x 1) and reflect the probabilities of a given image being that label. | With those constraints met the possibilities for a model are endless and are only limited by your compute resource and imagination. Thankfully for image recognition, there are several well known good neural network architectures. . First, though let&#39;s start with a simple feedforward network so we can see what is going on easily. Pytorch again provides torch.nn which has lots of different useful building blocks for creating neural network architecture. This simple feedforward network will use linear layers and Relu to take a flatten tensor of the image ((3*124*124), 1) to a 10 x 1 output. We will have 1 hidden layer. . One way to define a model in Pytorch is to define a class that inherits from nn.Module and call super in our __init__ and then also implement a forward method for your class. The forward method is what we use on every forward pass, unsurprisingly, and is what happens when you use an instantiated model like so model(..). If you know anything about python and dunder methods (double underscored methods) you will recognise this as very similar to implementing a __call__ method. . from torch import nn class SimpleNN(nn.Module): &quot;&quot;&quot;Simple Feed Forward Neural Net&quot;&quot;&quot; def __init__(self, n_inp, n_hidden, n_out): super().__init__() self.n_inp = n_inp self.n_hidden = n_hidden self.n_out = n_out self.in_to_hid = nn.Linear(n_inp, n_hidden) self.relu = nn.ReLU() self.hid_to_out = nn.Linear(n_hidden, n_out) self.softmax = nn.Softmax(dim=1) def forward(self, xb): xb = self.in_to_hid(xb) xb = self.relu(xb) xb = self.hid_to_out(xb) out = self.softmax(xb) return out . This is a really simple neural network. It goes from input to a hidden layer to output layer with these tensors of size: . input = (batch_size, n_inp) | input to hidden layer weight tensor = (n_inp, n_hidden) | hidden to output layer weight tensor = (n_hidden, n_out) | . We are essentially just doing a chain of matrix multiplication where matrix multiplication results in this shape: (m, n) x (n, o) = (m, o) . So with (batch_size, n_inp) x (n_inp, n_hidden) = (batch_size, n_hidden) . And then (batch_size, n_hidden) x (n_hidden, n_out) = (batch_size, n_out) . Right, so let&#39;s see this in action. . simple_net = SimpleNN(n_inp=3*124*124, n_hidden=512, n_out=10) one_batch = batch[&#39;image&#39;] # let&#39;s take just 3 images from this batch one_batch = one_batch[:3, :, :, :] # flatten each image tensor to something of shape=3x124x124 # this is just so we can input this tensor to a feedforward network print(&#39;Original batch size: &#39;, one_batch.shape) one_batch = one_batch.flatten(start_dim=1) print(&#39;Batch size after flatten: &#39;, one_batch.shape) output_of_model = simple_net(one_batch) print(&#39;Output size: &#39;, output_of_model.shape) output_of_model . Original batch size: torch.Size([3, 3, 124, 124]) Batch size after flatten: torch.Size([3, 46128]) Output size: torch.Size([3, 10]) . tensor([[0.1282, 0.1080, 0.1170, 0.0974, 0.0630, 0.1192, 0.0882, 0.0977, 0.0802, 0.1012], [0.1584, 0.0784, 0.1103, 0.1360, 0.0633, 0.0694, 0.0926, 0.0811, 0.1181, 0.0925], [0.0918, 0.0954, 0.1246, 0.0961, 0.0794, 0.0887, 0.0970, 0.1036, 0.1157, 0.1078]], grad_fn=&lt;SoftmaxBackward&gt;) . Models know what their parameters are and set the requires_grad to True by default. Here we should have four tensors that are the parameters of the model - each linear layer has a weight tensor and a bias tensor. It&#39;s important to note looking at the following output that nn.Linear transposes the weight tensor before doing the linear transform. . params = list(simple_net.parameters()) print(&#39;SimpleNN has&#39;, len(params), &#39;tensor parameters&#39;) for i, param in enumerate(params, start=1): print(f&#39;Parameter #{i}&#39;) print(&#39;Parameter is of type&#39;, param.dtype) print(&#39;Parameter has a shape&#39;, param.shape) print(f&#39;Parameter requires_grad={param.requires_grad} n&#39;) . SimpleNN has 4 tensor parameters Parameter #1 Parameter is of type torch.float32 Parameter has a shape torch.Size([512, 46128]) Parameter requires_grad=True Parameter #2 Parameter is of type torch.float32 Parameter has a shape torch.Size([512]) Parameter requires_grad=True Parameter #3 Parameter is of type torch.float32 Parameter has a shape torch.Size([10, 512]) Parameter requires_grad=True Parameter #4 Parameter is of type torch.float32 Parameter has a shape torch.Size([10]) Parameter requires_grad=True . So we have a model that can handle image input data. But this model is a bit boring for a computer vision task, let&#39;s use a small ResNet for this. ResNets are a type of deep computer vision neural net architecture that do very well on image classification tasks. Computer vision architectures differ from standard feedforward networks through their use of convolution layers which are great for image inputs as they are kernels that shift over the input features and we don&#39;t have to flatten the image to a long array, we can keep it its original shape for the input step. To get more of an idea about convolution layers see &quot;A guide to convolution arithmetic for deep learning&quot; . As the pre-trained ResNet that we can get from torchvision.models was trained on Imagenet we will make sure we set pretrained to False. In reality, if you have new data, using a pre-trained model to start with is a good idea - your model already knows how to do one task and won&#39;t take so much to adapt to a new one! . Similarly, the model from torchvision.models final layer creates an output that fits the shape of 1000 classes in Imagenet, but as we are going to learn from Imagenette data we only want output for 10 classes. Let&#39;s use this network and modify the output. . from torchvision.models import resnet18 class ModifiedResNet(nn.Module): def __init__(self, n_out): super().__init__() self.n_out = n_out self.body = nn.Sequential(*list(resnet18(pretrained=False).children())[:-1]) self.flatten = nn.Flatten(start_dim=1) self.fc = nn.Linear(512, n_out) def forward(self, xb): xb = self.body(xb) # need to flatten to get rid of 1x1 from the shape (bs x 512 x 1 x 1) xb = self.flatten(xb) return self.fc(xb) model = ModifiedResNet(n_out=10) model . ModifiedResNet( (body): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (8): AdaptiveAvgPool2d(output_size=(1, 1)) ) (flatten): Flatten(start_dim=1, end_dim=-1) (fc): Linear(in_features=512, out_features=10, bias=True) ) . . Loss . So to reiterate we have a way of batching our data and a model to go from input -&gt; prediction but now we need a way to check these predictions, and using this function to optimise the weights of the models. . Again torch.nn has these loss functions that we need. Here we are classifying with 10 possible labels, so we want a loss function that is lower when the probability output for the given class is high, i.e. the model is confident and correct about the class. And a high loss when the model is confidently wrong. For this we can use nn.CrossEntropyLoss which works for this multi-classification task. . Let&#39;s have look at this with a small 4 &quot;class&quot; example. In the first example here the 3rd label is the correct label and the &quot;model&quot; output has predicted that quite confidently. As you can see the loss is very small for this - there is not much the model can do to better predict this. In the second example, the &quot;model&quot; is confident that the label should be the 2nd label but in reality, it is the 3rd label. The loss is high - the model could stand to make improvements and do better. . output = torch.tensor([[0.5, 0.1, 12, 0.3]]) target = torch.tensor([[0., 0., 1., 0.]]) print(&#39;&quot;Model&quot; output probabilities:&#39;, output.softmax(dim=1)) print(&#39;Actual label:&#39;, target) loss_func = nn.CrossEntropyLoss() loss = loss_func(output, target.argmax(dim=1)) print(&#39;Model is confident and correct, loss=&#39;, loss, &#39; n&#39;) output = torch.tensor([[0.5, 12, 0.1, 0.3]]) target = torch.tensor([[0., 0., 1., 0.]]) print(&#39;&quot;Model&quot; output probabilities:&#39;, output.softmax(dim=1)) print(&#39;Actual label:&#39;, target) loss = loss_func(output, target.argmax(dim=1)) print(&#39;Model is confident and incorrect, loss=&#39;, loss, &#39; n&#39;) . &#34;Model&#34; output probabilities: tensor([[1.0130e-05, 6.7902e-06, 9.9997e-01, 8.2936e-06]]) Actual label: tensor([[0., 0., 1., 0.]]) Model is confident and correct, loss= tensor(2.5272e-05) &#34;Model&#34; output probabilities: tensor([[1.0130e-05, 9.9997e-01, 6.7902e-06, 8.2936e-06]]) Actual label: tensor([[0., 0., 1., 0.]]) Model is confident and incorrect, loss= tensor(11.9000) . For the loss function we provide the output of the model as a tensor which has the size (batch_size x number of classes) and the desired target label for each item in the batch so that they have the shape (batch_size,). We can use .argmax on a tensor to get which is index is the highest value, as we do with the target tensor above in the loss function. . With the output of this function, we can call loss.backward() to compute gradients for any tensor used in the calculation of the output probability, and use those gradients to update those tensors to hopefully lower the loss. . Optimizers and Learning Rate Schedulers . The final part of the puzzle before we can get to training are optimizers from torch.optim. When we come to update the parameters of our model we could call loss.backward() and iterate through our parameters to update according to some fixed learning rate like so: . for batch in train_dl: xb, targets = batch[&#39;image&#39;], batch[&#39;label&#39;] preds = model(xb) loss = loss_func(preds, targets) loss.backward() for param in model.parameters(): param.data = param.data - learning_rate * param.grad param.grad = None . But this is a bit unwieldy. PyTorch has lots of optimizers you can use to update model parameters and has the added benefit that you don&#39;t need to implement things such as momentum, or weight decay (L2 regularisation) as these are already there for you to use. To use an optimizer you can do this: . from torch.optim import Adam model = SomeModel() optimizer = Adam(model.parameters(), lr=0.003) ... for batch in train_dl: xb, targets = xb, targets = batch[&#39;image&#39;], batch[&#39;label&#39;] preds = model(xb) loss = loss_func(preds, targets) loss.backward() optimizer.step() optimizer.zero_grad() . As well as using optimizers it is a good idea to use learning rate schedulers. It is a common technique to use dynamic learning rates that change during training rather than using a flat learning rate throughout. PyTorch provides these through torch.optim.lr_scheduler. You can use these in the following way: . from torch.optim import Adam from torch.optim.lr_scheduler import OneCycleLR ... model = SomeModel() optimizer = Adam(model.parameters(), lr=0.003) scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=10) ... for epoch in range(10): for batch in train_dl: xb, targets = xb, targets = batch[&#39;image&#39;], batch[&#39;label&#39;] preds = model(xb) loss = loss_func(preds, targets) loss.backward() optimizer.step() optimizer.zero_grad() scheduler.step() . The advantage of using a scheduler like OneCycleLR is that we can start with a relatively low learning rate and making small updates to parameters before ramping up to a high learning rate about a third of the way in training allowing us to make bigger updates to the parameters. Finally, we slowly descend back to smaller learning rates and make less drastic updates when the model should have already learnt. This schedule was demonstrated in &quot;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&quot; where it was shown that this could lead to faster training. . Let&#39;s examine the schedule a bit more below: . from torch.optim import SGD from torch.optim.lr_scheduler import OneCycleLR opt = SGD(simple_net.parameters(), lr=0.003) sched = OneCycleLR(opt, max_lr=0.1, epochs=3, steps_per_epoch=100) step = [] lr = [] for i in range(3*100): step.append(i) lr.append(sched.get_last_lr()) opt.step() sched.step() f, ax = plt.subplots(1, 1) ax.plot(step, lr) _ = ax.set_ylabel(&#39;Learning Rate&#39;, fontsize=14) _ = ax.set_xlabel(&#39;Step&#39;, fontsize=14) _ = ax.tick_params(&#39;both&#39;, labelsize=14) ymin, ymax = ax.get_ylim() ax.vlines(x=100, ymin=ymin, ymax=ymax, linestyle=&#39;dashed&#39;, color=&#39;black&#39;) ax.vlines(x=200, ymin=ymin, ymax=ymax, linestyle=&#39;dashed&#39;, color=&#39;black&#39;) _ = ax.text(x=101, y=ymin+((ymax - ymin)/2), s=&#39;First Epoch&#39;, rotation=-90, fontsize=12, color=&#39;red&#39;) _ = ax.text(x=201, y=ymin+((ymax - ymin)/2), s=&#39;Second Epoch&#39;, rotation=-90, fontsize=12, color=&#39;red&#39;) . Training . Phew! We have made it to training. So what does PyTorch have up its sleeve to help us train? Well, nothing actually! We are free to implement the training loop however we want with the tools we have already seen. In fact, a training loop is very similar to the loops shown above in the Optimizer section. . Often the logic of the training loop doesn&#39;t change from application to application so it seems overkill to reimplement it each time. Luckily some libraries can help us out by reducing the boilerplate and provide useful utilities: . PyTorch Lightning | fast.ai | . I am a big fan of fast.ai and have found their courses to be really good for getting into and learning deep learning concepts. By using fast.ai this problem can be reduced to around 5 lines of code or so. I have yet to try out PyTorch Lightning, so maybe I will do that for a future post. Here, however, we want to see how we can do this with pure PyTorch. . Let&#39;s first set up a few things by re-initializing the ResNet18 model and . from torch.optim import Adam N_EPOCHS = 20 BASE_LR = 0.005 # if GPU is avaiable we can use .to(DEVICE) to put tensors on the GPU DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model = ModifiedResNet(n_out=10).to(DEVICE) opt = Adam(model.parameters(), lr=BASE_LR) sched = OneCycleLR(opt, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=N_EPOCHS) loss_func = nn.CrossEntropyLoss() . Now let&#39;s write the actual training loop. Here its pretty simple: . We iterate through the batches of the training dataloader | Forward pass of the images through the model | Compare to actual labels to compute the loss | Call loss.backward() to compute gradients, and use those to update the parameter of the models | Once iterated through the whole training dataloader iterate through the validation dataloader and use these batches to calculate loss (not for updating model, just as a metric) and accuracy. | Repeat the process for N epochs. | Several things to note. First is the use of model.train() and model.eval() - this sets certain elements of the model such as batch norm layers to track metrics during training but not during evaluation. The second thing is the use of torch.no_grad() in the validation loop which as the name implies prevents computation of gradients within this context block. Validation is just for computing metrics so gradients aren&#39;t needed during this step. . training_losses = [] validation_losses = [] validation_accuracies = [] for epoch in range(N_EPOCHS): train_loss = 0 model.train() for i, batch in enumerate(train_dl): if i == 0: print(f&#39;Epoch {epoch} - beginning training&#39;) xb, label = batch[&#39;image&#39;].to(DEVICE), batch[&#39;label&#39;].to(DEVICE) preds = model(xb) loss = loss_func(preds, label.argmax(axis=1).flatten()) train_loss += loss.item() loss.backward() opt.step() opt.zero_grad() sched.step() if i % 10 == 0: print(f&#39;Training - Epoch {epoch} - {100*(i/len(train_dl)):.2f}%&#39;) model.eval() val_loss, val_acc = 0, 0 with torch.no_grad(): for i, batch in enumerate(val_dl): if i == 0: print(f&#39;Epoch {epoch} - beginning validation&#39;) xb, label = batch[&#39;image&#39;].to(DEVICE), batch[&#39;label&#39;].to(DEVICE) preds = model(xb) loss = loss_func(preds, label.argmax(axis=1).flatten()) val_loss += loss.item() val_acc += (preds.argmax(1) == label.argmax(axis=1).flatten()).sum().item() if i % 10 == 0: print(f&#39;Validation - Epoch {epoch} - {100*(i/len(val_dl)):.2f}%&#39;) train_loss = train_loss / len(train_dl) training_losses.append(train_loss) val_loss = val_loss / len(val_dl) validation_losses.append(val_loss) val_acc = val_acc / len(val_dl.dataset) validation_accuracies.append(val_acc) print(f&#39;Epoch {epoch}: Train loss={train_loss:.3f} | Val Loss={val_loss:.3f} | Val Acc={val_acc*100:.3f}&#39;) . Epoch 0 - beginning training Training - Epoch 0 - 0.00% Training - Epoch 0 - 6.76% Training - Epoch 0 - 13.51% Training - Epoch 0 - 20.27% Training - Epoch 0 - 27.03% Training - Epoch 0 - 33.78% Training - Epoch 0 - 40.54% Training - Epoch 0 - 47.30% Training - Epoch 0 - 54.05% Training - Epoch 0 - 60.81% Training - Epoch 0 - 67.57% Training - Epoch 0 - 74.32% Training - Epoch 0 - 81.08% Training - Epoch 0 - 87.84% Training - Epoch 0 - 94.59% Epoch 0 - beginning validation Validation - Epoch 0 - 0.00% Validation - Epoch 0 - 31.25% Validation - Epoch 0 - 62.50% Validation - Epoch 0 - 93.75% Epoch 0: Train loss=1.614 | Val Loss=1.781 | Val Acc=47.643 Epoch 1 - beginning training Training - Epoch 1 - 0.00% Training - Epoch 1 - 6.76% Training - Epoch 1 - 13.51% Training - Epoch 1 - 20.27% Training - Epoch 1 - 27.03% Training - Epoch 1 - 33.78% Training - Epoch 1 - 40.54% Training - Epoch 1 - 47.30% Training - Epoch 1 - 54.05% Training - Epoch 1 - 60.81% Training - Epoch 1 - 67.57% Training - Epoch 1 - 74.32% Training - Epoch 1 - 81.08% Training - Epoch 1 - 87.84% Training - Epoch 1 - 94.59% Epoch 1 - beginning validation Validation - Epoch 1 - 0.00% Validation - Epoch 1 - 31.25% Validation - Epoch 1 - 62.50% Validation - Epoch 1 - 93.75% Epoch 1: Train loss=1.393 | Val Loss=1.756 | Val Acc=48.866 Epoch 2 - beginning training Training - Epoch 2 - 0.00% Training - Epoch 2 - 6.76% Training - Epoch 2 - 13.51% Training - Epoch 2 - 20.27% Training - Epoch 2 - 27.03% Training - Epoch 2 - 33.78% Training - Epoch 2 - 40.54% Training - Epoch 2 - 47.30% Training - Epoch 2 - 54.05% Training - Epoch 2 - 60.81% Training - Epoch 2 - 67.57% Training - Epoch 2 - 74.32% Training - Epoch 2 - 81.08% Training - Epoch 2 - 87.84% Training - Epoch 2 - 94.59% Epoch 2 - beginning validation Validation - Epoch 2 - 0.00% Validation - Epoch 2 - 31.25% Validation - Epoch 2 - 62.50% Validation - Epoch 2 - 93.75% Epoch 2: Train loss=1.347 | Val Loss=1.407 | Val Acc=54.318 Epoch 3 - beginning training Training - Epoch 3 - 0.00% Training - Epoch 3 - 6.76% Training - Epoch 3 - 13.51% Training - Epoch 3 - 20.27% Training - Epoch 3 - 27.03% Training - Epoch 3 - 33.78% Training - Epoch 3 - 40.54% Training - Epoch 3 - 47.30% Training - Epoch 3 - 54.05% Training - Epoch 3 - 60.81% Training - Epoch 3 - 67.57% Training - Epoch 3 - 74.32% Training - Epoch 3 - 81.08% Training - Epoch 3 - 87.84% Training - Epoch 3 - 94.59% Epoch 3 - beginning validation Validation - Epoch 3 - 0.00% Validation - Epoch 3 - 31.25% Validation - Epoch 3 - 62.50% Validation - Epoch 3 - 93.75% Epoch 3: Train loss=1.282 | Val Loss=1.821 | Val Acc=43.287 Epoch 4 - beginning training Training - Epoch 4 - 0.00% Training - Epoch 4 - 6.76% Training - Epoch 4 - 13.51% Training - Epoch 4 - 20.27% Training - Epoch 4 - 27.03% Training - Epoch 4 - 33.78% Training - Epoch 4 - 40.54% Training - Epoch 4 - 47.30% Training - Epoch 4 - 54.05% Training - Epoch 4 - 60.81% Training - Epoch 4 - 67.57% Training - Epoch 4 - 74.32% Training - Epoch 4 - 81.08% Training - Epoch 4 - 87.84% Training - Epoch 4 - 94.59% Epoch 4 - beginning validation Validation - Epoch 4 - 0.00% Validation - Epoch 4 - 31.25% Validation - Epoch 4 - 62.50% Validation - Epoch 4 - 93.75% Epoch 4: Train loss=1.158 | Val Loss=1.273 | Val Acc=59.414 Epoch 5 - beginning training Training - Epoch 5 - 0.00% Training - Epoch 5 - 6.76% Training - Epoch 5 - 13.51% Training - Epoch 5 - 20.27% Training - Epoch 5 - 27.03% Training - Epoch 5 - 33.78% Training - Epoch 5 - 40.54% Training - Epoch 5 - 47.30% Training - Epoch 5 - 54.05% Training - Epoch 5 - 60.81% Training - Epoch 5 - 67.57% Training - Epoch 5 - 74.32% Training - Epoch 5 - 81.08% Training - Epoch 5 - 87.84% Training - Epoch 5 - 94.59% Epoch 5 - beginning validation Validation - Epoch 5 - 0.00% Validation - Epoch 5 - 31.25% Validation - Epoch 5 - 62.50% Validation - Epoch 5 - 93.75% Epoch 5: Train loss=1.048 | Val Loss=1.448 | Val Acc=54.599 Epoch 6 - beginning training Training - Epoch 6 - 0.00% Training - Epoch 6 - 6.76% Training - Epoch 6 - 13.51% Training - Epoch 6 - 20.27% Training - Epoch 6 - 27.03% Training - Epoch 6 - 33.78% Training - Epoch 6 - 40.54% Training - Epoch 6 - 47.30% Training - Epoch 6 - 54.05% Training - Epoch 6 - 60.81% Training - Epoch 6 - 67.57% Training - Epoch 6 - 74.32% Training - Epoch 6 - 81.08% Training - Epoch 6 - 87.84% Training - Epoch 6 - 94.59% Epoch 6 - beginning validation Validation - Epoch 6 - 0.00% Validation - Epoch 6 - 31.25% Validation - Epoch 6 - 62.50% Validation - Epoch 6 - 93.75% Epoch 6: Train loss=0.973 | Val Loss=0.996 | Val Acc=67.618 Epoch 7 - beginning training Training - Epoch 7 - 0.00% Training - Epoch 7 - 6.76% Training - Epoch 7 - 13.51% Training - Epoch 7 - 20.27% Training - Epoch 7 - 27.03% Training - Epoch 7 - 33.78% Training - Epoch 7 - 40.54% Training - Epoch 7 - 47.30% Training - Epoch 7 - 54.05% Training - Epoch 7 - 60.81% Training - Epoch 7 - 67.57% Training - Epoch 7 - 74.32% Training - Epoch 7 - 81.08% Training - Epoch 7 - 87.84% Training - Epoch 7 - 94.59% Epoch 7 - beginning validation Validation - Epoch 7 - 0.00% Validation - Epoch 7 - 31.25% Validation - Epoch 7 - 62.50% Validation - Epoch 7 - 93.75% Epoch 7: Train loss=0.902 | Val Loss=0.931 | Val Acc=71.159 Epoch 8 - beginning training Training - Epoch 8 - 0.00% Training - Epoch 8 - 6.76% Training - Epoch 8 - 13.51% Training - Epoch 8 - 20.27% Training - Epoch 8 - 27.03% Training - Epoch 8 - 33.78% Training - Epoch 8 - 40.54% Training - Epoch 8 - 47.30% Training - Epoch 8 - 54.05% Training - Epoch 8 - 60.81% Training - Epoch 8 - 67.57% Training - Epoch 8 - 74.32% Training - Epoch 8 - 81.08% Training - Epoch 8 - 87.84% Training - Epoch 8 - 94.59% Epoch 8 - beginning validation Validation - Epoch 8 - 0.00% Validation - Epoch 8 - 31.25% Validation - Epoch 8 - 62.50% Validation - Epoch 8 - 93.75% Epoch 8: Train loss=0.824 | Val Loss=0.987 | Val Acc=69.427 Epoch 9 - beginning training Training - Epoch 9 - 0.00% Training - Epoch 9 - 6.76% Training - Epoch 9 - 13.51% Training - Epoch 9 - 20.27% Training - Epoch 9 - 27.03% Training - Epoch 9 - 33.78% Training - Epoch 9 - 40.54% Training - Epoch 9 - 47.30% Training - Epoch 9 - 54.05% Training - Epoch 9 - 60.81% Training - Epoch 9 - 67.57% Training - Epoch 9 - 74.32% Training - Epoch 9 - 81.08% Training - Epoch 9 - 87.84% Training - Epoch 9 - 94.59% Epoch 9 - beginning validation Validation - Epoch 9 - 0.00% Validation - Epoch 9 - 31.25% Validation - Epoch 9 - 62.50% Validation - Epoch 9 - 93.75% Epoch 9: Train loss=0.772 | Val Loss=0.807 | Val Acc=74.904 Epoch 10 - beginning training Training - Epoch 10 - 0.00% Training - Epoch 10 - 6.76% Training - Epoch 10 - 13.51% Training - Epoch 10 - 20.27% Training - Epoch 10 - 27.03% Training - Epoch 10 - 33.78% Training - Epoch 10 - 40.54% Training - Epoch 10 - 47.30% Training - Epoch 10 - 54.05% Training - Epoch 10 - 60.81% Training - Epoch 10 - 67.57% Training - Epoch 10 - 74.32% Training - Epoch 10 - 81.08% Training - Epoch 10 - 87.84% Training - Epoch 10 - 94.59% Epoch 10 - beginning validation Validation - Epoch 10 - 0.00% Validation - Epoch 10 - 31.25% Validation - Epoch 10 - 62.50% Validation - Epoch 10 - 93.75% Epoch 10: Train loss=0.714 | Val Loss=0.835 | Val Acc=73.987 Epoch 11 - beginning training Training - Epoch 11 - 0.00% Training - Epoch 11 - 6.76% Training - Epoch 11 - 13.51% Training - Epoch 11 - 20.27% Training - Epoch 11 - 27.03% Training - Epoch 11 - 33.78% Training - Epoch 11 - 40.54% Training - Epoch 11 - 47.30% Training - Epoch 11 - 54.05% Training - Epoch 11 - 60.81% Training - Epoch 11 - 67.57% Training - Epoch 11 - 74.32% Training - Epoch 11 - 81.08% Training - Epoch 11 - 87.84% Training - Epoch 11 - 94.59% Epoch 11 - beginning validation Validation - Epoch 11 - 0.00% Validation - Epoch 11 - 31.25% Validation - Epoch 11 - 62.50% Validation - Epoch 11 - 93.75% Epoch 11: Train loss=0.660 | Val Loss=0.865 | Val Acc=73.376 Epoch 12 - beginning training Training - Epoch 12 - 0.00% Training - Epoch 12 - 6.76% Training - Epoch 12 - 13.51% Training - Epoch 12 - 20.27% Training - Epoch 12 - 27.03% Training - Epoch 12 - 33.78% Training - Epoch 12 - 40.54% Training - Epoch 12 - 47.30% Training - Epoch 12 - 54.05% Training - Epoch 12 - 60.81% Training - Epoch 12 - 67.57% Training - Epoch 12 - 74.32% Training - Epoch 12 - 81.08% Training - Epoch 12 - 87.84% Training - Epoch 12 - 94.59% Epoch 12 - beginning validation Validation - Epoch 12 - 0.00% Validation - Epoch 12 - 31.25% Validation - Epoch 12 - 62.50% Validation - Epoch 12 - 93.75% Epoch 12: Train loss=0.625 | Val Loss=0.701 | Val Acc=77.860 Epoch 13 - beginning training Training - Epoch 13 - 0.00% Training - Epoch 13 - 6.76% Training - Epoch 13 - 13.51% Training - Epoch 13 - 20.27% Training - Epoch 13 - 27.03% Training - Epoch 13 - 33.78% Training - Epoch 13 - 40.54% Training - Epoch 13 - 47.30% Training - Epoch 13 - 54.05% Training - Epoch 13 - 60.81% Training - Epoch 13 - 67.57% Training - Epoch 13 - 74.32% Training - Epoch 13 - 81.08% Training - Epoch 13 - 87.84% Training - Epoch 13 - 94.59% Epoch 13 - beginning validation Validation - Epoch 13 - 0.00% Validation - Epoch 13 - 31.25% Validation - Epoch 13 - 62.50% Validation - Epoch 13 - 93.75% Epoch 13: Train loss=0.560 | Val Loss=0.610 | Val Acc=80.510 Epoch 14 - beginning training Training - Epoch 14 - 0.00% Training - Epoch 14 - 6.76% Training - Epoch 14 - 13.51% Training - Epoch 14 - 20.27% Training - Epoch 14 - 27.03% Training - Epoch 14 - 33.78% Training - Epoch 14 - 40.54% Training - Epoch 14 - 47.30% Training - Epoch 14 - 54.05% Training - Epoch 14 - 60.81% Training - Epoch 14 - 67.57% Training - Epoch 14 - 74.32% Training - Epoch 14 - 81.08% Training - Epoch 14 - 87.84% Training - Epoch 14 - 94.59% Epoch 14 - beginning validation Validation - Epoch 14 - 0.00% Validation - Epoch 14 - 31.25% Validation - Epoch 14 - 62.50% Validation - Epoch 14 - 93.75% Epoch 14: Train loss=0.510 | Val Loss=0.628 | Val Acc=79.847 Epoch 15 - beginning training Training - Epoch 15 - 0.00% Training - Epoch 15 - 6.76% Training - Epoch 15 - 13.51% Training - Epoch 15 - 20.27% Training - Epoch 15 - 27.03% Training - Epoch 15 - 33.78% Training - Epoch 15 - 40.54% Training - Epoch 15 - 47.30% Training - Epoch 15 - 54.05% Training - Epoch 15 - 60.81% Training - Epoch 15 - 67.57% Training - Epoch 15 - 74.32% Training - Epoch 15 - 81.08% Training - Epoch 15 - 87.84% Training - Epoch 15 - 94.59% Epoch 15 - beginning validation Validation - Epoch 15 - 0.00% Validation - Epoch 15 - 31.25% Validation - Epoch 15 - 62.50% Validation - Epoch 15 - 93.75% Epoch 15: Train loss=0.464 | Val Loss=0.571 | Val Acc=81.962 Epoch 16 - beginning training Training - Epoch 16 - 0.00% Training - Epoch 16 - 6.76% Training - Epoch 16 - 13.51% Training - Epoch 16 - 20.27% Training - Epoch 16 - 27.03% Training - Epoch 16 - 33.78% Training - Epoch 16 - 40.54% Training - Epoch 16 - 47.30% Training - Epoch 16 - 54.05% Training - Epoch 16 - 60.81% Training - Epoch 16 - 67.57% Training - Epoch 16 - 74.32% Training - Epoch 16 - 81.08% Training - Epoch 16 - 87.84% Training - Epoch 16 - 94.59% Epoch 16 - beginning validation Validation - Epoch 16 - 0.00% Validation - Epoch 16 - 31.25% Validation - Epoch 16 - 62.50% Validation - Epoch 16 - 93.75% Epoch 16: Train loss=0.413 | Val Loss=0.571 | Val Acc=81.962 Epoch 17 - beginning training Training - Epoch 17 - 0.00% Training - Epoch 17 - 6.76% Training - Epoch 17 - 13.51% Training - Epoch 17 - 20.27% Training - Epoch 17 - 27.03% Training - Epoch 17 - 33.78% Training - Epoch 17 - 40.54% Training - Epoch 17 - 47.30% Training - Epoch 17 - 54.05% Training - Epoch 17 - 60.81% Training - Epoch 17 - 67.57% Training - Epoch 17 - 74.32% Training - Epoch 17 - 81.08% Training - Epoch 17 - 87.84% Training - Epoch 17 - 94.59% Epoch 17 - beginning validation Validation - Epoch 17 - 0.00% Validation - Epoch 17 - 31.25% Validation - Epoch 17 - 62.50% Validation - Epoch 17 - 93.75% Epoch 17: Train loss=0.373 | Val Loss=0.523 | Val Acc=83.669 Epoch 18 - beginning training Training - Epoch 18 - 0.00% Training - Epoch 18 - 6.76% Training - Epoch 18 - 13.51% Training - Epoch 18 - 20.27% Training - Epoch 18 - 27.03% Training - Epoch 18 - 33.78% Training - Epoch 18 - 40.54% Training - Epoch 18 - 47.30% Training - Epoch 18 - 54.05% Training - Epoch 18 - 60.81% Training - Epoch 18 - 67.57% Training - Epoch 18 - 74.32% Training - Epoch 18 - 81.08% Training - Epoch 18 - 87.84% Training - Epoch 18 - 94.59% Epoch 18 - beginning validation Validation - Epoch 18 - 0.00% Validation - Epoch 18 - 31.25% Validation - Epoch 18 - 62.50% Validation - Epoch 18 - 93.75% Epoch 18: Train loss=0.345 | Val Loss=0.524 | Val Acc=83.669 Epoch 19 - beginning training Training - Epoch 19 - 0.00% Training - Epoch 19 - 6.76% Training - Epoch 19 - 13.51% Training - Epoch 19 - 20.27% Training - Epoch 19 - 27.03% Training - Epoch 19 - 33.78% Training - Epoch 19 - 40.54% Training - Epoch 19 - 47.30% Training - Epoch 19 - 54.05% Training - Epoch 19 - 60.81% Training - Epoch 19 - 67.57% Training - Epoch 19 - 74.32% Training - Epoch 19 - 81.08% Training - Epoch 19 - 87.84% Training - Epoch 19 - 94.59% Epoch 19 - beginning validation Validation - Epoch 19 - 0.00% Validation - Epoch 19 - 31.25% Validation - Epoch 19 - 62.50% Validation - Epoch 19 - 93.75% Epoch 19: Train loss=0.324 | Val Loss=0.517 | Val Acc=83.618 . . Below are the training and validation losses, alongside the accuracy - here we can see we are getting to around 83% accuracy in around 20 epochs. We can play around with training for longer, changing the hyperparameters (learning rate, regularisation) or using a bigger model to see if we can improve upon this. It is definitely possible to get to +90% accuracy with 200 epochs from looking at the Imagenette leaderboard. . If you decide to check out the libraries that can help with training, one thing that fast.ai gives us for training is a learning rate finder which helps up to pick a good value for learning rates. . Prediction . We have our model and now it&#39;s been trained so let&#39;s use it for its actual purpose - prediction! . I have grabbed a label conversion dictionary from a fast.ai tutorial for the Imagenette labels so that we can output the actual name of the item rather than the name of the directory. . lbl_dict = dict( n01440764=&#39;tench&#39;, n02102040=&#39;English springer&#39;, n02979186=&#39;cassette player&#39;, n03000684=&#39;chain saw&#39;, n03028079=&#39;church&#39;, n03394916=&#39;French horn&#39;, n03417042=&#39;garbage truck&#39;, n03425413=&#39;gas pump&#39;, n03445777=&#39;golf ball&#39;, n03888257=&#39;parachute&#39; ) . Before prediction, we will set all parameters in the model .requires_grad to False so that gradients aren&#39;t tracked and make a little predict function. For this, we want to pass in an image and apply the same valid transforms from the validation dataloader so that we can take a centre crop and convert it to a tensor. . for p in model.parameters(): p.requires_grad = False def predict(img, model, transforms): model.eval() with torch.no_grad(): # need to add a leading dimension to make a batch of 1 out = model(transforms(img).unsqueeze(0).to(DEVICE)) probs = out.softmax(1) lbl = lbl_dict[unique_labels[probs.argmax().item()]] return lbl, probs.max().item() fname = val_files[2452] img = Image.open(fname).convert(&#39;RGB&#39;) show_image(img) predict(img, model, valid_transforms) . (&#39;garbage truck&#39;, 0.9994460940361023) . fname = val_files[899] img = Image.open(fname).convert(&#39;RGB&#39;) show_image(img) predict(img, model, valid_transforms) . (&#39;English springer&#39;, 0.9999816417694092) . Conclusion . In summary then to use PyTorch we need: . Data, and a way of getting it into tensors with Dataset and DataLoader classes. You should have some training data for learning and validation data for computing metrics such as accuracy. Ideally, you should also reserve some more data as a final test set, which you will only check accuracy against once you have finished - you have selected the best model, best hyperparameter. | A model to take input data and produce output (probabilities for classification, actual number values for regression). The model will have parameters which through learning we will change to make better predictions. | Losses and optimizers to update the model parameters with. Loss should compare the output with known labels, and through gradient descent, we aim to make the output of the loss function lower. Optimizers to help us with weight updating methods during training. | . References . As mentioned at the beginning of this post PyTorch is a well-documented library with lots of good tutorials and examples. This post covers a lot of the basics that are also covered in the &quot;Getting Started&quot; tutorial of the official docs and is a good place to also check out for a similar approach. . I also found helpful: . WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS | Fast.ai docs about Imagenette | .",
            "url": "https://jc639.github.io/blog/pytorch/deep-learning/neural-nets/2021/04/06/_04_02_Pytorch_refresher.html",
            "relUrl": "/pytorch/deep-learning/neural-nets/2021/04/06/_04_02_Pytorch_refresher.html",
            "date": "  Apr 6, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Github Actions are Awesome!",
            "content": "Github Actions are a way to automate your workflow so you can build continuous-integration/continuous development (CI/CD) workflows. You can run tests, check your library or package can build, generate and commit new files all on various triggers such as pushes, pull requests or even schedules. It&#39;s really awesome for automating lots of different tasks and with scripts the possibilities are endless. It&#39;s even how this website gets built. . Fastpages . This website is built with the amazing fastpages, which alongside nbdev is great for a gentle introduction to Github Actions. . Fastpages allows me to write this post in a Jupyter notebook, and then when I push these notebooks to the blog repository an action takes place that converts them to markdown files and builds the Jekyll site that you are looking at now. . I think this is really neat but I thought it would also be cool to use this workflow to generate an ever-changing header for the front page that is based on the blog posts that have been published recently. . . An action workflow . Fastpages comes with a CI YAML file that looks something like this. I will break this down bit by bit to explain and then show how I modified it so that a new header is generated both when pushing new notebooks and on a schedule: . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: - name: Check if secret exists if: github.event_name == &#39;push&#39; run: | if [ -z &quot;$deploy_key&quot; ] then echo &quot;You do not have a secret named SSH_DEPLOY_KEY. This means you did not follow the setup instructions carefully. Please try setting up your repo again with the right secrets.&quot; exit 1; fi env: deploy_key: $ - name: Copy Repository Contents uses: actions/checkout@main with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;jekyll build -V --strict_front_matter --trace&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site . The first bit sets up when the Action file gets run. The following gives the Action a name and specifies it should run on a push to master branch, or pull requests. The workflow_dispatch: allows manual triggering of the workflow from the actions tab or REST API on Github. . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: . Next, we set up the job and what it runs on - here it is the latest ubuntu image. What this means is that each time this runs we get a self-contained environment in which to carry out the steps of the workflow. You can see below that we can sprinkle our action files with conditional if: ... statements allowing control flow in our actions. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest . Now it&#39;s time to define the actual steps that we want to execute. The key part here is that we give the step a name and then either have a run: or uses: key for the given step. If we have a run: key we can execute statements as if we were in the bash terminal (this being a ubuntu image) such as cd .. or ls etc. . The use: allows us to use specific actions created by others - of which there are many on the GitHub Actions marketplace. You can pass arguments by using the with: key to these prespecified actions. . The first step below just checks to see if we have set up a deploy key, something which should be done when we first set up a blog with fastpages: . - name: Check if secret exists if: github.event_name == &#39;push&#39; run: | if [ -z &quot;$deploy_key&quot; ] then echo &quot;You do not have a secret named SSH_DEPLOY_KEY. This means you did not follow the setup instructions carefully. Please try setting up your repo again with the right secrets.&quot; exit 1; fi env: deploy_key: $ . Next, it uses a marketplace action to checkout our blog repository and then uses a local action directory to convert our notebooks to markdown files. This conversion puts the markdown in the _posts/ directory of the blog repository. . - name: Copy Repository Contents uses: actions/checkout@main with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The final few steps build and deploy the Jekyll blog. First, the _site/ directory is cleared and then rebuilt using a Docker container image. The CNAME step is only pertinent if you have a custom domain name, which this blog doesn&#39;t so can be ignored. . Finally the last action peaceiris/actions-gh-pages@v3 takes the fresh _site/ and deploys this to a Github Page, which allows hosting of static sites. Easy right? . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;jekyll build -V --strict_front_matter --trace&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site . How to make the custom header image . To make the custom header image I wrote a little script of which the main execution function is shown below in the collapsable code fold. It&#39;s not too important to understand it for this explanation, just that it is simple python script that produces a matplotlib plot. The main thing is that it reads the markdown posts in the _posts/ folder to get the titles and dates published, and then it can construct the plot with a list of the titles and time since today&#39;s date expressed in days. . As blog posts only ever get put in the _posts/ directory at site build time and the resulting markdown is not committed to the repository I realised I can slot this script into the current ci.yaml workflow. . def line_plot(post_titles: list, deltas: list, cutoff=-30): &quot;&quot;&quot;Creates the line plot in the XKCD style. Args: post_titles (list): list of post titles deltas (list): list of time since comparison time cutoff (int, optional): cutoff point. Defaults to -30. Returns: tuple : plt.fig, plt.ax &quot;&quot;&quot; with plt.xkcd(): f, ax = plt.subplots(1, 1) x_vals = [i for i in range(cutoff, 0, 1)] y_counts = Counter(deltas) y_vals = [y_counts[x] for x in x_vals] ax.plot(x_vals, y_vals) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.set_title(&#39;WELCOME TO THE BLOG!&#39;, fontweight=&#39;bold&#39;, y=1.05) max_y_val = max(y_vals) ax.set_ylim(top=max_y_val+0.3+0.2*len(y_counts)) ax.set_yticks(range(0, max_y_val+1)) ax.set_xlabel(&#39;Days ago...&#39;) ax.set_ylabel(&#39;Number of posts&#39;) arrowprops = dict( arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;angle3,angleA=0,angleB=90&quot;) titles_arr = np.array(post_titles) deltas_arr = np.array(deltas) x_offset = 2 y_offset = max_y_val + 0.2*len(y_counts) + 0.2 for x, count in y_counts.items(): post_titles = titles_arr[deltas_arr == x] ax.annotate(&#39; n+ n&#39;.join(post_titles), xy=(x, count), xytext=(x+x_offset, y_offset), arrowprops=arrowprops) y_offset -= 0.2 ax.text(x=1.2, y=0.6, s=&#39;Days since posting...&#39;, transform=ax.transAxes) bbox_props = dict(boxstyle=&quot;round&quot;, fc=&quot;white&quot;, ec=&quot;black&quot;) if len(deltas) &gt; 0: last_post_days = str(abs(deltas[-1]) - 1) else: last_post_days = &#39;+&#39; + str(abs(cutoff)) ax.text(1.3, 0.4, last_post_days, bbox=bbox_props, transform=ax.transAxes, fontsize=24) exclam = &#39;&quot;Nice!&quot;&#39; if int(last_post_days) &lt; 14 else &#39;&quot;UH OH!&quot;&#39; ax.text(1.28, 0.1, exclam, transform=ax.transAxes, rotation=25) f.set_size_inches(12, 2.5) return f, ax . . Here are the changes I made to make that work. . Firstly I had to add another trigger to the action. It&#39;s fine that it runs on push as I want new blog posts to be added to the header as they are published. But I also want it to update every day so that the time since publishing updates daily for each post. Handily, GitHub actions have a schedule trigger where we can set a schedule for the action to occur using Cron expressions. . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: schedule: - cron: &quot;0 1 * * *&quot; . This Cron expression means the action will happen every day at 1 am. Check this handy website if you need to write Cron expression https://crontab.guru/. . Next, I slot the script and associated setup in between - name: convert notebooks and word docs to posts and the - name: setup directories for Jekyll build steps. . To use the script we need to have python and the required libraries installed. Luckily this is again quite easy using the actions/setup-python action followed by a run: pip install -r requirements.txt step. These are the - name: setup python and - name: Install dependencies steps, respectively. . The next step installs the humor sans font that is required by plt.xkcd and then finally we run the script in the - name: make-header step. . - name: convert notebooks and word docs to posts uses: ./_action_files - name: setup python uses: actions/setup-python@v2 with: python-version: 3.7 - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Install humor sans font run: | sudo apt-get update -y sudo apt-get install -y fonts-humor-sans rm -rf ~/.cache/matplotlib - name: make-header run: | python scripts/make_header.py - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . . The very last thing we have to change is the conditional if: in the final - name: Deploy step to also run this when the github.event_name is equal to &#39;schedule&#39;. Viola! . - name: Deploy if: ( github.event_name == &#39;push&#39; ) || ( github.event_name == &#39;schedule&#39; ) uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site . Here is the new ci.yaml workflow file in full: . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: schedule: - cron: &quot;0 1 * * *&quot; jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: - name: Check if secret exists if: ( github.event_name == &#39;push&#39; ) || ( github.event_name == &#39;schedule&#39; ) run: | if [ -z &quot;$deploy_key&quot; ] then echo &quot;You do not have a secret named SSH_DEPLOY_KEY. This means you did not follow the setup instructions carefully. Please try setting up your repo again with the right secrets.&quot; exit 1; fi env: deploy_key: $ - name: Copy Repository Contents uses: actions/checkout@main with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files - name: setup python uses: actions/setup-python@v2 with: python-version: 3.7 - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Install humor sans font run: | sudo apt-get update -y sudo apt-get install -y fonts-humor-sans rm -rf ~/.cache/matplotlib - name: make-header run: | python scripts/make_header.py - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;jekyll build -V --strict_front_matter --trace&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : - name: Deploy if: ( github.event_name == &#39;push&#39; ) || ( github.event_name == &#39;schedule&#39; ) uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site .",
            "url": "https://jc639.github.io/blog/github/ci/actions/2021/03/06/github_actions.html",
            "relUrl": "/github/ci/actions/2021/03/06/github_actions.html",
            "date": "  Mar 6, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Hello",
            "content": "Testing 1, 2, 3... Is anyone out there? Hello to anyone reading this and welcome to this blog. I intend to write here semi-regularly covering various topics in the data science space, as well as deep learning. . I am a data scientist at an FTSE 100 company, and do lots of various things covering a wide range of data science techniques but not so much deep learning. . I really enjoy deep learning and as you can see in the past I have done some projects in the past using deep learning techniques, such as: . handwriting recognition | character RNN | album cover genre image recognition | . But I want to do more stuff regularly and plan to use this blog to do that. The writing here will be a consolidation of any learning and is mainly for me, but if anyone else finds it useful that is great! .",
            "url": "https://jc639.github.io/blog/2021/02/27/Hello.html",
            "relUrl": "/2021/02/27/Hello.html",
            "date": "  Feb 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a data scientist at a FTSE 100 company. This blog represents my personal opinions and musings on data science. . I decided to start blogging after completing the fastai deep learning course, where they suggest writing can help you solidify your learnings. Two reasons given for starting a blog stood out to me: . Its like a resume but better! | Helps you learn | . I am keenly interested in deep learning but dont get much opportunity to apply the skills I learn at my job unfortunately. I want to use this blog to help with learning but also document those skills that might not be evident from my CV. If someone finds something useful that I have written as part of that, then thats great! .",
          "url": "https://jc639.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ sitemap.xml | absolute_url }} | .",
          "url": "https://jc639.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}