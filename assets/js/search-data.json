{
  
    
        "post0": {
            "title": "Making Pet Faces",
            "content": ". In this post, I will be using the cropped pets data from a previous post with a deep-convolutional generative adversarial network (DCGAN) to generate new pet photos. This will closely follow the Pytorch tutorial but I&#39;ll play around with some refactoring and examine how a generative model can go from a vector of random noise to something with image dimensions (3 x H x W). . Dataset . Before we get to the modelling we need some data as usual. Here we are creating a generative model. Generative models use random noise as an input which seems a bit strange at first, but the goal of the generator model is to use this noise input to generate fake images that fool a discriminator model. . We don&#39;t really have targets in the true sense of supervised learning here but we want to be able to get batches of real images while training. The discriminator model&#39;s goal is to learn to determine between these fake and real images, as one gets better the other model has to improve to beat the other one, and in this way, they are adversarial networks! . The dataset here returns a batch of real image tensors and a batch of random noise. The noise is used with the generator to create a batch of fake images, which the discriminator has to classify as such in contrast to the real image batch.. . Right lets create a PyTorch dataset suitable for this task. As per usual we need to implement a __len__ and __getitem__ after inheriting from torch.utils.data.Dataset. . import torch from torch.utils.data import Dataset from PIL import Image class PetFeatures(Dataset): def __init__(self, files, noise_len, transforms=None): &quot;&quot;&quot;Initialises Dataset :files: list of files to actual images :noise: length of the noise vector :transform: composed transforms &quot;&quot;&quot; self.files = files self.noise_len = noise_len self.transforms = transforms def __len__(self): return len(self.files) def __getitem__(self, idx): random_noise = torch.randn(self.noise_len, 1, 1) img = Image.open(files[idx]).convert(&#39;RGB&#39;) if self.transforms: img = self.transforms(img) return random_noise.float(), img.float() . I have uploaded the cropped images from the previous post into the Colab environment so let&#39;s grab them and have a look at a few. . from pathlib import Path files = list(Path(&#39;cropped_images&#39;).glob(&#39;*&#39;)) print(files[:2]) . [PosixPath(&#39;cropped_images/test_crop_Russian_Blue_97.jpg&#39;), PosixPath(&#39;cropped_images/english_cocker_spaniel_111.jpg&#39;)] . Let&#39;s also compose some transforms that we will pass to the PetFeatures dataset. Here we are resizing the smallest edge to 128px and then taking a centre crop of 128x128px. Using a RandomHorizontalFlip the image will be randomly mirrored. . from torchvision.transforms import (Compose, ToTensor, Normalize, Resize, CenterCrop, RandomHorizontalFlip) img_transforms = Compose( [Resize(128), CenterCrop(128), RandomHorizontalFlip(), ToTensor(), Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))] ) . Let&#39;s double-check that the dataset returns a random noise vector each time by getting the items at the first index twice. . pet_ds = PetFeatures(files=files, noise_len=100, transforms=img_transforms) noise, img_tensor = pet_ds[0] print(&#39;Noise shape: &#39;, noise.shape) print(&#39;Image tensor shape: &#39;, img_tensor.shape, &#39; n&#39;) print(&#39;First value of noise = &#39;, noise[0].item()) print(&#39;Getting same item again...&#39;) noise, _ = pet_ds[0] print(&#39;First value of noise this time =&#39;, noise[0].item()) . Noise shape: torch.Size([100, 1, 1]) Image tensor shape: torch.Size([3, 128, 128]) First value of noise = -1.0172492265701294 Getting same item again... First value of noise this time = 1.2278869152069092 . Generator . The generator model has to go from the random noise vector to a tensor that is the same shape as the img_tensor above. . Here is the original implementation from the paper. . . Let&#39;s examine what&#39;s going on here. . We first go from an input of shape 100 x 1 x 1 to a tensor of shape 1024 x 4 x 4. Then from this to 512 x 8 x 8. Then from this to 256 x 16 x 16... and so until the final layers generates something with a shape 3 x 64 x 64 which we can consider an image tensor with 3 channels and height/width of 64. . Do you see the pattern here? The first and final layers are special cases but all layers in between halve the number of features but double the height x width. . How do we actually upsample though? . Transposed convolution . A transposed convolution is a convolutional layer that is capable of upsampling. Let&#39;s see how this works with a stride of 2 and padding of 1 in a 2d example. Although we specify these parameters they aren&#39;t used in the usual way. The stride and padding are what we would use to go from output -&gt; input dimensions in the downsampling operation if we were performing a convolution. . To do a transposed convolution follow these steps . Insert in between each row and column of the input stride - 1 zeros | Pad this modified tensor with padding - kernel - 1 number of zeros on the outside | Then do a standard convolution with this modified input but always with a stride 1 | Below we have the input and kernel tensor for this example: . input = torch.randn(4, 4) print(&#39;Input tensor: n&#39;, input) kernel = torch.randn(2, 2) print(&#39; nKernel: n&#39;, kernel) . Input tensor: tensor([[ 0.4502, -0.0037, -1.7679, 1.1498], [ 0.2148, -1.1413, -0.0395, 0.0553], [ 0.9184, 0.5407, -0.0092, 0.4064], [ 0.8990, 0.9067, 1.3046, 0.0442]]) Kernel: tensor([[ 0.4862, -0.4461], [-0.9699, -0.5816]]) . First add the zeros 2 - 1 (stride=2) between rows and columns: . add_rows = torch.zeros(7, 4) add_rows[0, :] = input[0, :] for i in range(1, 4): add_rows[i*2, :] = input[i, :] add_columns = torch.zeros(7, 7) add_columns[:, 0] = add_rows[:, 0] for i in range(1, 4): add_columns[:, i*2] = add_rows[:, i] print(&#39;Modified tensor: n&#39;, add_columns) . Modified tensor: tensor([[ 0.4502, 0.0000, -0.0037, 0.0000, -1.7679, 0.0000, 1.1498], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.2148, 0.0000, -1.1413, 0.0000, -0.0395, 0.0000, 0.0553], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.9184, 0.0000, 0.5407, 0.0000, -0.0092, 0.0000, 0.4064], [ 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000], [ 0.8990, 0.0000, 0.9067, 0.0000, 1.3046, 0.0000, 0.0442]]) . Add any padding which is calculated with kernel - padding - 1, in this example this is 2 - 1 - 1 - so not anything to do. . Then we do an ordinary convolution but always with a stride of 1, where we slide the kernel tensor over the modified input and sum up the elements, like so: . output = torch.zeros(6, 6) for i in range(6): for j in range(6): output[i, j] = (add_columns[i: i+2, j:j+2] * kernel).sum() print(&#39;Output: n&#39;, output) . Output: tensor([[ 0.2189, 0.0017, -0.0018, 0.7886, -0.8594, -0.5129], [-0.2083, 0.6638, 1.1069, 0.0230, 0.0383, -0.0322], [ 0.1044, 0.5091, -0.5548, 0.0176, -0.0192, -0.0247], [-0.8907, -0.3145, -0.5244, 0.0054, 0.0090, -0.2364], [ 0.4465, -0.2412, 0.2629, 0.0041, -0.0045, -0.1813], [-0.8719, -0.5274, -0.8794, -0.7588, -1.2653, -0.0257]]) . So as you can see we have gone from the input of shape 4 x 4 to an output of 6 x 6. The kernel is the weight in this operation, and should with updates in training get better at helping us make good upsample operations. . The model . So let&#39;s turn to the model. The transposed convolution layers are followed by the BatchNorm layer and then a rectified linear layer, except for the last layer which has a Tanh layer to get the output in the range -1 to 1. Let&#39;s first implement that. . from torch import nn def generate_post_conv(n_out, last=False): &quot;&quot;&quot;Layers to follow the transposed convolution. If last=True then Tanh, else Bn followed by Relu&quot;&quot;&quot; if last: return nn.Tanh() return nn.Sequential(nn.BatchNorm2d(n_out), nn.ReLU(True)) . With this in place, we can make a generic deconvolution block. All we need to specify the number of features in, and the number of features we want out. The defaults for the kernel size, stride and padding are set so that by default we go from a shape of n_inputs, h, w to n_ouputs, 2h, 2w. . class DeconvBlock(nn.Module): &quot;&quot;&quot; &quot;Deconvolution &quot; Block using Transposed Convolution&quot;&quot;&quot; def __init__(self, n_in, n_out, k_size=4, stride=2, padding=1, bias=False, last=False): super(DeconvBlock, self).__init__() self.block = nn.Sequential( nn.ConvTranspose2d(n_in, n_out, k_size, stride, padding, bias=False), generate_post_conv(n_out, last=last) ) def forward(self, x): return self.block(x) . We now have everything we need to create a Generator model. Let&#39;s generically implement this so we can specify the number of features in, the number of features as a result of the first transposed layer and then the final output image size. . Here the img_size has to be of $2^n$ where $n$ is in the range $[3-10]$. Similarly as we half the feature size on each transposed convolution step the n_feats has also to be of a power of $2$. . By default the model returns something of size 3x128x128, we only need to specify the dimension of the random noise vector. . class Generator(nn.Module): def __init__(self, n_in, n_feats=1024, img_size=128): &quot;&quot;&quot;Generator model for DCGAN :n_in: shape of random noise input :n_feats: number of features as the output of the first transposed convolution layer. Each subsequent layer will have half the number of features of the previous :img_size: size of the final 3 channel tensor output - should be in size 2^n but no more than 1024&quot;&quot;&quot; super(Generator, self).__init__() self.n_in = n_in self.n_feats = n_feats out_sizes = [2**i for i in range(3, 11)] if img_size not in out_sizes: raise Exception(f&#39;img_size must be 2^n and in range {out_sizes[0]}-{out_sizes[-1]}&#39;) layers = [DeconvBlock(n_in=n_in, n_out=n_feats, stride=1, padding=0)] for i, size in enumerate(out_sizes): in_feats = int(n_feats/(2**i)) out_feats = int(n_feats/(2**(i+1))) if size == img_size: layers += [DeconvBlock(n_in=in_feats, n_out=3, last=True)] break layers += [DeconvBlock(n_in=in_feats, n_out=out_feats)] self.gen = nn.Sequential(*layers) def forward(self, input): return self.gen(input) . Let&#39;s use a single item batch and iterate through the children of the model, and have a look at the size of each output. Click the show button if you want to see the output. . generator = Generator(n_in=100) with torch.no_grad(): out = torch.randn(1, 100, 1, 1) for children in generator.children(): for c in children.children(): print(&#39;Step: &#39;, c) out = c(out) print(&#39;Output shape: &#39;, out.shape) . Step: DeconvBlock( (block): Sequential( (0): ConvTranspose2d(100, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU(inplace=True) ) ) ) Output shape: torch.Size([1, 1024, 4, 4]) Step: DeconvBlock( (block): Sequential( (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU(inplace=True) ) ) ) Output shape: torch.Size([1, 512, 8, 8]) Step: DeconvBlock( (block): Sequential( (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU(inplace=True) ) ) ) Output shape: torch.Size([1, 256, 16, 16]) Step: DeconvBlock( (block): Sequential( (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU(inplace=True) ) ) ) Output shape: torch.Size([1, 128, 32, 32]) Step: DeconvBlock( (block): Sequential( (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): Sequential( (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (1): ReLU(inplace=True) ) ) ) Output shape: torch.Size([1, 64, 64, 64]) Step: DeconvBlock( (block): Sequential( (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): Tanh() ) ) Output shape: torch.Size([1, 3, 128, 128]) . . Additionally, we want to use the weight initialization as suggested in the DCGAN paper. Here we want to avoid applying the weight normalization to ConvBlock which is defined below as it is actually built on an nn.Sequential object and doesn&#39;t have a single m.weight.data hence the and classname.find(&#39;ConvBlock&#39;) == -1:. . def weights_init(m): &quot;&quot;&quot;Weight initialisation as suggested in paper&quot;&quot;&quot; classname = m.__class__.__name__ if classname.find(&#39;Conv&#39;) != -1 and classname.find(&#39;ConvBlock&#39;) == -1: nn.init.normal_(m.weight.data, 0.0, 0.02) elif classname.find(&#39;BatchNorm&#39;) != -1: nn.init.normal_(m.weight.data, 1.0, 0.02) nn.init.constant_(m.bias.data, 0) generator = generator.apply(weights_init) . Discriminator . The discriminator model is composed of several convolution layers consisting of conv followed by batchnorm and then leaky ReLU activation. . The first layer does not have batchnorm as per the paper: &quot;Directly applying batchnorm to all layers, however, resulted in sample oscillation and model instability. This was avoided by not applying batchnorm to the generator output layer and the discriminator input layer.&quot; . Let&#39;s do what we did with the Generator and refactor the code from the PyTorch tutorial a little bit so we can build these a bit more dynamically. . The ConvBlock below implements this general pattern of convolution followed by batchnorm (if not in the first layer) and leaky ReLU. . class ConvBlock(nn.Module): def __init__(self, n_in, n_out, k_size=4, stride=2, padding=1, first=False): &quot;&quot;&quot;Convolution followed by batchnorm, if not first layer, and leaky ReLU :n_in: number of input features :n_out: number of output features :k_size: kernel size of convolution :stride: stride of convolution :padding: amount of padding to apply :first: Boolean indicating whether its first layer in the Discriminator &quot;&quot;&quot; super(ConvBlock, self).__init__() layers = [nn.Conv2d(n_in, n_out, k_size, stride, padding, bias=False)] if not first: layers += [nn.BatchNorm2d(n_out)] layers += [nn.LeakyReLU(0.2, inplace=True)] self.conv_block = nn.Sequential(*layers) def forward(self, x): return self.conv_block(x) . Here the Discriminator is implemented quite similarly to the Generator but we are doubling the number of features with each ConvBlock and halving the height and weight of the features tensors. . class Discriminator(nn.Module): def __init__(self, start_feats, img_size=128): &quot;&quot;&quot;Discrimator Model for DCGAN :start_feats: the number of features after the first conv operation :img_size: size of input image &quot;&quot;&quot; super(Discriminator, self).__init__() self.img_size = img_size self.start_feats = start_feats layers = [ConvBlock(n_in=3, n_out=start_feats, first=True)] out_sizes = [2**i for i in range(3, 11)][::-1] start_ind = out_sizes.index(img_size) + 1 for i, sizes in enumerate(out_sizes[start_ind:]): in_feats = int(start_feats*(2**i)) out_feats = int(start_feats*(2**(i+1))) layers += [ConvBlock(n_in=in_feats, n_out=out_feats)] layers += [nn.Conv2d(out_feats, 1, 4, 1, 0, bias=False), nn.Sigmoid()] self.disc = nn.Sequential(*layers) def forward(self, input): return self.disc(input) . Using the old output tensor from running through the Generator let&#39;s run this through the discriminator and see the structure of the network and the size of the tensor after each step. Click on the show button if you want to see the output. . discriminator = Discriminator(32, img_size=128) discriminator = discriminator.apply(weights_init) d_out = out.detach() with torch.no_grad(): for children in discriminator.children(): for c in children.children(): print(c) d_out = c(d_out) print(d_out.shape) . ConvBlock( (conv_block): Sequential( (0): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): LeakyReLU(negative_slope=0.2, inplace=True) ) ) torch.Size([1, 32, 64, 64]) ConvBlock( (conv_block): Sequential( (0): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.2, inplace=True) ) ) torch.Size([1, 64, 32, 32]) ConvBlock( (conv_block): Sequential( (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.2, inplace=True) ) ) torch.Size([1, 128, 16, 16]) ConvBlock( (conv_block): Sequential( (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.2, inplace=True) ) ) torch.Size([1, 256, 8, 8]) ConvBlock( (conv_block): Sequential( (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): LeakyReLU(negative_slope=0.2, inplace=True) ) ) torch.Size([1, 512, 4, 4]) Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False) torch.Size([1, 1, 1, 1]) Sigmoid() torch.Size([1, 1, 1, 1]) . . Training . We have everything we need so now it is time to get training. . First, let&#39;s create a transform to unnormalize the output tensors of the generator. In this way, we can convert the output tensors from the generator back to something with a range between 0-1 and plot it as an image. . class UnNormalize(object): def __init__(self, mean, std): self.mean = mean self.std = std def __call__(self, tensor): &quot;&quot;&quot;Reverses the normalization transform. tensor: Tensor image of size (C, H, W) to be unnormalized. &quot;&quot;&quot; for t, m, s in zip(tensor, self.mean, self.std): t.mul_(s).add_(m) # The normalize code -&gt; t.sub_(m).div_(s) return tensor . With this in place let&#39;s set up a class that can handle the training of the GAN. . The training is a two-step process. First, update the discriminator after calling .backward on the output of the loss function; first with the output of the discriminator from the real image batch and then again with fake images from the generator. Rather than giving the labels of the real images as 1, label smoothing by setting the target at 0.9 helps prevent the discriminator from getting overconfident and can stabilise training, which is what is implemented here. Notice the generated.detach() in the update_discriminator, this detaches the tensor from the computational graph and will prevent gradients from being backpropagated from this variable - i.e. we don&#39;t want to compute gradients for the generator at this step. The update_discriminator uses the discriminator&#39;s optimizer and finally performs a .step to update the weights of the discriminator. . Secondly, update the generator - here in the aptly named update_generator. Here we use the generated fake image batch again - this time without .detach and get the discriminator to classify again. This time we pass to the loss function the label of 1 - we want the generator to get better at producing images and fool the discriminator. We call .backward on the output of the loss function and call the .step of the generator&#39;s optimizer to update its weights. . import matplotlib.pyplot as plt unnorm = UnNormalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) class GanTrainer: def __init__(self, generator, discriminator, opt_G, opt_D, dataloader, loss_func, fixed_noise, device, unnorm=unnorm): self.generator = generator self.discriminator = discriminator self.opt_G = opt_G self.opt_D = opt_D self.dataloader = dataloader self.loss_func = loss_func self.fixed_noise = fixed_noise self.device = device self.unnorm = unnorm def train(self, num_epochs, save_img=True, img_path=&#39;drive/MyDrive/Colab Notebooks/GAN_images/&#39;, show_img=True, show_every=5, run_number=1): &quot;&quot;&quot;Train the Generator and the Discriminator :num_epochs: Number of epochs to train for :save_img: Bool, whether to save images created by the generator of the fixed noise batch :img_path: Where to save images :show_img: Whether to show the images :show_every: How often to show the images :run_number: The training run number, if you want to use .train multiple times you can increase this number and not save over original images &quot;&quot;&quot; G_losses = [] D_losses = [] for epoch in range(1, num_epochs+1): for i, data in enumerate(self.dataloader): errD, D_x, D_G_z1, label, generated = self.update_discriminator(batch=data) errG, D_G_z2 = self.update_generator(generated=generated, label=label) if i % 50 == 0: epoch_state = f&#39;[{epoch}/{num_epochs}][{i}/{len(self.dataloader)}] t&#39; loss_state = f&#39;Loss_D: {errD.item():.4f} tLoss_G: {errG.item():.4f} t&#39; disc_sigmoid_state = f&#39;D(x): {D_x:.4f} t D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}&#39; msg = epoch_state + loss_state + disc_sigmoid_state print(msg) G_losses.append(errG.item()) D_losses.append(errD.item()) f, ax = self.plot_fixed_batch() if save_img: f.savefig(img_path + f&#39;/{run_number}_fixed_batch_{epoch}.png&#39;) if show_img: if epoch % show_every == 0: plt.show() return G_losses, D_losses def update_discriminator(self, batch): &quot;&quot;&quot;Does an update of the Discriminator model&quot;&quot;&quot; # real images through discriminator self.discriminator.zero_grad() real_imgs = batch[1].to(self.device) label = torch.full((real_imgs.shape[0],), 0.9, dtype=torch.float, device=self.device) output = self.discriminator(real_imgs).view(-1) errD_real = self.loss_func(output, label) errD_real.backward() D_x = output.mean().item() # noise through gen - detach, then through discriminator noise = batch[0].to(self.device) generated = self.generator(noise) label.fill_(0.) output = self.discriminator(generated.detach()).view(-1) errD_fake = self.loss_func(output, label) errD_fake.backward() D_G_z1 = output.mean().item() errD = errD_real + errD_fake self.opt_D.step() return errD, D_x, D_G_z1, label, generated def update_generator(self, generated, label): &quot;&quot;&quot;Does an update of the generator model&quot;&quot;&quot; self.generator.zero_grad() label.fill_(1.0) output = self.discriminator(generated).view(-1) errG = self.loss_func(output, label) errG.backward() D_G_z2 = output.mean().item() self.opt_G.step() return errG, D_G_z2 def plot_fixed_batch(self): &quot;&quot;&quot;Returns a figure, axes object of a plot of the fixed noise batch&quot;&quot;&quot; self.generator.eval() with torch.no_grad(): f, ax = plt.subplots(1, self.fixed_noise.shape[0]) gen_imgs = generator(self.fixed_noise) for i in range(fixed_noise.shape[0]): unnormed_tensor = self.unnorm(gen_imgs[i]) ax[i].imshow(unnormed_tensor.permute(1, 2, 0).to(&#39;cpu&#39;)) ax[i].set_axis_off() self.generator.train() return f, ax . That&#39;s it, let&#39;s train. We just need to set up a few things so we can initialise the GanTrainer class. Here I have set the learning rates slightly different for the discriminator and the generator. This idea was introduced in GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. This is another trick to help achieve stable GAN training. . from torch.utils.data import DataLoader dev = torch.device(&#39;cuda&#39;) if torch.cuda.is_available() else &#39;cpu&#39; generator = generator.to(dev) discriminator = discriminator.to(dev) loss_func = nn.BCELoss() disc_lr = 0.0002 gen_lr = 0.0002 opt_D = torch.optim.Adam(discriminator.parameters(), lr=disc_lr, betas=(0.5, 0.999)) opt_G = torch.optim.Adam(generator.parameters(), lr=gen_lr, betas=(0.5, 0.999)) train_dl = DataLoader(pet_ds, batch_size=16) fixed_noise = torch.randn(3, 100, 1, 1, device=dev) gan_trainer = GanTrainer(generator=generator, discriminator=discriminator, opt_G=opt_G, opt_D=opt_D, dataloader=train_dl, loss_func=loss_func, fixed_noise=fixed_noise, device=dev) . OK, OK, let&#39;s actually train. The output of this step is below the collapsable fold as there is a lot of printing the metrics. You can see images that are being generated with the fixed random noise batch every 5 epochs. . G_loss, D_loss = gan_trainer.train(200, show_every=5, run_number=4) . [1/200][0/462] Loss_D: 1.8670 Loss_G: 8.6389 D(x): 0.6746 D(G(z)): 0.6488 / 0.0004 [1/200][50/462] Loss_D: 0.4819 Loss_G: 5.5368 D(x): 0.8278 D(G(z)): 0.0170 / 0.0056 [1/200][100/462] Loss_D: 1.4428 Loss_G: 9.6282 D(x): 0.2889 D(G(z)): 0.0013 / 0.0001 [1/200][150/462] Loss_D: 2.2278 Loss_G: 2.2850 D(x): 0.1525 D(G(z)): 0.0480 / 0.1280 [1/200][200/462] Loss_D: 0.7549 Loss_G: 2.6972 D(x): 0.6939 D(G(z)): 0.1026 / 0.0854 [1/200][250/462] Loss_D: 0.8043 Loss_G: 3.8881 D(x): 0.7256 D(G(z)): 0.1195 / 0.0270 [1/200][300/462] Loss_D: 0.8939 Loss_G: 3.7618 D(x): 0.5992 D(G(z)): 0.1110 / 0.0338 [1/200][350/462] Loss_D: 0.7129 Loss_G: 5.0684 D(x): 0.6610 D(G(z)): 0.0922 / 0.0072 [1/200][400/462] Loss_D: 0.8754 Loss_G: 5.4185 D(x): 0.8423 D(G(z)): 0.2979 / 0.0085 [1/200][450/462] Loss_D: 0.5770 Loss_G: 4.1807 D(x): 0.9320 D(G(z)): 0.1488 / 0.0233 [2/200][0/462] Loss_D: 0.6943 Loss_G: 3.3643 D(x): 0.6992 D(G(z)): 0.0747 / 0.0419 [2/200][50/462] Loss_D: 1.1936 Loss_G: 2.5681 D(x): 0.3712 D(G(z)): 0.0793 / 0.0937 [2/200][100/462] Loss_D: 1.1270 Loss_G: 2.1307 D(x): 0.5237 D(G(z)): 0.2933 / 0.1309 [2/200][150/462] Loss_D: 2.3559 Loss_G: 7.6391 D(x): 0.6214 D(G(z)): 0.7789 / 0.0006 [2/200][200/462] Loss_D: 1.2263 Loss_G: 1.7357 D(x): 0.4699 D(G(z)): 0.2318 / 0.2070 [2/200][250/462] Loss_D: 0.5937 Loss_G: 2.3744 D(x): 0.7900 D(G(z)): 0.1613 / 0.1063 [2/200][300/462] Loss_D: 0.8083 Loss_G: 2.9033 D(x): 0.6046 D(G(z)): 0.1284 / 0.0681 [2/200][350/462] Loss_D: 0.6768 Loss_G: 3.9184 D(x): 0.7257 D(G(z)): 0.1807 / 0.0252 [2/200][400/462] Loss_D: 0.7576 Loss_G: 4.0411 D(x): 0.6952 D(G(z)): 0.1743 / 0.0253 [2/200][450/462] Loss_D: 1.0516 Loss_G: 4.2003 D(x): 0.7725 D(G(z)): 0.3912 / 0.0247 [3/200][0/462] Loss_D: 1.5063 Loss_G: 2.3706 D(x): 0.2858 D(G(z)): 0.0235 / 0.1136 [3/200][50/462] Loss_D: 1.0732 Loss_G: 1.3415 D(x): 0.5566 D(G(z)): 0.3163 / 0.3075 [3/200][100/462] Loss_D: 2.1848 Loss_G: 2.4400 D(x): 0.1599 D(G(z)): 0.0211 / 0.1128 [3/200][150/462] Loss_D: 1.2369 Loss_G: 1.7466 D(x): 0.4923 D(G(z)): 0.2779 / 0.2279 [3/200][200/462] Loss_D: 1.1225 Loss_G: 2.3727 D(x): 0.6165 D(G(z)): 0.3730 / 0.1130 [3/200][250/462] Loss_D: 1.0981 Loss_G: 1.6655 D(x): 0.6458 D(G(z)): 0.3434 / 0.2190 [3/200][300/462] Loss_D: 1.2645 Loss_G: 3.0914 D(x): 0.4397 D(G(z)): 0.2833 / 0.0526 [3/200][350/462] Loss_D: 1.9508 Loss_G: 1.9111 D(x): 0.1561 D(G(z)): 0.0265 / 0.1589 [3/200][400/462] Loss_D: 0.9481 Loss_G: 2.9072 D(x): 0.7808 D(G(z)): 0.3163 / 0.0724 [3/200][450/462] Loss_D: 1.0259 Loss_G: 1.9496 D(x): 0.5081 D(G(z)): 0.0922 / 0.1990 [4/200][0/462] Loss_D: 1.1604 Loss_G: 1.7051 D(x): 0.5047 D(G(z)): 0.3078 / 0.2049 [4/200][50/462] Loss_D: 1.2462 Loss_G: 1.0694 D(x): 0.3468 D(G(z)): 0.1375 / 0.3694 [4/200][100/462] Loss_D: 0.8333 Loss_G: 1.9118 D(x): 0.5484 D(G(z)): 0.1541 / 0.1661 [4/200][150/462] Loss_D: 0.8121 Loss_G: 2.8404 D(x): 0.4995 D(G(z)): 0.0466 / 0.0782 [4/200][200/462] Loss_D: 1.3289 Loss_G: 2.4531 D(x): 0.4954 D(G(z)): 0.2845 / 0.1197 [4/200][250/462] Loss_D: 1.7176 Loss_G: 2.2742 D(x): 0.4865 D(G(z)): 0.5164 / 0.1228 [4/200][300/462] Loss_D: 0.8589 Loss_G: 1.7486 D(x): 0.5364 D(G(z)): 0.1054 / 0.1996 [4/200][350/462] Loss_D: 1.3374 Loss_G: 2.5640 D(x): 0.4294 D(G(z)): 0.2294 / 0.1009 [4/200][400/462] Loss_D: 0.9700 Loss_G: 2.3946 D(x): 0.5632 D(G(z)): 0.2656 / 0.1239 [4/200][450/462] Loss_D: 1.3382 Loss_G: 2.0225 D(x): 0.6278 D(G(z)): 0.4855 / 0.1551 [5/200][0/462] Loss_D: 1.6852 Loss_G: 2.5237 D(x): 0.7745 D(G(z)): 0.6752 / 0.0945 [5/200][50/462] Loss_D: 0.8753 Loss_G: 3.2484 D(x): 0.7793 D(G(z)): 0.3331 / 0.0489 [5/200][100/462] Loss_D: 1.1803 Loss_G: 1.6460 D(x): 0.3555 D(G(z)): 0.0854 / 0.2019 [5/200][150/462] Loss_D: 1.6676 Loss_G: 1.1572 D(x): 0.4998 D(G(z)): 0.5523 / 0.3380 [5/200][200/462] Loss_D: 1.0694 Loss_G: 1.9831 D(x): 0.5481 D(G(z)): 0.3087 / 0.1542 [5/200][250/462] Loss_D: 1.5668 Loss_G: 2.0810 D(x): 0.6198 D(G(z)): 0.5405 / 0.1486 [5/200][300/462] Loss_D: 1.0471 Loss_G: 2.5423 D(x): 0.7466 D(G(z)): 0.4209 / 0.0908 [5/200][350/462] Loss_D: 1.2370 Loss_G: 1.8869 D(x): 0.4552 D(G(z)): 0.2043 / 0.1829 [5/200][400/462] Loss_D: 1.0284 Loss_G: 2.0504 D(x): 0.4998 D(G(z)): 0.1796 / 0.1648 [5/200][450/462] Loss_D: 1.3587 Loss_G: 1.6351 D(x): 0.3458 D(G(z)): 0.2007 / 0.2177 . [6/200][0/462] Loss_D: 1.3039 Loss_G: 1.2361 D(x): 0.5249 D(G(z)): 0.3938 / 0.3077 [6/200][50/462] Loss_D: 0.8378 Loss_G: 2.1117 D(x): 0.5814 D(G(z)): 0.1918 / 0.1365 [6/200][100/462] Loss_D: 1.2230 Loss_G: 1.4384 D(x): 0.4280 D(G(z)): 0.2715 / 0.2752 [6/200][150/462] Loss_D: 0.9794 Loss_G: 3.0619 D(x): 0.7660 D(G(z)): 0.3756 / 0.0557 [6/200][200/462] Loss_D: 1.1789 Loss_G: 2.7534 D(x): 0.7260 D(G(z)): 0.4685 / 0.0750 [6/200][250/462] Loss_D: 1.6039 Loss_G: 4.0566 D(x): 0.8870 D(G(z)): 0.6629 / 0.0202 [6/200][300/462] Loss_D: 1.4542 Loss_G: 1.6224 D(x): 0.5686 D(G(z)): 0.5108 / 0.2245 [6/200][350/462] Loss_D: 1.4139 Loss_G: 2.4366 D(x): 0.7285 D(G(z)): 0.5804 / 0.1021 [6/200][400/462] Loss_D: 1.2913 Loss_G: 2.1848 D(x): 0.4161 D(G(z)): 0.1984 / 0.1430 [6/200][450/462] Loss_D: 1.3119 Loss_G: 1.9213 D(x): 0.4905 D(G(z)): 0.3782 / 0.1760 [7/200][0/462] Loss_D: 1.2077 Loss_G: 1.6791 D(x): 0.5368 D(G(z)): 0.3833 / 0.2072 [7/200][50/462] Loss_D: 1.2718 Loss_G: 2.2153 D(x): 0.6281 D(G(z)): 0.4559 / 0.1307 [7/200][100/462] Loss_D: 1.2810 Loss_G: 1.0467 D(x): 0.4914 D(G(z)): 0.3749 / 0.3771 [7/200][150/462] Loss_D: 1.2500 Loss_G: 1.8879 D(x): 0.5182 D(G(z)): 0.3554 / 0.1767 [7/200][200/462] Loss_D: 0.9493 Loss_G: 2.7774 D(x): 0.5589 D(G(z)): 0.2131 / 0.0735 [7/200][250/462] Loss_D: 1.8384 Loss_G: 1.3687 D(x): 0.5524 D(G(z)): 0.6567 / 0.2806 [7/200][300/462] Loss_D: 1.3450 Loss_G: 1.7490 D(x): 0.4753 D(G(z)): 0.3717 / 0.1992 [7/200][350/462] Loss_D: 0.8192 Loss_G: 2.1988 D(x): 0.7605 D(G(z)): 0.3106 / 0.1303 [7/200][400/462] Loss_D: 0.7244 Loss_G: 2.4652 D(x): 0.6365 D(G(z)): 0.1188 / 0.0896 [7/200][450/462] Loss_D: 1.1441 Loss_G: 1.7709 D(x): 0.4696 D(G(z)): 0.2597 / 0.1920 [8/200][0/462] Loss_D: 1.4466 Loss_G: 1.4975 D(x): 0.4016 D(G(z)): 0.3608 / 0.2567 [8/200][50/462] Loss_D: 1.4400 Loss_G: 1.2850 D(x): 0.5421 D(G(z)): 0.4771 / 0.2946 [8/200][100/462] Loss_D: 1.5068 Loss_G: 1.2810 D(x): 0.3075 D(G(z)): 0.2384 / 0.2901 [8/200][150/462] Loss_D: 0.9096 Loss_G: 1.8655 D(x): 0.5703 D(G(z)): 0.1950 / 0.1835 [8/200][200/462] Loss_D: 1.2830 Loss_G: 2.1007 D(x): 0.6188 D(G(z)): 0.4496 / 0.1461 [8/200][250/462] Loss_D: 1.3165 Loss_G: 1.2443 D(x): 0.3752 D(G(z)): 0.2390 / 0.3189 [8/200][300/462] Loss_D: 1.5289 Loss_G: 1.1318 D(x): 0.4364 D(G(z)): 0.4175 / 0.3580 [8/200][350/462] Loss_D: 1.3281 Loss_G: 1.9169 D(x): 0.4810 D(G(z)): 0.3940 / 0.1672 [8/200][400/462] Loss_D: 1.3952 Loss_G: 1.4839 D(x): 0.3913 D(G(z)): 0.2670 / 0.2471 [8/200][450/462] Loss_D: 0.8835 Loss_G: 2.4122 D(x): 0.7720 D(G(z)): 0.3585 / 0.1030 [9/200][0/462] Loss_D: 1.4895 Loss_G: 1.4892 D(x): 0.4233 D(G(z)): 0.3703 / 0.2595 [9/200][50/462] Loss_D: 1.2868 Loss_G: 1.6261 D(x): 0.6540 D(G(z)): 0.4817 / 0.2229 [9/200][100/462] Loss_D: 1.4709 Loss_G: 1.8222 D(x): 0.3207 D(G(z)): 0.1888 / 0.1904 [9/200][150/462] Loss_D: 1.1243 Loss_G: 1.8687 D(x): 0.5627 D(G(z)): 0.3025 / 0.1852 [9/200][200/462] Loss_D: 1.2965 Loss_G: 1.6727 D(x): 0.4568 D(G(z)): 0.3304 / 0.2135 [9/200][250/462] Loss_D: 1.5598 Loss_G: 1.1946 D(x): 0.4139 D(G(z)): 0.4359 / 0.3343 [9/200][300/462] Loss_D: 1.2688 Loss_G: 1.3622 D(x): 0.4055 D(G(z)): 0.2927 / 0.2771 [9/200][350/462] Loss_D: 1.4438 Loss_G: 1.9383 D(x): 0.4712 D(G(z)): 0.4574 / 0.1711 [9/200][400/462] Loss_D: 1.4112 Loss_G: 1.8867 D(x): 0.3547 D(G(z)): 0.2665 / 0.1949 [9/200][450/462] Loss_D: 1.4936 Loss_G: 1.4402 D(x): 0.4927 D(G(z)): 0.4319 / 0.2632 [10/200][0/462] Loss_D: 1.5137 Loss_G: 1.4714 D(x): 0.4241 D(G(z)): 0.3775 / 0.2521 [10/200][50/462] Loss_D: 1.2611 Loss_G: 1.4000 D(x): 0.4917 D(G(z)): 0.3615 / 0.2912 [10/200][100/462] Loss_D: 1.3000 Loss_G: 1.2685 D(x): 0.3995 D(G(z)): 0.2802 / 0.3061 [10/200][150/462] Loss_D: 0.9584 Loss_G: 1.9259 D(x): 0.5935 D(G(z)): 0.2553 / 0.1752 [10/200][200/462] Loss_D: 1.1797 Loss_G: 1.8130 D(x): 0.5271 D(G(z)): 0.3230 / 0.1882 [10/200][250/462] Loss_D: 1.3300 Loss_G: 1.8021 D(x): 0.4623 D(G(z)): 0.3246 / 0.1888 [10/200][300/462] Loss_D: 1.4454 Loss_G: 1.3791 D(x): 0.4214 D(G(z)): 0.4003 / 0.2678 [10/200][350/462] Loss_D: 1.4186 Loss_G: 1.4200 D(x): 0.4941 D(G(z)): 0.4348 / 0.2630 [10/200][400/462] Loss_D: 1.2593 Loss_G: 1.7319 D(x): 0.4521 D(G(z)): 0.3143 / 0.2027 [10/200][450/462] Loss_D: 0.9143 Loss_G: 2.2794 D(x): 0.7639 D(G(z)): 0.3832 / 0.1162 . [11/200][0/462] Loss_D: 1.3702 Loss_G: 1.3733 D(x): 0.3462 D(G(z)): 0.1518 / 0.2864 [11/200][50/462] Loss_D: 1.2326 Loss_G: 1.9031 D(x): 0.4172 D(G(z)): 0.2550 / 0.1854 [11/200][100/462] Loss_D: 1.1991 Loss_G: 1.6425 D(x): 0.4582 D(G(z)): 0.2546 / 0.2170 [11/200][150/462] Loss_D: 1.3557 Loss_G: 1.8417 D(x): 0.4589 D(G(z)): 0.3780 / 0.1733 [11/200][200/462] Loss_D: 1.0941 Loss_G: 2.1394 D(x): 0.5052 D(G(z)): 0.2928 / 0.1355 [11/200][250/462] Loss_D: 1.2402 Loss_G: 2.2177 D(x): 0.6563 D(G(z)): 0.4528 / 0.1325 [11/200][300/462] Loss_D: 1.2409 Loss_G: 1.6203 D(x): 0.5066 D(G(z)): 0.3522 / 0.2232 [11/200][350/462] Loss_D: 0.9956 Loss_G: 2.8362 D(x): 0.7561 D(G(z)): 0.3602 / 0.0703 [11/200][400/462] Loss_D: 0.7457 Loss_G: 2.7503 D(x): 0.7090 D(G(z)): 0.2121 / 0.0814 [11/200][450/462] Loss_D: 1.1013 Loss_G: 3.2499 D(x): 0.8270 D(G(z)): 0.4539 / 0.0523 [12/200][0/462] Loss_D: 1.6689 Loss_G: 1.5257 D(x): 0.2867 D(G(z)): 0.1372 / 0.2544 [12/200][50/462] Loss_D: 0.9179 Loss_G: 2.0631 D(x): 0.5686 D(G(z)): 0.1967 / 0.1565 [12/200][100/462] Loss_D: 1.0662 Loss_G: 1.8523 D(x): 0.5497 D(G(z)): 0.2701 / 0.1722 [12/200][150/462] Loss_D: 0.8551 Loss_G: 2.6322 D(x): 0.7547 D(G(z)): 0.3060 / 0.0846 [12/200][200/462] Loss_D: 0.9207 Loss_G: 2.7431 D(x): 0.7691 D(G(z)): 0.3259 / 0.0931 [12/200][250/462] Loss_D: 0.8563 Loss_G: 2.8794 D(x): 0.7314 D(G(z)): 0.2646 / 0.0628 [12/200][300/462] Loss_D: 1.1435 Loss_G: 1.5009 D(x): 0.5036 D(G(z)): 0.2904 / 0.2444 [12/200][350/462] Loss_D: 1.6396 Loss_G: 1.5806 D(x): 0.2386 D(G(z)): 0.1424 / 0.2508 [12/200][400/462] Loss_D: 0.9598 Loss_G: 3.1445 D(x): 0.6305 D(G(z)): 0.2626 / 0.0689 [12/200][450/462] Loss_D: 1.4637 Loss_G: 2.3412 D(x): 0.5612 D(G(z)): 0.4797 / 0.1121 [13/200][0/462] Loss_D: 1.4902 Loss_G: 1.7784 D(x): 0.4472 D(G(z)): 0.3778 / 0.1944 [13/200][50/462] Loss_D: 0.9983 Loss_G: 1.8885 D(x): 0.6323 D(G(z)): 0.3517 / 0.1609 [13/200][100/462] Loss_D: 1.4466 Loss_G: 1.2801 D(x): 0.3522 D(G(z)): 0.2265 / 0.3153 [13/200][150/462] Loss_D: 0.9439 Loss_G: 2.4536 D(x): 0.6338 D(G(z)): 0.2938 / 0.1085 [13/200][200/462] Loss_D: 1.0629 Loss_G: 2.3126 D(x): 0.4993 D(G(z)): 0.2177 / 0.1179 [13/200][250/462] Loss_D: 1.0834 Loss_G: 2.1292 D(x): 0.6806 D(G(z)): 0.3939 / 0.1334 [13/200][300/462] Loss_D: 1.1769 Loss_G: 1.3536 D(x): 0.4993 D(G(z)): 0.2716 / 0.2794 [13/200][350/462] Loss_D: 1.0604 Loss_G: 2.7498 D(x): 0.6848 D(G(z)): 0.3966 / 0.0731 [13/200][400/462] Loss_D: 1.1779 Loss_G: 2.0714 D(x): 0.4643 D(G(z)): 0.2247 / 0.1537 [13/200][450/462] Loss_D: 1.1140 Loss_G: 2.2672 D(x): 0.6449 D(G(z)): 0.3779 / 0.1153 [14/200][0/462] Loss_D: 1.1399 Loss_G: 1.9104 D(x): 0.5579 D(G(z)): 0.2944 / 0.1670 [14/200][50/462] Loss_D: 0.8153 Loss_G: 2.0588 D(x): 0.6759 D(G(z)): 0.2456 / 0.1505 [14/200][100/462] Loss_D: 0.9028 Loss_G: 1.9958 D(x): 0.6266 D(G(z)): 0.2662 / 0.1595 [14/200][150/462] Loss_D: 0.7402 Loss_G: 2.2562 D(x): 0.7074 D(G(z)): 0.2127 / 0.1209 [14/200][200/462] Loss_D: 0.6827 Loss_G: 3.2715 D(x): 0.6903 D(G(z)): 0.1514 / 0.0513 [14/200][250/462] Loss_D: 1.2516 Loss_G: 1.8746 D(x): 0.5833 D(G(z)): 0.4227 / 0.1660 [14/200][300/462] Loss_D: 0.9570 Loss_G: 2.0257 D(x): 0.5789 D(G(z)): 0.2567 / 0.1621 [14/200][350/462] Loss_D: 1.2987 Loss_G: 2.3806 D(x): 0.6094 D(G(z)): 0.4217 / 0.1234 [14/200][400/462] Loss_D: 0.9715 Loss_G: 1.6769 D(x): 0.5004 D(G(z)): 0.1226 / 0.2157 [14/200][450/462] Loss_D: 1.1422 Loss_G: 3.1016 D(x): 0.8896 D(G(z)): 0.5113 / 0.0525 [15/200][0/462] Loss_D: 0.9914 Loss_G: 2.2781 D(x): 0.5582 D(G(z)): 0.2474 / 0.1326 [15/200][50/462] Loss_D: 0.7752 Loss_G: 2.6070 D(x): 0.7272 D(G(z)): 0.2266 / 0.0915 [15/200][100/462] Loss_D: 1.0444 Loss_G: 1.7620 D(x): 0.5039 D(G(z)): 0.1611 / 0.2005 [15/200][150/462] Loss_D: 1.2759 Loss_G: 1.6312 D(x): 0.3412 D(G(z)): 0.1031 / 0.2352 [15/200][200/462] Loss_D: 0.7767 Loss_G: 3.7063 D(x): 0.8011 D(G(z)): 0.2422 / 0.0365 [15/200][250/462] Loss_D: 1.0096 Loss_G: 1.7392 D(x): 0.5210 D(G(z)): 0.2359 / 0.2071 [15/200][300/462] Loss_D: 1.0571 Loss_G: 1.8910 D(x): 0.5073 D(G(z)): 0.1619 / 0.1753 [15/200][350/462] Loss_D: 1.3036 Loss_G: 1.5070 D(x): 0.3304 D(G(z)): 0.1103 / 0.2525 [15/200][400/462] Loss_D: 0.9890 Loss_G: 1.6234 D(x): 0.4752 D(G(z)): 0.1496 / 0.2168 [15/200][450/462] Loss_D: 1.0966 Loss_G: 3.5047 D(x): 0.8995 D(G(z)): 0.4869 / 0.0350 . [16/200][0/462] Loss_D: 1.0053 Loss_G: 1.6144 D(x): 0.5048 D(G(z)): 0.1995 / 0.2309 [16/200][50/462] Loss_D: 1.0408 Loss_G: 2.6230 D(x): 0.7303 D(G(z)): 0.3538 / 0.0978 [16/200][100/462] Loss_D: 0.9737 Loss_G: 1.9805 D(x): 0.6936 D(G(z)): 0.3349 / 0.1540 [16/200][150/462] Loss_D: 1.0993 Loss_G: 1.9375 D(x): 0.4577 D(G(z)): 0.1679 / 0.1650 [16/200][200/462] Loss_D: 1.0045 Loss_G: 2.3621 D(x): 0.4428 D(G(z)): 0.0907 / 0.1157 [16/200][250/462] Loss_D: 1.1260 Loss_G: 2.0576 D(x): 0.7583 D(G(z)): 0.3998 / 0.1699 [16/200][300/462] Loss_D: 0.8386 Loss_G: 2.0859 D(x): 0.5819 D(G(z)): 0.1702 / 0.1388 [16/200][350/462] Loss_D: 1.0793 Loss_G: 1.6439 D(x): 0.5310 D(G(z)): 0.2784 / 0.2136 [16/200][400/462] Loss_D: 0.9904 Loss_G: 1.8990 D(x): 0.5567 D(G(z)): 0.2215 / 0.1779 [16/200][450/462] Loss_D: 1.4549 Loss_G: 3.6075 D(x): 0.8766 D(G(z)): 0.6121 / 0.0353 [17/200][0/462] Loss_D: 1.3410 Loss_G: 1.9000 D(x): 0.3849 D(G(z)): 0.1931 / 0.1924 [17/200][50/462] Loss_D: 1.0656 Loss_G: 1.9977 D(x): 0.5544 D(G(z)): 0.2334 / 0.1978 [17/200][100/462] Loss_D: 1.1955 Loss_G: 1.2725 D(x): 0.4341 D(G(z)): 0.2011 / 0.3253 [17/200][150/462] Loss_D: 1.0141 Loss_G: 2.6804 D(x): 0.6228 D(G(z)): 0.2931 / 0.1049 [17/200][200/462] Loss_D: 0.9698 Loss_G: 3.6380 D(x): 0.7429 D(G(z)): 0.3498 / 0.0324 [17/200][250/462] Loss_D: 1.3170 Loss_G: 2.1492 D(x): 0.4420 D(G(z)): 0.2349 / 0.1303 [17/200][300/462] Loss_D: 1.0204 Loss_G: 2.5299 D(x): 0.7248 D(G(z)): 0.3602 / 0.0933 [17/200][350/462] Loss_D: 1.2456 Loss_G: 2.1745 D(x): 0.5699 D(G(z)): 0.3697 / 0.1333 [17/200][400/462] Loss_D: 1.1353 Loss_G: 1.9230 D(x): 0.5172 D(G(z)): 0.3043 / 0.1640 [17/200][450/462] Loss_D: 1.0683 Loss_G: 2.6349 D(x): 0.7214 D(G(z)): 0.3927 / 0.0892 [18/200][0/462] Loss_D: 1.2024 Loss_G: 2.1779 D(x): 0.5283 D(G(z)): 0.2784 / 0.1550 [18/200][50/462] Loss_D: 0.9581 Loss_G: 2.4829 D(x): 0.6720 D(G(z)): 0.3478 / 0.0877 [18/200][100/462] Loss_D: 1.1766 Loss_G: 1.6628 D(x): 0.4637 D(G(z)): 0.2368 / 0.2263 [18/200][150/462] Loss_D: 0.9352 Loss_G: 1.8655 D(x): 0.7040 D(G(z)): 0.3521 / 0.1724 [18/200][200/462] Loss_D: 1.0281 Loss_G: 2.5924 D(x): 0.5311 D(G(z)): 0.2207 / 0.0811 [18/200][250/462] Loss_D: 0.9860 Loss_G: 2.1949 D(x): 0.7259 D(G(z)): 0.3545 / 0.1446 [18/200][300/462] Loss_D: 1.0937 Loss_G: 1.7850 D(x): 0.5415 D(G(z)): 0.2537 / 0.2065 [18/200][350/462] Loss_D: 0.7703 Loss_G: 2.3400 D(x): 0.7452 D(G(z)): 0.2779 / 0.1125 [18/200][400/462] Loss_D: 1.2240 Loss_G: 1.4494 D(x): 0.3925 D(G(z)): 0.1190 / 0.2816 [18/200][450/462] Loss_D: 1.3675 Loss_G: 2.5187 D(x): 0.7941 D(G(z)): 0.5483 / 0.0983 [19/200][0/462] Loss_D: 1.3555 Loss_G: 1.4434 D(x): 0.3461 D(G(z)): 0.1392 / 0.2757 [19/200][50/462] Loss_D: 1.1217 Loss_G: 3.0421 D(x): 0.7887 D(G(z)): 0.4610 / 0.0585 [19/200][100/462] Loss_D: 1.1927 Loss_G: 1.8257 D(x): 0.5189 D(G(z)): 0.2986 / 0.1861 [19/200][150/462] Loss_D: 0.8473 Loss_G: 2.9246 D(x): 0.8063 D(G(z)): 0.3397 / 0.0614 [19/200][200/462] Loss_D: 0.6154 Loss_G: 3.3622 D(x): 0.7726 D(G(z)): 0.1661 / 0.0428 [19/200][250/462] Loss_D: 1.0989 Loss_G: 1.5944 D(x): 0.5591 D(G(z)): 0.2977 / 0.2368 [19/200][300/462] Loss_D: 0.8397 Loss_G: 2.1598 D(x): 0.5951 D(G(z)): 0.1864 / 0.1539 [19/200][350/462] Loss_D: 1.1183 Loss_G: 2.4401 D(x): 0.7917 D(G(z)): 0.4605 / 0.1076 [19/200][400/462] Loss_D: 1.1696 Loss_G: 2.0401 D(x): 0.5805 D(G(z)): 0.3642 / 0.1520 [19/200][450/462] Loss_D: 0.7836 Loss_G: 2.4300 D(x): 0.7568 D(G(z)): 0.2693 / 0.1029 [20/200][0/462] Loss_D: 1.0213 Loss_G: 1.9847 D(x): 0.4894 D(G(z)): 0.2008 / 0.1512 [20/200][50/462] Loss_D: 1.1043 Loss_G: 1.7291 D(x): 0.4540 D(G(z)): 0.1864 / 0.2100 [20/200][100/462] Loss_D: 1.2791 Loss_G: 1.6017 D(x): 0.4540 D(G(z)): 0.2976 / 0.2375 [20/200][150/462] Loss_D: 1.1095 Loss_G: 2.3878 D(x): 0.6326 D(G(z)): 0.3748 / 0.1069 [20/200][200/462] Loss_D: 0.7880 Loss_G: 3.1328 D(x): 0.6667 D(G(z)): 0.2214 / 0.0547 [20/200][250/462] Loss_D: 1.3014 Loss_G: 2.8769 D(x): 0.7725 D(G(z)): 0.5202 / 0.0731 [20/200][300/462] Loss_D: 1.0614 Loss_G: 1.3510 D(x): 0.4838 D(G(z)): 0.1893 / 0.2767 [20/200][350/462] Loss_D: 1.2114 Loss_G: 2.0079 D(x): 0.4907 D(G(z)): 0.2923 / 0.1515 [20/200][400/462] Loss_D: 1.0916 Loss_G: 1.6588 D(x): 0.4317 D(G(z)): 0.1041 / 0.2373 [20/200][450/462] Loss_D: 0.8114 Loss_G: 2.5545 D(x): 0.7051 D(G(z)): 0.2679 / 0.0952 . [21/200][0/462] Loss_D: 1.0216 Loss_G: 1.9045 D(x): 0.4776 D(G(z)): 0.1697 / 0.1809 [21/200][50/462] Loss_D: 1.0100 Loss_G: 1.9720 D(x): 0.5684 D(G(z)): 0.2620 / 0.1671 [21/200][100/462] Loss_D: 1.0995 Loss_G: 2.2231 D(x): 0.5935 D(G(z)): 0.3638 / 0.1186 [21/200][150/462] Loss_D: 1.1258 Loss_G: 1.6837 D(x): 0.4225 D(G(z)): 0.1587 / 0.2095 [21/200][200/462] Loss_D: 0.9335 Loss_G: 2.2568 D(x): 0.5128 D(G(z)): 0.1629 / 0.1313 [21/200][250/462] Loss_D: 1.3587 Loss_G: 2.4592 D(x): 0.7413 D(G(z)): 0.5416 / 0.0967 [21/200][300/462] Loss_D: 0.9100 Loss_G: 2.2173 D(x): 0.6816 D(G(z)): 0.3048 / 0.1238 [21/200][350/462] Loss_D: 1.0343 Loss_G: 2.1763 D(x): 0.6543 D(G(z)): 0.3375 / 0.1366 [21/200][400/462] Loss_D: 1.0429 Loss_G: 1.7149 D(x): 0.4581 D(G(z)): 0.1169 / 0.2277 [21/200][450/462] Loss_D: 0.7138 Loss_G: 2.1260 D(x): 0.6005 D(G(z)): 0.0988 / 0.1281 [22/200][0/462] Loss_D: 1.4320 Loss_G: 2.1562 D(x): 0.4115 D(G(z)): 0.3063 / 0.1279 [22/200][50/462] Loss_D: 1.3277 Loss_G: 3.1202 D(x): 0.7613 D(G(z)): 0.5249 / 0.0556 [22/200][100/462] Loss_D: 1.3955 Loss_G: 1.4404 D(x): 0.2884 D(G(z)): 0.1140 / 0.2785 [22/200][150/462] Loss_D: 1.0629 Loss_G: 2.9912 D(x): 0.8071 D(G(z)): 0.4440 / 0.0603 [22/200][200/462] Loss_D: 0.9601 Loss_G: 3.1325 D(x): 0.6468 D(G(z)): 0.3209 / 0.0522 [22/200][250/462] Loss_D: 1.0442 Loss_G: 2.3887 D(x): 0.6193 D(G(z)): 0.3518 / 0.1013 [22/200][300/462] Loss_D: 1.2252 Loss_G: 1.8584 D(x): 0.3759 D(G(z)): 0.0725 / 0.1879 [22/200][350/462] Loss_D: 1.2855 Loss_G: 2.9945 D(x): 0.7455 D(G(z)): 0.5303 / 0.0598 [22/200][400/462] Loss_D: 1.0428 Loss_G: 1.5931 D(x): 0.4573 D(G(z)): 0.1289 / 0.2426 [22/200][450/462] Loss_D: 0.9182 Loss_G: 2.1598 D(x): 0.6580 D(G(z)): 0.2718 / 0.1354 [23/200][0/462] Loss_D: 1.0817 Loss_G: 2.5108 D(x): 0.4492 D(G(z)): 0.0741 / 0.1121 [23/200][50/462] Loss_D: 0.8214 Loss_G: 2.8929 D(x): 0.8236 D(G(z)): 0.3347 / 0.0633 [23/200][100/462] Loss_D: 1.2570 Loss_G: 1.7218 D(x): 0.4212 D(G(z)): 0.2521 / 0.2080 [23/200][150/462] Loss_D: 1.0696 Loss_G: 2.9880 D(x): 0.7042 D(G(z)): 0.4009 / 0.0560 [23/200][200/462] Loss_D: 0.7600 Loss_G: 2.7150 D(x): 0.5455 D(G(z)): 0.0551 / 0.0794 [23/200][250/462] Loss_D: 0.9785 Loss_G: 2.2672 D(x): 0.6006 D(G(z)): 0.2651 / 0.1199 [23/200][300/462] Loss_D: 0.7619 Loss_G: 2.5338 D(x): 0.6312 D(G(z)): 0.1587 / 0.0993 [23/200][350/462] Loss_D: 1.0227 Loss_G: 4.0339 D(x): 0.8554 D(G(z)): 0.4524 / 0.0240 [23/200][400/462] Loss_D: 1.1442 Loss_G: 2.6795 D(x): 0.7208 D(G(z)): 0.4229 / 0.0786 [23/200][450/462] Loss_D: 0.8723 Loss_G: 2.4814 D(x): 0.6289 D(G(z)): 0.2184 / 0.0982 [24/200][0/462] Loss_D: 1.3686 Loss_G: 3.8902 D(x): 0.6893 D(G(z)): 0.5618 / 0.0227 [24/200][50/462] Loss_D: 0.8301 Loss_G: 2.3752 D(x): 0.5872 D(G(z)): 0.1396 / 0.1045 [24/200][100/462] Loss_D: 1.0099 Loss_G: 2.3444 D(x): 0.5544 D(G(z)): 0.2330 / 0.1267 [24/200][150/462] Loss_D: 0.9152 Loss_G: 1.6289 D(x): 0.5375 D(G(z)): 0.1694 / 0.2089 [24/200][200/462] Loss_D: 0.8532 Loss_G: 2.9330 D(x): 0.5697 D(G(z)): 0.1432 / 0.0634 [24/200][250/462] Loss_D: 1.0163 Loss_G: 2.5368 D(x): 0.5761 D(G(z)): 0.2434 / 0.0921 [24/200][300/462] Loss_D: 0.8876 Loss_G: 2.7952 D(x): 0.6679 D(G(z)): 0.2647 / 0.0801 [24/200][350/462] Loss_D: 1.0355 Loss_G: 2.9154 D(x): 0.7987 D(G(z)): 0.4159 / 0.0572 [24/200][400/462] Loss_D: 0.9187 Loss_G: 1.4494 D(x): 0.5268 D(G(z)): 0.1083 / 0.2639 [24/200][450/462] Loss_D: 0.9311 Loss_G: 3.8667 D(x): 0.7246 D(G(z)): 0.3358 / 0.0266 [25/200][0/462] Loss_D: 0.9548 Loss_G: 2.5062 D(x): 0.6556 D(G(z)): 0.3047 / 0.0881 [25/200][50/462] Loss_D: 1.2400 Loss_G: 2.6877 D(x): 0.6250 D(G(z)): 0.4425 / 0.0801 [25/200][100/462] Loss_D: 0.8453 Loss_G: 2.6029 D(x): 0.7602 D(G(z)): 0.2936 / 0.0892 [25/200][150/462] Loss_D: 0.8940 Loss_G: 3.0116 D(x): 0.7410 D(G(z)): 0.2949 / 0.0581 [25/200][200/462] Loss_D: 0.7985 Loss_G: 2.7458 D(x): 0.6536 D(G(z)): 0.2095 / 0.0721 [25/200][250/462] Loss_D: 1.0814 Loss_G: 3.3341 D(x): 0.8430 D(G(z)): 0.4339 / 0.0459 [25/200][300/462] Loss_D: 0.9558 Loss_G: 1.8265 D(x): 0.5128 D(G(z)): 0.1566 / 0.1830 [25/200][350/462] Loss_D: 1.9117 Loss_G: 4.5864 D(x): 0.8708 D(G(z)): 0.7178 / 0.0173 [25/200][400/462] Loss_D: 0.8641 Loss_G: 1.9393 D(x): 0.5276 D(G(z)): 0.1007 / 0.1578 [25/200][450/462] Loss_D: 1.1162 Loss_G: 2.8717 D(x): 0.6845 D(G(z)): 0.4005 / 0.0649 . [26/200][0/462] Loss_D: 1.0037 Loss_G: 2.0771 D(x): 0.5164 D(G(z)): 0.1612 / 0.1520 [26/200][50/462] Loss_D: 0.8981 Loss_G: 3.3391 D(x): 0.7999 D(G(z)): 0.3614 / 0.0398 [26/200][100/462] Loss_D: 0.9045 Loss_G: 1.8536 D(x): 0.6205 D(G(z)): 0.2666 / 0.1754 [26/200][150/462] Loss_D: 0.9533 Loss_G: 2.4431 D(x): 0.7093 D(G(z)): 0.3392 / 0.1070 [26/200][200/462] Loss_D: 0.7203 Loss_G: 3.5417 D(x): 0.7885 D(G(z)): 0.2317 / 0.0425 [26/200][250/462] Loss_D: 1.0242 Loss_G: 2.8408 D(x): 0.7417 D(G(z)): 0.3766 / 0.0661 [26/200][300/462] Loss_D: 0.8109 Loss_G: 2.8370 D(x): 0.6197 D(G(z)): 0.2011 / 0.0730 [26/200][350/462] Loss_D: 0.9801 Loss_G: 2.7309 D(x): 0.7882 D(G(z)): 0.3529 / 0.0773 [26/200][400/462] Loss_D: 0.9137 Loss_G: 2.7511 D(x): 0.5415 D(G(z)): 0.1257 / 0.0855 [26/200][450/462] Loss_D: 0.7419 Loss_G: 2.4073 D(x): 0.6105 D(G(z)): 0.0880 / 0.1095 [27/200][0/462] Loss_D: 1.2217 Loss_G: 3.9526 D(x): 0.6846 D(G(z)): 0.4329 / 0.0302 [27/200][50/462] Loss_D: 1.1405 Loss_G: 2.9840 D(x): 0.7334 D(G(z)): 0.4179 / 0.0621 [27/200][100/462] Loss_D: 0.8689 Loss_G: 2.1184 D(x): 0.5436 D(G(z)): 0.1371 / 0.1682 [27/200][150/462] Loss_D: 1.1030 Loss_G: 3.4514 D(x): 0.8461 D(G(z)): 0.4992 / 0.0363 [27/200][200/462] Loss_D: 0.5839 Loss_G: 3.0951 D(x): 0.7191 D(G(z)): 0.0947 / 0.0550 [27/200][250/462] Loss_D: 0.7256 Loss_G: 2.4783 D(x): 0.7570 D(G(z)): 0.2448 / 0.0932 [27/200][300/462] Loss_D: 0.7257 Loss_G: 3.3701 D(x): 0.7725 D(G(z)): 0.2285 / 0.0442 [27/200][350/462] Loss_D: 1.0330 Loss_G: 3.0112 D(x): 0.7895 D(G(z)): 0.4053 / 0.0650 [27/200][400/462] Loss_D: 0.5959 Loss_G: 3.6514 D(x): 0.7883 D(G(z)): 0.1423 / 0.0380 [27/200][450/462] Loss_D: 0.6268 Loss_G: 2.6348 D(x): 0.7338 D(G(z)): 0.1142 / 0.0817 [28/200][0/462] Loss_D: 1.0422 Loss_G: 2.0624 D(x): 0.4713 D(G(z)): 0.1570 / 0.1674 [28/200][50/462] Loss_D: 0.8441 Loss_G: 2.8273 D(x): 0.5801 D(G(z)): 0.0776 / 0.0793 [28/200][100/462] Loss_D: 1.0618 Loss_G: 2.1037 D(x): 0.5818 D(G(z)): 0.2339 / 0.1714 [28/200][150/462] Loss_D: 0.8764 Loss_G: 2.3243 D(x): 0.5644 D(G(z)): 0.1726 / 0.1153 [28/200][200/462] Loss_D: 0.6459 Loss_G: 3.7105 D(x): 0.8084 D(G(z)): 0.1784 / 0.0301 [28/200][250/462] Loss_D: 0.9084 Loss_G: 3.3458 D(x): 0.7671 D(G(z)): 0.3123 / 0.0389 [28/200][300/462] Loss_D: 0.8589 Loss_G: 3.0498 D(x): 0.8327 D(G(z)): 0.3492 / 0.0553 [28/200][350/462] Loss_D: 1.3092 Loss_G: 4.1331 D(x): 0.8985 D(G(z)): 0.5337 / 0.0229 [28/200][400/462] Loss_D: 1.0067 Loss_G: 2.8946 D(x): 0.7542 D(G(z)): 0.3447 / 0.0628 [28/200][450/462] Loss_D: 0.6086 Loss_G: 2.6567 D(x): 0.7112 D(G(z)): 0.1141 / 0.0890 [29/200][0/462] Loss_D: 0.9183 Loss_G: 2.6534 D(x): 0.6418 D(G(z)): 0.2680 / 0.0874 [29/200][50/462] Loss_D: 0.9149 Loss_G: 2.5584 D(x): 0.7121 D(G(z)): 0.3132 / 0.0874 [29/200][100/462] Loss_D: 1.1990 Loss_G: 2.9330 D(x): 0.6799 D(G(z)): 0.4375 / 0.0626 [29/200][150/462] Loss_D: 1.0128 Loss_G: 2.4446 D(x): 0.6002 D(G(z)): 0.2815 / 0.0950 [29/200][200/462] Loss_D: 0.5742 Loss_G: 3.7288 D(x): 0.7997 D(G(z)): 0.1429 / 0.0302 [29/200][250/462] Loss_D: 1.5126 Loss_G: 4.4846 D(x): 0.8795 D(G(z)): 0.6013 / 0.0156 [29/200][300/462] Loss_D: 0.8487 Loss_G: 3.1998 D(x): 0.7255 D(G(z)): 0.2908 / 0.0493 [29/200][350/462] Loss_D: 0.9246 Loss_G: 3.3042 D(x): 0.8496 D(G(z)): 0.3567 / 0.0488 [29/200][400/462] Loss_D: 0.7842 Loss_G: 3.0828 D(x): 0.6211 D(G(z)): 0.1366 / 0.0533 [29/200][450/462] Loss_D: 0.6211 Loss_G: 2.9199 D(x): 0.7324 D(G(z)): 0.1377 / 0.0658 [30/200][0/462] Loss_D: 0.7103 Loss_G: 2.9199 D(x): 0.7911 D(G(z)): 0.2367 / 0.0644 [30/200][50/462] Loss_D: 0.9060 Loss_G: 3.1982 D(x): 0.7449 D(G(z)): 0.3090 / 0.0458 [30/200][100/462] Loss_D: 1.0620 Loss_G: 2.7490 D(x): 0.8327 D(G(z)): 0.4127 / 0.0716 [30/200][150/462] Loss_D: 0.7947 Loss_G: 3.8827 D(x): 0.8724 D(G(z)): 0.3056 / 0.0273 [30/200][200/462] Loss_D: 0.7403 Loss_G: 3.6602 D(x): 0.8119 D(G(z)): 0.2438 / 0.0290 [30/200][250/462] Loss_D: 1.1170 Loss_G: 1.9626 D(x): 0.5204 D(G(z)): 0.2657 / 0.1540 [30/200][300/462] Loss_D: 0.9707 Loss_G: 3.8320 D(x): 0.7773 D(G(z)): 0.3582 / 0.0312 [30/200][350/462] Loss_D: 1.1417 Loss_G: 1.2906 D(x): 0.4013 D(G(z)): 0.1208 / 0.3011 [30/200][400/462] Loss_D: 0.7910 Loss_G: 2.3994 D(x): 0.6318 D(G(z)): 0.1265 / 0.1051 [30/200][450/462] Loss_D: 0.5499 Loss_G: 3.2032 D(x): 0.6981 D(G(z)): 0.0580 / 0.0616 . [31/200][0/462] Loss_D: 0.6304 Loss_G: 2.7474 D(x): 0.7023 D(G(z)): 0.1209 / 0.0767 [31/200][50/462] Loss_D: 0.9005 Loss_G: 2.4342 D(x): 0.5652 D(G(z)): 0.1903 / 0.1054 [31/200][100/462] Loss_D: 0.7636 Loss_G: 2.1969 D(x): 0.5518 D(G(z)): 0.0699 / 0.1440 [31/200][150/462] Loss_D: 0.9541 Loss_G: 3.2361 D(x): 0.7805 D(G(z)): 0.3832 / 0.0453 [31/200][200/462] Loss_D: 0.7831 Loss_G: 3.2496 D(x): 0.8226 D(G(z)): 0.2936 / 0.0446 [31/200][250/462] Loss_D: 0.9347 Loss_G: 2.1720 D(x): 0.6416 D(G(z)): 0.2994 / 0.1261 [31/200][300/462] Loss_D: 0.8361 Loss_G: 3.1085 D(x): 0.7040 D(G(z)): 0.2751 / 0.0549 [31/200][350/462] Loss_D: 1.4006 Loss_G: 4.0640 D(x): 0.8987 D(G(z)): 0.5296 / 0.0237 [31/200][400/462] Loss_D: 1.5594 Loss_G: 1.1252 D(x): 0.2513 D(G(z)): 0.0422 / 0.3673 [31/200][450/462] Loss_D: 0.7842 Loss_G: 2.1214 D(x): 0.5490 D(G(z)): 0.0640 / 0.1582 [32/200][0/462] Loss_D: 1.4356 Loss_G: 4.3987 D(x): 0.9437 D(G(z)): 0.5544 / 0.0210 [32/200][50/462] Loss_D: 0.8857 Loss_G: 2.3543 D(x): 0.6887 D(G(z)): 0.2737 / 0.1096 [32/200][100/462] Loss_D: 0.7245 Loss_G: 2.0553 D(x): 0.7149 D(G(z)): 0.1705 / 0.1477 [32/200][150/462] Loss_D: 0.8886 Loss_G: 2.0921 D(x): 0.5792 D(G(z)): 0.1734 / 0.1375 [32/200][200/462] Loss_D: 0.6413 Loss_G: 3.5201 D(x): 0.7359 D(G(z)): 0.1582 / 0.0388 [32/200][250/462] Loss_D: 0.7971 Loss_G: 2.6989 D(x): 0.7036 D(G(z)): 0.2406 / 0.0816 [32/200][300/462] Loss_D: 1.0031 Loss_G: 3.9441 D(x): 0.8898 D(G(z)): 0.4347 / 0.0241 [32/200][350/462] Loss_D: 1.0371 Loss_G: 3.3565 D(x): 0.8280 D(G(z)): 0.4209 / 0.0422 [32/200][400/462] Loss_D: 0.7774 Loss_G: 2.7271 D(x): 0.6230 D(G(z)): 0.1607 / 0.0769 [32/200][450/462] Loss_D: 0.7658 Loss_G: 3.4047 D(x): 0.6103 D(G(z)): 0.1289 / 0.0441 [33/200][0/462] Loss_D: 0.7939 Loss_G: 2.7555 D(x): 0.6743 D(G(z)): 0.1646 / 0.0717 [33/200][50/462] Loss_D: 0.9963 Loss_G: 3.7614 D(x): 0.8071 D(G(z)): 0.3859 / 0.0269 [33/200][100/462] Loss_D: 1.0911 Loss_G: 2.0426 D(x): 0.4207 D(G(z)): 0.0887 / 0.1684 [33/200][150/462] Loss_D: 0.8078 Loss_G: 2.5688 D(x): 0.6673 D(G(z)): 0.2052 / 0.0889 [33/200][200/462] Loss_D: 0.9264 Loss_G: 3.2841 D(x): 0.6351 D(G(z)): 0.2441 / 0.0526 [33/200][250/462] Loss_D: 0.8452 Loss_G: 2.0343 D(x): 0.5241 D(G(z)): 0.1069 / 0.1400 [33/200][300/462] Loss_D: 0.6396 Loss_G: 3.1942 D(x): 0.7405 D(G(z)): 0.1587 / 0.0632 [33/200][350/462] Loss_D: 1.5807 Loss_G: 4.5780 D(x): 0.9799 D(G(z)): 0.5937 / 0.0122 [33/200][400/462] Loss_D: 0.7608 Loss_G: 3.0407 D(x): 0.6489 D(G(z)): 0.1287 / 0.0590 [33/200][450/462] Loss_D: 0.6814 Loss_G: 3.3644 D(x): 0.7394 D(G(z)): 0.1362 / 0.0426 [34/200][0/462] Loss_D: 0.6336 Loss_G: 2.6967 D(x): 0.6674 D(G(z)): 0.1025 / 0.0777 [34/200][50/462] Loss_D: 0.7586 Loss_G: 2.1765 D(x): 0.6371 D(G(z)): 0.1476 / 0.1273 [34/200][100/462] Loss_D: 0.8610 Loss_G: 3.0761 D(x): 0.6986 D(G(z)): 0.2660 / 0.0533 [34/200][150/462] Loss_D: 0.7719 Loss_G: 2.1587 D(x): 0.6167 D(G(z)): 0.1395 / 0.1359 [34/200][200/462] Loss_D: 0.8783 Loss_G: 1.9545 D(x): 0.5360 D(G(z)): 0.1131 / 0.1839 [34/200][250/462] Loss_D: 0.7842 Loss_G: 3.0290 D(x): 0.7402 D(G(z)): 0.2552 / 0.0596 [34/200][300/462] Loss_D: 0.9341 Loss_G: 2.7158 D(x): 0.4952 D(G(z)): 0.0761 / 0.0874 [34/200][350/462] Loss_D: 0.6647 Loss_G: 3.1430 D(x): 0.8449 D(G(z)): 0.2530 / 0.0505 [34/200][400/462] Loss_D: 0.9382 Loss_G: 2.7936 D(x): 0.4768 D(G(z)): 0.0931 / 0.0885 [34/200][450/462] Loss_D: 1.1760 Loss_G: 2.9018 D(x): 0.3617 D(G(z)): 0.0283 / 0.0957 [35/200][0/462] Loss_D: 0.9699 Loss_G: 3.0083 D(x): 0.6970 D(G(z)): 0.3223 / 0.0575 [35/200][50/462] Loss_D: 0.9433 Loss_G: 2.8145 D(x): 0.5278 D(G(z)): 0.1742 / 0.0876 [35/200][100/462] Loss_D: 1.0947 Loss_G: 2.0084 D(x): 0.4769 D(G(z)): 0.1987 / 0.1451 [35/200][150/462] Loss_D: 0.7592 Loss_G: 3.8586 D(x): 0.8307 D(G(z)): 0.2850 / 0.0250 [35/200][200/462] Loss_D: 0.6332 Loss_G: 3.9374 D(x): 0.7826 D(G(z)): 0.1921 / 0.0206 [35/200][250/462] Loss_D: 1.2221 Loss_G: 2.9880 D(x): 0.6390 D(G(z)): 0.3719 / 0.0666 [35/200][300/462] Loss_D: 0.6988 Loss_G: 3.0587 D(x): 0.6130 D(G(z)): 0.1116 / 0.0619 [35/200][350/462] Loss_D: 1.1659 Loss_G: 4.3674 D(x): 0.8730 D(G(z)): 0.4910 / 0.0152 [35/200][400/462] Loss_D: 1.0886 Loss_G: 4.4812 D(x): 0.8660 D(G(z)): 0.4559 / 0.0139 [35/200][450/462] Loss_D: 0.7352 Loss_G: 2.7786 D(x): 0.5989 D(G(z)): 0.0912 / 0.0944 . [36/200][0/462] Loss_D: 0.9028 Loss_G: 3.8232 D(x): 0.7798 D(G(z)): 0.3388 / 0.0282 [36/200][50/462] Loss_D: 0.6696 Loss_G: 2.8762 D(x): 0.6824 D(G(z)): 0.0912 / 0.0778 [36/200][100/462] Loss_D: 1.0339 Loss_G: 2.0827 D(x): 0.5528 D(G(z)): 0.1964 / 0.1625 [36/200][150/462] Loss_D: 0.7765 Loss_G: 2.7283 D(x): 0.7242 D(G(z)): 0.2448 / 0.0944 [36/200][200/462] Loss_D: 1.0205 Loss_G: 4.0461 D(x): 0.7735 D(G(z)): 0.3476 / 0.0210 [36/200][250/462] Loss_D: 0.7504 Loss_G: 3.0090 D(x): 0.6974 D(G(z)): 0.2221 / 0.0589 [36/200][300/462] Loss_D: 0.6954 Loss_G: 2.5910 D(x): 0.7361 D(G(z)): 0.1765 / 0.0900 [36/200][350/462] Loss_D: 0.5924 Loss_G: 2.5012 D(x): 0.7437 D(G(z)): 0.1189 / 0.0947 [36/200][400/462] Loss_D: 1.0337 Loss_G: 2.2586 D(x): 0.5198 D(G(z)): 0.2082 / 0.1255 [36/200][450/462] Loss_D: 0.7581 Loss_G: 2.6206 D(x): 0.6605 D(G(z)): 0.1701 / 0.0834 [37/200][0/462] Loss_D: 1.1455 Loss_G: 4.2312 D(x): 0.8567 D(G(z)): 0.4599 / 0.0223 [37/200][50/462] Loss_D: 0.9566 Loss_G: 1.9023 D(x): 0.5306 D(G(z)): 0.1525 / 0.1811 [37/200][100/462] Loss_D: 1.0477 Loss_G: 2.7643 D(x): 0.5760 D(G(z)): 0.2553 / 0.0756 [37/200][150/462] Loss_D: 0.8110 Loss_G: 3.9996 D(x): 0.8709 D(G(z)): 0.3227 / 0.0208 [37/200][200/462] Loss_D: 0.8006 Loss_G: 2.9863 D(x): 0.6468 D(G(z)): 0.1766 / 0.0644 [37/200][250/462] Loss_D: 0.8074 Loss_G: 2.4240 D(x): 0.6977 D(G(z)): 0.2602 / 0.1001 [37/200][300/462] Loss_D: 0.6316 Loss_G: 3.7406 D(x): 0.8262 D(G(z)): 0.1851 / 0.0286 [37/200][350/462] Loss_D: 0.9120 Loss_G: 2.8936 D(x): 0.7199 D(G(z)): 0.2842 / 0.0678 [37/200][400/462] Loss_D: 0.8855 Loss_G: 3.7390 D(x): 0.7851 D(G(z)): 0.3551 / 0.0274 [37/200][450/462] Loss_D: 1.5753 Loss_G: 6.5238 D(x): 0.8240 D(G(z)): 0.6383 / 0.0023 [38/200][0/462] Loss_D: 0.7702 Loss_G: 1.9420 D(x): 0.5786 D(G(z)): 0.0937 / 0.1710 [38/200][50/462] Loss_D: 1.2476 Loss_G: 2.0336 D(x): 0.3934 D(G(z)): 0.0768 / 0.1648 [38/200][100/462] Loss_D: 0.8141 Loss_G: 2.1939 D(x): 0.5657 D(G(z)): 0.1199 / 0.1384 [38/200][150/462] Loss_D: 0.8884 Loss_G: 3.5969 D(x): 0.8251 D(G(z)): 0.3280 / 0.0342 [38/200][200/462] Loss_D: 0.7617 Loss_G: 3.5539 D(x): 0.8397 D(G(z)): 0.2827 / 0.0387 [38/200][250/462] Loss_D: 0.7910 Loss_G: 2.4404 D(x): 0.6309 D(G(z)): 0.1816 / 0.1010 [38/200][300/462] Loss_D: 0.7043 Loss_G: 2.2591 D(x): 0.5814 D(G(z)): 0.0589 / 0.1499 [38/200][350/462] Loss_D: 0.9015 Loss_G: 2.4569 D(x): 0.6853 D(G(z)): 0.2806 / 0.0952 [38/200][400/462] Loss_D: 0.8236 Loss_G: 2.5883 D(x): 0.6821 D(G(z)): 0.1651 / 0.0974 [38/200][450/462] Loss_D: 0.7173 Loss_G: 2.7117 D(x): 0.5700 D(G(z)): 0.0411 / 0.0993 [39/200][0/462] Loss_D: 0.6497 Loss_G: 3.8993 D(x): 0.7115 D(G(z)): 0.1225 / 0.0308 [39/200][50/462] Loss_D: 0.6643 Loss_G: 3.7696 D(x): 0.8513 D(G(z)): 0.2103 / 0.0278 [39/200][100/462] Loss_D: 0.6804 Loss_G: 2.8363 D(x): 0.6577 D(G(z)): 0.1147 / 0.0757 [39/200][150/462] Loss_D: 0.7217 Loss_G: 3.1680 D(x): 0.7272 D(G(z)): 0.2007 / 0.0521 [39/200][200/462] Loss_D: 0.7093 Loss_G: 3.4881 D(x): 0.7827 D(G(z)): 0.2347 / 0.0406 [39/200][250/462] Loss_D: 0.7017 Loss_G: 2.9087 D(x): 0.7495 D(G(z)): 0.2120 / 0.0625 [39/200][300/462] Loss_D: 0.8011 Loss_G: 2.7917 D(x): 0.6077 D(G(z)): 0.1770 / 0.0768 [39/200][350/462] Loss_D: 0.6785 Loss_G: 2.7403 D(x): 0.7568 D(G(z)): 0.1966 / 0.0758 [39/200][400/462] Loss_D: 0.6679 Loss_G: 3.9288 D(x): 0.8882 D(G(z)): 0.2359 / 0.0251 [39/200][450/462] Loss_D: 0.8640 Loss_G: 2.6269 D(x): 0.5834 D(G(z)): 0.0889 / 0.1093 [40/200][0/462] Loss_D: 1.1861 Loss_G: 4.1623 D(x): 0.8992 D(G(z)): 0.4567 / 0.0228 [40/200][50/462] Loss_D: 1.1170 Loss_G: 3.9984 D(x): 0.8017 D(G(z)): 0.4123 / 0.0283 [40/200][100/462] Loss_D: 0.7789 Loss_G: 2.5341 D(x): 0.6034 D(G(z)): 0.1725 / 0.1079 [40/200][150/462] Loss_D: 0.9605 Loss_G: 3.7200 D(x): 0.7551 D(G(z)): 0.3509 / 0.0346 [40/200][200/462] Loss_D: 0.8563 Loss_G: 4.5423 D(x): 0.9274 D(G(z)): 0.3129 / 0.0129 [40/200][250/462] Loss_D: 0.9113 Loss_G: 1.5900 D(x): 0.4982 D(G(z)): 0.0845 / 0.2362 [40/200][300/462] Loss_D: 0.6476 Loss_G: 4.0725 D(x): 0.8853 D(G(z)): 0.2135 / 0.0242 [40/200][350/462] Loss_D: 0.4495 Loss_G: 3.3973 D(x): 0.8666 D(G(z)): 0.0812 / 0.0515 [40/200][400/462] Loss_D: 0.8567 Loss_G: 2.6268 D(x): 0.5929 D(G(z)): 0.0703 / 0.0933 [40/200][450/462] Loss_D: 0.6084 Loss_G: 3.4415 D(x): 0.7247 D(G(z)): 0.1080 / 0.0452 . [41/200][0/462] Loss_D: 1.0092 Loss_G: 4.0706 D(x): 0.9392 D(G(z)): 0.4258 / 0.0201 [41/200][50/462] Loss_D: 0.8949 Loss_G: 1.7103 D(x): 0.5353 D(G(z)): 0.0970 / 0.2012 [41/200][100/462] Loss_D: 0.6970 Loss_G: 2.1850 D(x): 0.7096 D(G(z)): 0.1704 / 0.1269 [41/200][150/462] Loss_D: 0.6533 Loss_G: 2.7227 D(x): 0.6384 D(G(z)): 0.0816 / 0.0924 [41/200][200/462] Loss_D: 0.6492 Loss_G: 2.7128 D(x): 0.7820 D(G(z)): 0.1984 / 0.0721 [41/200][250/462] Loss_D: 0.8250 Loss_G: 4.3393 D(x): 0.8462 D(G(z)): 0.2972 / 0.0196 [41/200][300/462] Loss_D: 1.0107 Loss_G: 4.0674 D(x): 0.7748 D(G(z)): 0.3845 / 0.0232 [41/200][350/462] Loss_D: 1.1283 Loss_G: 1.6332 D(x): 0.4376 D(G(z)): 0.1495 / 0.2228 [41/200][400/462] Loss_D: 0.8073 Loss_G: 2.8675 D(x): 0.6172 D(G(z)): 0.1322 / 0.0749 [41/200][450/462] Loss_D: 0.5974 Loss_G: 3.9733 D(x): 0.9075 D(G(z)): 0.1746 / 0.0239 [42/200][0/462] Loss_D: 0.9265 Loss_G: 2.1323 D(x): 0.5305 D(G(z)): 0.0940 / 0.1836 [42/200][50/462] Loss_D: 0.8681 Loss_G: 2.6978 D(x): 0.6153 D(G(z)): 0.1783 / 0.0870 [42/200][100/462] Loss_D: 1.1287 Loss_G: 1.5054 D(x): 0.4637 D(G(z)): 0.2195 / 0.2560 [42/200][150/462] Loss_D: 0.6427 Loss_G: 2.8702 D(x): 0.7045 D(G(z)): 0.1174 / 0.0664 [42/200][200/462] Loss_D: 0.7730 Loss_G: 3.0129 D(x): 0.6790 D(G(z)): 0.1798 / 0.0611 [42/200][250/462] Loss_D: 0.5597 Loss_G: 3.7348 D(x): 0.9226 D(G(z)): 0.1668 / 0.0308 [42/200][300/462] Loss_D: 0.5311 Loss_G: 3.3439 D(x): 0.7291 D(G(z)): 0.0365 / 0.0456 [42/200][350/462] Loss_D: 0.6725 Loss_G: 3.8126 D(x): 0.8755 D(G(z)): 0.2288 / 0.0271 [42/200][400/462] Loss_D: 0.7995 Loss_G: 2.4292 D(x): 0.5636 D(G(z)): 0.0977 / 0.1085 [42/200][450/462] Loss_D: 0.7460 Loss_G: 2.9440 D(x): 0.6169 D(G(z)): 0.0926 / 0.0735 [43/200][0/462] Loss_D: 0.7466 Loss_G: 3.2190 D(x): 0.8126 D(G(z)): 0.2449 / 0.0489 [43/200][50/462] Loss_D: 0.5749 Loss_G: 3.4731 D(x): 0.7914 D(G(z)): 0.1180 / 0.0439 [43/200][100/462] Loss_D: 0.7331 Loss_G: 2.3749 D(x): 0.6614 D(G(z)): 0.1338 / 0.1076 [43/200][150/462] Loss_D: 0.5658 Loss_G: 4.1323 D(x): 0.9126 D(G(z)): 0.1844 / 0.0200 [43/200][200/462] Loss_D: 0.7315 Loss_G: 3.5935 D(x): 0.8070 D(G(z)): 0.2404 / 0.0349 [43/200][250/462] Loss_D: 0.7726 Loss_G: 3.2103 D(x): 0.6994 D(G(z)): 0.2181 / 0.0466 [43/200][300/462] Loss_D: 0.6354 Loss_G: 3.7096 D(x): 0.7655 D(G(z)): 0.1154 / 0.0356 [43/200][350/462] Loss_D: 0.7833 Loss_G: 5.0018 D(x): 0.8634 D(G(z)): 0.2774 / 0.0090 [43/200][400/462] Loss_D: 0.7894 Loss_G: 2.2408 D(x): 0.5497 D(G(z)): 0.1052 / 0.1308 [43/200][450/462] Loss_D: 0.6665 Loss_G: 3.5088 D(x): 0.5937 D(G(z)): 0.0421 / 0.0477 [44/200][0/462] Loss_D: 0.9244 Loss_G: 3.9634 D(x): 0.7644 D(G(z)): 0.2961 / 0.0221 [44/200][50/462] Loss_D: 0.4908 Loss_G: 3.6673 D(x): 0.7378 D(G(z)): 0.0460 / 0.0493 [44/200][100/462] Loss_D: 0.6006 Loss_G: 3.1186 D(x): 0.7676 D(G(z)): 0.1272 / 0.0506 [44/200][150/462] Loss_D: 0.7441 Loss_G: 3.3504 D(x): 0.7782 D(G(z)): 0.2540 / 0.0445 [44/200][200/462] Loss_D: 0.8072 Loss_G: 3.4950 D(x): 0.6851 D(G(z)): 0.2402 / 0.0385 [44/200][250/462] Loss_D: 0.6334 Loss_G: 3.8432 D(x): 0.8136 D(G(z)): 0.2027 / 0.0272 [44/200][300/462] Loss_D: 0.5560 Loss_G: 3.1032 D(x): 0.7915 D(G(z)): 0.1226 / 0.0549 [44/200][350/462] Loss_D: 0.6241 Loss_G: 3.8183 D(x): 0.8068 D(G(z)): 0.1359 / 0.0279 [44/200][400/462] Loss_D: 0.6686 Loss_G: 3.4476 D(x): 0.6107 D(G(z)): 0.0475 / 0.0553 [44/200][450/462] Loss_D: 0.7195 Loss_G: 4.9567 D(x): 0.8693 D(G(z)): 0.2436 / 0.0099 [45/200][0/462] Loss_D: 0.8255 Loss_G: 2.6301 D(x): 0.6114 D(G(z)): 0.1136 / 0.1034 [45/200][50/462] Loss_D: 0.7130 Loss_G: 4.2227 D(x): 0.8859 D(G(z)): 0.2409 / 0.0213 [45/200][100/462] Loss_D: 0.6205 Loss_G: 3.0478 D(x): 0.7164 D(G(z)): 0.1272 / 0.0592 [45/200][150/462] Loss_D: 0.8026 Loss_G: 4.1997 D(x): 0.8597 D(G(z)): 0.3188 / 0.0186 [45/200][200/462] Loss_D: 0.9203 Loss_G: 4.1769 D(x): 0.9062 D(G(z)): 0.3672 / 0.0213 [45/200][250/462] Loss_D: 0.9899 Loss_G: 4.8604 D(x): 0.8514 D(G(z)): 0.3820 / 0.0093 [45/200][300/462] Loss_D: 0.5050 Loss_G: 3.4490 D(x): 0.7772 D(G(z)): 0.0791 / 0.0502 [45/200][350/462] Loss_D: 0.6740 Loss_G: 3.1615 D(x): 0.7954 D(G(z)): 0.1926 / 0.0552 [45/200][400/462] Loss_D: 0.6658 Loss_G: 3.4836 D(x): 0.7666 D(G(z)): 0.1632 / 0.0410 [45/200][450/462] Loss_D: 0.5564 Loss_G: 4.1847 D(x): 0.7718 D(G(z)): 0.0850 / 0.0215 . [46/200][0/462] Loss_D: 0.5805 Loss_G: 3.2415 D(x): 0.7903 D(G(z)): 0.1464 / 0.0462 [46/200][50/462] Loss_D: 0.6930 Loss_G: 4.7053 D(x): 0.9116 D(G(z)): 0.2344 / 0.0126 [46/200][100/462] Loss_D: 0.5396 Loss_G: 3.7016 D(x): 0.7896 D(G(z)): 0.1149 / 0.0367 [46/200][150/462] Loss_D: 0.6854 Loss_G: 2.3163 D(x): 0.6343 D(G(z)): 0.0945 / 0.1151 [46/200][200/462] Loss_D: 0.4881 Loss_G: 3.0157 D(x): 0.8257 D(G(z)): 0.0704 / 0.0712 [46/200][250/462] Loss_D: 0.9170 Loss_G: 4.1662 D(x): 0.7938 D(G(z)): 0.3483 / 0.0229 [46/200][300/462] Loss_D: 0.8002 Loss_G: 3.7949 D(x): 0.8350 D(G(z)): 0.2433 / 0.0269 [46/200][350/462] Loss_D: 0.5665 Loss_G: 3.9709 D(x): 0.8424 D(G(z)): 0.1517 / 0.0254 [46/200][400/462] Loss_D: 0.8771 Loss_G: 3.5126 D(x): 0.7113 D(G(z)): 0.2028 / 0.0375 [46/200][450/462] Loss_D: 0.7590 Loss_G: 3.0455 D(x): 0.5821 D(G(z)): 0.0855 / 0.0646 [47/200][0/462] Loss_D: 0.7351 Loss_G: 3.6259 D(x): 0.8264 D(G(z)): 0.2717 / 0.0301 [47/200][50/462] Loss_D: 0.6923 Loss_G: 3.4814 D(x): 0.8168 D(G(z)): 0.2125 / 0.0380 [47/200][100/462] Loss_D: 0.7943 Loss_G: 2.4404 D(x): 0.5878 D(G(z)): 0.0991 / 0.1169 [47/200][150/462] Loss_D: 0.8549 Loss_G: 4.3383 D(x): 0.8834 D(G(z)): 0.3587 / 0.0192 [47/200][200/462] Loss_D: 0.7058 Loss_G: 2.6408 D(x): 0.6903 D(G(z)): 0.1410 / 0.0876 [47/200][250/462] Loss_D: 0.7486 Loss_G: 3.6951 D(x): 0.8330 D(G(z)): 0.2767 / 0.0314 [47/200][300/462] Loss_D: 0.7660 Loss_G: 3.5712 D(x): 0.6565 D(G(z)): 0.1417 / 0.0410 [47/200][350/462] Loss_D: 0.7427 Loss_G: 4.8414 D(x): 0.9411 D(G(z)): 0.2817 / 0.0100 [47/200][400/462] Loss_D: 0.8354 Loss_G: 3.6526 D(x): 0.6477 D(G(z)): 0.2036 / 0.0291 [47/200][450/462] Loss_D: 0.5179 Loss_G: 3.9371 D(x): 0.9234 D(G(z)): 0.1328 / 0.0227 [48/200][0/462] Loss_D: 0.6975 Loss_G: 3.1071 D(x): 0.6246 D(G(z)): 0.0977 / 0.0627 [48/200][50/462] Loss_D: 0.7936 Loss_G: 2.4932 D(x): 0.6910 D(G(z)): 0.1934 / 0.0967 [48/200][100/462] Loss_D: 0.6747 Loss_G: 3.3873 D(x): 0.8144 D(G(z)): 0.1473 / 0.0440 [48/200][150/462] Loss_D: 0.8131 Loss_G: 2.5514 D(x): 0.7267 D(G(z)): 0.2531 / 0.1033 [48/200][200/462] Loss_D: 0.6694 Loss_G: 3.1308 D(x): 0.6211 D(G(z)): 0.0737 / 0.0681 [48/200][250/462] Loss_D: 0.7866 Loss_G: 2.0069 D(x): 0.6319 D(G(z)): 0.1138 / 0.1521 [48/200][300/462] Loss_D: 0.6023 Loss_G: 4.0283 D(x): 0.8392 D(G(z)): 0.1521 / 0.0240 [48/200][350/462] Loss_D: 0.6345 Loss_G: 4.2805 D(x): 0.8842 D(G(z)): 0.2104 / 0.0201 [48/200][400/462] Loss_D: 0.5964 Loss_G: 3.8994 D(x): 0.8721 D(G(z)): 0.1680 / 0.0238 [48/200][450/462] Loss_D: 0.6586 Loss_G: 3.0626 D(x): 0.6974 D(G(z)): 0.1000 / 0.0757 [49/200][0/462] Loss_D: 0.6942 Loss_G: 3.0113 D(x): 0.6927 D(G(z)): 0.1377 / 0.0749 [49/200][50/462] Loss_D: 0.7123 Loss_G: 3.4438 D(x): 0.6064 D(G(z)): 0.0343 / 0.0555 [49/200][100/462] Loss_D: 0.6889 Loss_G: 3.4649 D(x): 0.7300 D(G(z)): 0.1283 / 0.0463 [49/200][150/462] Loss_D: 0.9099 Loss_G: 2.4179 D(x): 0.5413 D(G(z)): 0.1502 / 0.1120 [49/200][200/462] Loss_D: 0.6935 Loss_G: 5.0385 D(x): 0.8784 D(G(z)): 0.2207 / 0.0085 [49/200][250/462] Loss_D: 0.8869 Loss_G: 2.3569 D(x): 0.5947 D(G(z)): 0.1404 / 0.1180 [49/200][300/462] Loss_D: 0.6256 Loss_G: 3.1735 D(x): 0.7502 D(G(z)): 0.1153 / 0.0609 [49/200][350/462] Loss_D: 0.6663 Loss_G: 3.8328 D(x): 0.8237 D(G(z)): 0.2037 / 0.0281 [49/200][400/462] Loss_D: 0.7075 Loss_G: 3.9364 D(x): 0.8627 D(G(z)): 0.2645 / 0.0244 [49/200][450/462] Loss_D: 0.6304 Loss_G: 4.2870 D(x): 0.6953 D(G(z)): 0.0951 / 0.0181 [50/200][0/462] Loss_D: 0.6424 Loss_G: 4.0819 D(x): 0.7964 D(G(z)): 0.1760 / 0.0226 [50/200][50/462] Loss_D: 0.6346 Loss_G: 3.1843 D(x): 0.8197 D(G(z)): 0.1588 / 0.0573 [50/200][100/462] Loss_D: 0.5293 Loss_G: 3.2732 D(x): 0.6930 D(G(z)): 0.0468 / 0.0536 [50/200][150/462] Loss_D: 0.5998 Loss_G: 3.2869 D(x): 0.6864 D(G(z)): 0.0611 / 0.0494 [50/200][200/462] Loss_D: 1.1200 Loss_G: 4.1237 D(x): 0.7208 D(G(z)): 0.4132 / 0.0201 [50/200][250/462] Loss_D: 0.7890 Loss_G: 3.7511 D(x): 0.8728 D(G(z)): 0.3142 / 0.0260 [50/200][300/462] Loss_D: 0.5798 Loss_G: 3.1507 D(x): 0.6911 D(G(z)): 0.0658 / 0.0544 [50/200][350/462] Loss_D: 0.7734 Loss_G: 2.1518 D(x): 0.6238 D(G(z)): 0.1538 / 0.1299 [50/200][400/462] Loss_D: 0.5646 Loss_G: 3.2936 D(x): 0.8060 D(G(z)): 0.1421 / 0.0552 [50/200][450/462] Loss_D: 0.4931 Loss_G: 4.7746 D(x): 0.8011 D(G(z)): 0.0735 / 0.0111 . [51/200][0/462] Loss_D: 0.5637 Loss_G: 4.0167 D(x): 0.7802 D(G(z)): 0.1261 / 0.0301 [51/200][50/462] Loss_D: 0.6018 Loss_G: 3.2595 D(x): 0.7524 D(G(z)): 0.1051 / 0.0448 [51/200][100/462] Loss_D: 0.7891 Loss_G: 3.3713 D(x): 0.7248 D(G(z)): 0.2053 / 0.0451 [51/200][150/462] Loss_D: 0.5854 Loss_G: 2.8208 D(x): 0.6454 D(G(z)): 0.0343 / 0.0760 [51/200][200/462] Loss_D: 0.7402 Loss_G: 3.9086 D(x): 0.7866 D(G(z)): 0.2097 / 0.0269 [51/200][250/462] Loss_D: 0.5290 Loss_G: 3.9616 D(x): 0.8797 D(G(z)): 0.1470 / 0.0234 [51/200][300/462] Loss_D: 0.5205 Loss_G: 3.5981 D(x): 0.8099 D(G(z)): 0.0959 / 0.0375 [51/200][350/462] Loss_D: 0.8857 Loss_G: 5.0892 D(x): 0.9415 D(G(z)): 0.3140 / 0.0071 [51/200][400/462] Loss_D: 0.8330 Loss_G: 2.1139 D(x): 0.5885 D(G(z)): 0.0734 / 0.1348 [51/200][450/462] Loss_D: 0.4998 Loss_G: 4.5129 D(x): 0.9052 D(G(z)): 0.1172 / 0.0156 [52/200][0/462] Loss_D: 0.6411 Loss_G: 3.9492 D(x): 0.7372 D(G(z)): 0.1467 / 0.0266 [52/200][50/462] Loss_D: 0.4896 Loss_G: 3.2422 D(x): 0.8443 D(G(z)): 0.0946 / 0.0449 [52/200][100/462] Loss_D: 0.7061 Loss_G: 2.0017 D(x): 0.5867 D(G(z)): 0.0634 / 0.1916 [52/200][150/462] Loss_D: 0.7344 Loss_G: 3.6480 D(x): 0.7941 D(G(z)): 0.2308 / 0.0362 [52/200][200/462] Loss_D: 0.5139 Loss_G: 4.2374 D(x): 0.8222 D(G(z)): 0.0880 / 0.0230 [52/200][250/462] Loss_D: 0.7063 Loss_G: 3.6558 D(x): 0.7761 D(G(z)): 0.2118 / 0.0294 [52/200][300/462] Loss_D: 0.8424 Loss_G: 3.6488 D(x): 0.6568 D(G(z)): 0.1713 / 0.0361 [52/200][350/462] Loss_D: 0.5441 Loss_G: 3.9273 D(x): 0.8163 D(G(z)): 0.1084 / 0.0296 [52/200][400/462] Loss_D: 0.5882 Loss_G: 3.8648 D(x): 0.7184 D(G(z)): 0.0575 / 0.0307 [52/200][450/462] Loss_D: 0.4787 Loss_G: 3.8823 D(x): 0.8238 D(G(z)): 0.0832 / 0.0284 [53/200][0/462] Loss_D: 0.6754 Loss_G: 2.9856 D(x): 0.8549 D(G(z)): 0.1935 / 0.0572 [53/200][50/462] Loss_D: 1.0288 Loss_G: 2.1165 D(x): 0.4964 D(G(z)): 0.0703 / 0.1606 [53/200][100/462] Loss_D: 0.4879 Loss_G: 3.5452 D(x): 0.8216 D(G(z)): 0.0723 / 0.0346 [53/200][150/462] Loss_D: 1.0184 Loss_G: 6.1354 D(x): 0.8709 D(G(z)): 0.4100 / 0.0032 [53/200][200/462] Loss_D: 0.7397 Loss_G: 4.1083 D(x): 0.7917 D(G(z)): 0.2108 / 0.0221 [53/200][250/462] Loss_D: 0.6349 Loss_G: 3.6634 D(x): 0.8209 D(G(z)): 0.1815 / 0.0325 [53/200][300/462] Loss_D: 0.7302 Loss_G: 4.2915 D(x): 0.9106 D(G(z)): 0.2257 / 0.0195 [53/200][350/462] Loss_D: 0.4444 Loss_G: 4.2767 D(x): 0.8361 D(G(z)): 0.0656 / 0.0199 [53/200][400/462] Loss_D: 0.6963 Loss_G: 4.7553 D(x): 0.9193 D(G(z)): 0.2300 / 0.0127 [53/200][450/462] Loss_D: 0.5538 Loss_G: 3.6787 D(x): 0.7418 D(G(z)): 0.0513 / 0.0332 [54/200][0/462] Loss_D: 0.7367 Loss_G: 2.9044 D(x): 0.6710 D(G(z)): 0.1358 / 0.0682 [54/200][50/462] Loss_D: 0.4704 Loss_G: 3.8915 D(x): 0.8687 D(G(z)): 0.0956 / 0.0246 [54/200][100/462] Loss_D: 0.5936 Loss_G: 3.3369 D(x): 0.8516 D(G(z)): 0.1509 / 0.0704 [54/200][150/462] Loss_D: 0.6290 Loss_G: 3.4223 D(x): 0.7539 D(G(z)): 0.1452 / 0.0386 [54/200][200/462] Loss_D: 0.6783 Loss_G: 5.3054 D(x): 0.9198 D(G(z)): 0.2069 / 0.0063 [54/200][250/462] Loss_D: 0.8329 Loss_G: 4.0929 D(x): 0.8087 D(G(z)): 0.3175 / 0.0225 [54/200][300/462] Loss_D: 0.6429 Loss_G: 3.2214 D(x): 0.6713 D(G(z)): 0.0671 / 0.0556 [54/200][350/462] Loss_D: 1.4532 Loss_G: 7.4082 D(x): 0.9834 D(G(z)): 0.5381 / 0.0011 [54/200][400/462] Loss_D: 0.7107 Loss_G: 3.1754 D(x): 0.6319 D(G(z)): 0.1128 / 0.0600 [54/200][450/462] Loss_D: 0.5188 Loss_G: 4.1655 D(x): 0.9178 D(G(z)): 0.1147 / 0.0262 [55/200][0/462] Loss_D: 0.5341 Loss_G: 2.7876 D(x): 0.8174 D(G(z)): 0.0710 / 0.0766 [55/200][50/462] Loss_D: 0.6938 Loss_G: 3.4137 D(x): 0.7914 D(G(z)): 0.2199 / 0.0415 [55/200][100/462] Loss_D: 0.8147 Loss_G: 2.9433 D(x): 0.5225 D(G(z)): 0.0760 / 0.0882 [55/200][150/462] Loss_D: 0.5773 Loss_G: 3.8472 D(x): 0.8788 D(G(z)): 0.1811 / 0.0289 [55/200][200/462] Loss_D: 0.5641 Loss_G: 3.1434 D(x): 0.7928 D(G(z)): 0.1289 / 0.0573 [55/200][250/462] Loss_D: 0.6921 Loss_G: 2.6990 D(x): 0.5956 D(G(z)): 0.0375 / 0.0959 [55/200][300/462] Loss_D: 0.6894 Loss_G: 4.4049 D(x): 0.9549 D(G(z)): 0.2406 / 0.0158 [55/200][350/462] Loss_D: 0.5954 Loss_G: 4.3463 D(x): 0.8605 D(G(z)): 0.1668 / 0.0192 [55/200][400/462] Loss_D: 0.7240 Loss_G: 4.5289 D(x): 0.9097 D(G(z)): 0.2539 / 0.0121 [55/200][450/462] Loss_D: 0.4343 Loss_G: 3.9025 D(x): 0.8034 D(G(z)): 0.0412 / 0.0279 . [56/200][0/462] Loss_D: 0.5472 Loss_G: 3.8416 D(x): 0.8467 D(G(z)): 0.1350 / 0.0265 [56/200][50/462] Loss_D: 0.7975 Loss_G: 4.6700 D(x): 0.8272 D(G(z)): 0.2794 / 0.0145 [56/200][100/462] Loss_D: 0.4804 Loss_G: 4.3683 D(x): 0.9162 D(G(z)): 0.1043 / 0.0159 [56/200][150/462] Loss_D: 0.8141 Loss_G: 5.2837 D(x): 0.9189 D(G(z)): 0.2360 / 0.0067 [56/200][200/462] Loss_D: 0.5850 Loss_G: 4.5437 D(x): 0.9026 D(G(z)): 0.1451 / 0.0157 [56/200][250/462] Loss_D: 1.2817 Loss_G: 6.0107 D(x): 0.9652 D(G(z)): 0.4909 / 0.0034 [56/200][300/462] Loss_D: 0.6723 Loss_G: 3.2775 D(x): 0.6888 D(G(z)): 0.1136 / 0.0671 [56/200][350/462] Loss_D: 0.6399 Loss_G: 4.4656 D(x): 0.8929 D(G(z)): 0.2060 / 0.0147 [56/200][400/462] Loss_D: 0.5704 Loss_G: 3.8455 D(x): 0.7806 D(G(z)): 0.1194 / 0.0295 [56/200][450/462] Loss_D: 0.5253 Loss_G: 4.0685 D(x): 0.7699 D(G(z)): 0.0786 / 0.0295 [57/200][0/462] Loss_D: 0.5480 Loss_G: 3.3741 D(x): 0.7423 D(G(z)): 0.0921 / 0.0477 [57/200][50/462] Loss_D: 0.6550 Loss_G: 2.9625 D(x): 0.6558 D(G(z)): 0.0542 / 0.0760 [57/200][100/462] Loss_D: 0.4997 Loss_G: 4.1494 D(x): 0.8044 D(G(z)): 0.0614 / 0.0321 [57/200][150/462] Loss_D: 1.0122 Loss_G: 2.1435 D(x): 0.4490 D(G(z)): 0.0463 / 0.1797 [57/200][200/462] Loss_D: 0.7166 Loss_G: 5.1386 D(x): 0.9668 D(G(z)): 0.2385 / 0.0093 [57/200][250/462] Loss_D: 0.8543 Loss_G: 2.2878 D(x): 0.5224 D(G(z)): 0.0224 / 0.1606 [57/200][300/462] Loss_D: 0.6517 Loss_G: 3.8641 D(x): 0.7665 D(G(z)): 0.1613 / 0.0234 [57/200][350/462] Loss_D: 0.6881 Loss_G: 4.3988 D(x): 0.8596 D(G(z)): 0.1995 / 0.0163 [57/200][400/462] Loss_D: 0.4563 Loss_G: 3.3952 D(x): 0.8340 D(G(z)): 0.0525 / 0.0438 [57/200][450/462] Loss_D: 0.5889 Loss_G: 4.7170 D(x): 0.9363 D(G(z)): 0.1815 / 0.0108 [58/200][0/462] Loss_D: 0.4908 Loss_G: 4.0502 D(x): 0.7911 D(G(z)): 0.0662 / 0.0324 [58/200][50/462] Loss_D: 0.9187 Loss_G: 3.7344 D(x): 0.4629 D(G(z)): 0.0279 / 0.0682 [58/200][100/462] Loss_D: 0.4874 Loss_G: 4.2040 D(x): 0.7439 D(G(z)): 0.0295 / 0.0234 [58/200][150/462] Loss_D: 0.5901 Loss_G: 3.9966 D(x): 0.9033 D(G(z)): 0.1655 / 0.0245 [58/200][200/462] Loss_D: 0.5691 Loss_G: 3.3873 D(x): 0.7562 D(G(z)): 0.1187 / 0.0468 [58/200][250/462] Loss_D: 0.5936 Loss_G: 4.4140 D(x): 0.8904 D(G(z)): 0.1752 / 0.0173 [58/200][300/462] Loss_D: 0.7659 Loss_G: 5.1216 D(x): 0.8616 D(G(z)): 0.2786 / 0.0109 [58/200][350/462] Loss_D: 0.5503 Loss_G: 3.5274 D(x): 0.7702 D(G(z)): 0.1037 / 0.0375 [58/200][400/462] Loss_D: 0.7190 Loss_G: 3.5706 D(x): 0.5779 D(G(z)): 0.0153 / 0.0447 [58/200][450/462] Loss_D: 0.6080 Loss_G: 4.8413 D(x): 0.9340 D(G(z)): 0.1776 / 0.0104 [59/200][0/462] Loss_D: 0.4056 Loss_G: 4.2351 D(x): 0.8466 D(G(z)): 0.0460 / 0.0253 [59/200][50/462] Loss_D: 0.7551 Loss_G: 4.8156 D(x): 0.8735 D(G(z)): 0.2505 / 0.0102 [59/200][100/462] Loss_D: 0.4834 Loss_G: 3.3268 D(x): 0.8332 D(G(z)): 0.0742 / 0.0535 [59/200][150/462] Loss_D: 0.5028 Loss_G: 2.9455 D(x): 0.7750 D(G(z)): 0.0602 / 0.0674 [59/200][200/462] Loss_D: 0.6427 Loss_G: 4.8886 D(x): 0.9199 D(G(z)): 0.2178 / 0.0110 [59/200][250/462] Loss_D: 0.4225 Loss_G: 4.6269 D(x): 0.8429 D(G(z)): 0.0393 / 0.0181 [59/200][300/462] Loss_D: 0.7432 Loss_G: 5.3048 D(x): 0.9623 D(G(z)): 0.2664 / 0.0063 [59/200][350/462] Loss_D: 0.7312 Loss_G: 4.3964 D(x): 0.9172 D(G(z)): 0.2513 / 0.0147 [59/200][400/462] Loss_D: 0.6787 Loss_G: 4.6227 D(x): 0.9082 D(G(z)): 0.2482 / 0.0118 [59/200][450/462] Loss_D: 0.4932 Loss_G: 3.6078 D(x): 0.8565 D(G(z)): 0.0533 / 0.0414 [60/200][0/462] Loss_D: 0.8086 Loss_G: 5.1281 D(x): 0.8347 D(G(z)): 0.2967 / 0.0073 [60/200][50/462] Loss_D: 0.4977 Loss_G: 3.9479 D(x): 0.8792 D(G(z)): 0.0847 / 0.0261 [60/200][100/462] Loss_D: 0.4907 Loss_G: 3.9492 D(x): 0.7684 D(G(z)): 0.0481 / 0.0292 [60/200][150/462] Loss_D: 0.8008 Loss_G: 2.6122 D(x): 0.5311 D(G(z)): 0.0191 / 0.1122 [60/200][200/462] Loss_D: 0.6431 Loss_G: 2.7546 D(x): 0.7429 D(G(z)): 0.1324 / 0.0829 [60/200][250/462] Loss_D: 0.7054 Loss_G: 4.0345 D(x): 0.8612 D(G(z)): 0.2452 / 0.0207 [60/200][300/462] Loss_D: 0.6652 Loss_G: 4.4808 D(x): 0.8556 D(G(z)): 0.2152 / 0.0159 [60/200][350/462] Loss_D: 0.7686 Loss_G: 3.8192 D(x): 0.9279 D(G(z)): 0.2210 / 0.0268 [60/200][400/462] Loss_D: 0.4536 Loss_G: 3.9993 D(x): 0.8027 D(G(z)): 0.0503 / 0.0242 [60/200][450/462] Loss_D: 0.4122 Loss_G: 4.0140 D(x): 0.8319 D(G(z)): 0.0248 / 0.0262 . [61/200][0/462] Loss_D: 1.0103 Loss_G: 3.2832 D(x): 0.5597 D(G(z)): 0.2290 / 0.0480 [61/200][50/462] Loss_D: 0.6877 Loss_G: 3.0636 D(x): 0.7008 D(G(z)): 0.0573 / 0.0710 [61/200][100/462] Loss_D: 0.5883 Loss_G: 3.1013 D(x): 0.6912 D(G(z)): 0.0584 / 0.0717 [61/200][150/462] Loss_D: 0.5558 Loss_G: 4.2321 D(x): 0.8993 D(G(z)): 0.1337 / 0.0194 [61/200][200/462] Loss_D: 0.6145 Loss_G: 3.7580 D(x): 0.9475 D(G(z)): 0.1880 / 0.0275 [61/200][250/462] Loss_D: 0.7136 Loss_G: 4.2830 D(x): 0.8129 D(G(z)): 0.2164 / 0.0201 [61/200][300/462] Loss_D: 0.7111 Loss_G: 4.5386 D(x): 0.7654 D(G(z)): 0.2242 / 0.0136 [61/200][350/462] Loss_D: 0.5754 Loss_G: 3.2146 D(x): 0.7362 D(G(z)): 0.0687 / 0.0569 [61/200][400/462] Loss_D: 0.5926 Loss_G: 2.9719 D(x): 0.7271 D(G(z)): 0.0791 / 0.0652 [61/200][450/462] Loss_D: 0.5955 Loss_G: 4.4558 D(x): 0.8528 D(G(z)): 0.1478 / 0.0152 [62/200][0/462] Loss_D: 0.6658 Loss_G: 3.0871 D(x): 0.6123 D(G(z)): 0.0199 / 0.0730 [62/200][50/462] Loss_D: 0.8713 Loss_G: 4.6284 D(x): 0.6839 D(G(z)): 0.2705 / 0.0149 [62/200][100/462] Loss_D: 0.6628 Loss_G: 3.2773 D(x): 0.6508 D(G(z)): 0.0343 / 0.0804 [62/200][150/462] Loss_D: 0.5163 Loss_G: 3.6050 D(x): 0.7958 D(G(z)): 0.0729 / 0.0348 [62/200][200/462] Loss_D: 0.9286 Loss_G: 2.4196 D(x): 0.4735 D(G(z)): 0.0266 / 0.1319 [62/200][250/462] Loss_D: 0.4940 Loss_G: 3.9322 D(x): 0.8383 D(G(z)): 0.0710 / 0.0272 [62/200][300/462] Loss_D: 0.5436 Loss_G: 3.4634 D(x): 0.8085 D(G(z)): 0.1185 / 0.0402 [62/200][350/462] Loss_D: 0.4669 Loss_G: 4.0706 D(x): 0.9120 D(G(z)): 0.0719 / 0.0330 [62/200][400/462] Loss_D: 0.4002 Loss_G: 4.5129 D(x): 0.8545 D(G(z)): 0.0423 / 0.0189 [62/200][450/462] Loss_D: 1.2537 Loss_G: 7.4103 D(x): 0.9566 D(G(z)): 0.5035 / 0.0010 [63/200][0/462] Loss_D: 0.4540 Loss_G: 4.2772 D(x): 0.8157 D(G(z)): 0.0272 / 0.0219 [63/200][50/462] Loss_D: 0.7520 Loss_G: 5.5414 D(x): 0.9231 D(G(z)): 0.2708 / 0.0047 [63/200][100/462] Loss_D: 0.4929 Loss_G: 4.6166 D(x): 0.8612 D(G(z)): 0.0897 / 0.0132 [63/200][150/462] Loss_D: 0.5208 Loss_G: 3.7748 D(x): 0.9055 D(G(z)): 0.1107 / 0.0336 [63/200][200/462] Loss_D: 0.4695 Loss_G: 3.5341 D(x): 0.8146 D(G(z)): 0.0406 / 0.0448 [63/200][250/462] Loss_D: 0.6901 Loss_G: 3.0850 D(x): 0.6593 D(G(z)): 0.0914 / 0.0671 [63/200][300/462] Loss_D: 0.5047 Loss_G: 4.4019 D(x): 0.7719 D(G(z)): 0.0255 / 0.0197 [63/200][350/462] Loss_D: 0.5550 Loss_G: 3.2389 D(x): 0.7110 D(G(z)): 0.0332 / 0.0738 [63/200][400/462] Loss_D: 0.7428 Loss_G: 4.4902 D(x): 0.8001 D(G(z)): 0.2386 / 0.0176 [63/200][450/462] Loss_D: 0.6260 Loss_G: 3.7964 D(x): 0.7018 D(G(z)): 0.0880 / 0.0345 [64/200][0/462] Loss_D: 0.5403 Loss_G: 4.8749 D(x): 0.9492 D(G(z)): 0.1278 / 0.0109 [64/200][50/462] Loss_D: 0.7106 Loss_G: 3.4266 D(x): 0.7048 D(G(z)): 0.1572 / 0.0456 [64/200][100/462] Loss_D: 0.4438 Loss_G: 4.5188 D(x): 0.9255 D(G(z)): 0.0820 / 0.0137 [64/200][150/462] Loss_D: 0.6079 Loss_G: 3.2659 D(x): 0.6514 D(G(z)): 0.0642 / 0.0597 [64/200][200/462] Loss_D: 0.6604 Loss_G: 3.4914 D(x): 0.7640 D(G(z)): 0.1445 / 0.0363 [64/200][250/462] Loss_D: 0.4639 Loss_G: 4.1828 D(x): 0.7896 D(G(z)): 0.0388 / 0.0342 [64/200][300/462] Loss_D: 0.5443 Loss_G: 2.7541 D(x): 0.7640 D(G(z)): 0.0806 / 0.0770 [64/200][350/462] Loss_D: 0.5897 Loss_G: 3.6075 D(x): 0.7941 D(G(z)): 0.0713 / 0.0380 [64/200][400/462] Loss_D: 0.7112 Loss_G: 2.1812 D(x): 0.6053 D(G(z)): 0.0912 / 0.1528 [64/200][450/462] Loss_D: 0.5450 Loss_G: 4.5090 D(x): 0.8569 D(G(z)): 0.1343 / 0.0158 [65/200][0/462] Loss_D: 0.4828 Loss_G: 4.3873 D(x): 0.8849 D(G(z)): 0.0862 / 0.0175 [65/200][50/462] Loss_D: 0.5302 Loss_G: 3.2663 D(x): 0.7956 D(G(z)): 0.0954 / 0.0603 [65/200][100/462] Loss_D: 0.6333 Loss_G: 2.9244 D(x): 0.7351 D(G(z)): 0.0764 / 0.0621 [65/200][150/462] Loss_D: 1.5762 Loss_G: 9.1141 D(x): 0.8790 D(G(z)): 0.5748 / 0.0002 [65/200][200/462] Loss_D: 0.5617 Loss_G: 3.1387 D(x): 0.8041 D(G(z)): 0.1096 / 0.0523 [65/200][250/462] Loss_D: 0.5347 Loss_G: 3.2346 D(x): 0.8491 D(G(z)): 0.1195 / 0.0551 [65/200][300/462] Loss_D: 0.9619 Loss_G: 6.0124 D(x): 0.9660 D(G(z)): 0.3682 / 0.0034 [65/200][350/462] Loss_D: 0.5946 Loss_G: 3.6565 D(x): 0.7648 D(G(z)): 0.1073 / 0.0455 [65/200][400/462] Loss_D: 0.6343 Loss_G: 3.5077 D(x): 0.6985 D(G(z)): 0.0783 / 0.0380 [65/200][450/462] Loss_D: 0.4351 Loss_G: 3.8406 D(x): 0.8373 D(G(z)): 0.0467 / 0.0348 . [66/200][0/462] Loss_D: 0.4042 Loss_G: 5.0288 D(x): 0.8806 D(G(z)): 0.0268 / 0.0118 [66/200][50/462] Loss_D: 0.6163 Loss_G: 3.7998 D(x): 0.5873 D(G(z)): 0.0067 / 0.0369 [66/200][100/462] Loss_D: 0.4792 Loss_G: 4.9709 D(x): 0.8397 D(G(z)): 0.0292 / 0.0152 [66/200][150/462] Loss_D: 0.9535 Loss_G: 5.9949 D(x): 0.8757 D(G(z)): 0.3224 / 0.0054 [66/200][200/462] Loss_D: 0.5473 Loss_G: 3.8717 D(x): 0.8605 D(G(z)): 0.1452 / 0.0252 [66/200][250/462] Loss_D: 0.5279 Loss_G: 4.1225 D(x): 0.7964 D(G(z)): 0.0935 / 0.0270 [66/200][300/462] Loss_D: 0.6745 Loss_G: 2.9033 D(x): 0.6192 D(G(z)): 0.0250 / 0.0827 [66/200][350/462] Loss_D: 0.4930 Loss_G: 4.4514 D(x): 0.9279 D(G(z)): 0.0971 / 0.0157 [66/200][400/462] Loss_D: 0.4724 Loss_G: 5.5501 D(x): 0.7498 D(G(z)): 0.0113 / 0.0103 [66/200][450/462] Loss_D: 0.5174 Loss_G: 4.2157 D(x): 0.8535 D(G(z)): 0.0935 / 0.0270 [67/200][0/462] Loss_D: 0.5081 Loss_G: 3.7471 D(x): 0.8010 D(G(z)): 0.0892 / 0.0289 [67/200][50/462] Loss_D: 0.4970 Loss_G: 3.4633 D(x): 0.8222 D(G(z)): 0.0727 / 0.0400 [67/200][100/462] Loss_D: 0.8882 Loss_G: 5.9375 D(x): 0.9115 D(G(z)): 0.3061 / 0.0039 [67/200][150/462] Loss_D: 0.5319 Loss_G: 3.6024 D(x): 0.7660 D(G(z)): 0.0483 / 0.0448 [67/200][200/462] Loss_D: 0.4307 Loss_G: 3.8492 D(x): 0.8349 D(G(z)): 0.0556 / 0.0348 [67/200][250/462] Loss_D: 0.5989 Loss_G: 4.4152 D(x): 0.8625 D(G(z)): 0.1686 / 0.0172 [67/200][300/462] Loss_D: 0.5868 Loss_G: 4.7182 D(x): 0.9592 D(G(z)): 0.1437 / 0.0117 [67/200][350/462] Loss_D: 0.5078 Loss_G: 4.9724 D(x): 0.8900 D(G(z)): 0.0797 / 0.0090 [67/200][400/462] Loss_D: 0.5535 Loss_G: 4.2434 D(x): 0.8041 D(G(z)): 0.1123 / 0.0194 [67/200][450/462] Loss_D: 0.6512 Loss_G: 2.9839 D(x): 0.7646 D(G(z)): 0.1506 / 0.0591 [68/200][0/462] Loss_D: 0.5454 Loss_G: 5.4096 D(x): 0.9334 D(G(z)): 0.1454 / 0.0058 [68/200][50/462] Loss_D: 0.5574 Loss_G: 3.9550 D(x): 0.7239 D(G(z)): 0.0533 / 0.0321 [68/200][100/462] Loss_D: 0.4811 Loss_G: 4.0604 D(x): 0.8353 D(G(z)): 0.0677 / 0.0286 [68/200][150/462] Loss_D: 0.5675 Loss_G: 3.7818 D(x): 0.8550 D(G(z)): 0.1338 / 0.0333 [68/200][200/462] Loss_D: 1.0750 Loss_G: 7.5228 D(x): 0.9483 D(G(z)): 0.4331 / 0.0014 [68/200][250/462] Loss_D: 0.6016 Loss_G: 2.3981 D(x): 0.7391 D(G(z)): 0.1044 / 0.1337 [68/200][300/462] Loss_D: 0.6898 Loss_G: 3.8228 D(x): 0.7621 D(G(z)): 0.1722 / 0.0308 [68/200][350/462] Loss_D: 0.4176 Loss_G: 4.0006 D(x): 0.8956 D(G(z)): 0.0519 / 0.0331 [68/200][400/462] Loss_D: 0.5252 Loss_G: 3.8368 D(x): 0.7648 D(G(z)): 0.0529 / 0.0420 [68/200][450/462] Loss_D: 0.4775 Loss_G: 4.3246 D(x): 0.9272 D(G(z)): 0.0948 / 0.0196 [69/200][0/462] Loss_D: 0.5795 Loss_G: 4.5229 D(x): 0.8036 D(G(z)): 0.1353 / 0.0132 [69/200][50/462] Loss_D: 0.6125 Loss_G: 4.6081 D(x): 0.8840 D(G(z)): 0.1735 / 0.0143 [69/200][100/462] Loss_D: 0.4688 Loss_G: 4.1304 D(x): 0.8340 D(G(z)): 0.0717 / 0.0180 [69/200][150/462] Loss_D: 0.7521 Loss_G: 3.2995 D(x): 0.7088 D(G(z)): 0.1734 / 0.0446 [69/200][200/462] Loss_D: 0.6261 Loss_G: 2.5431 D(x): 0.8171 D(G(z)): 0.1443 / 0.1023 [69/200][250/462] Loss_D: 0.5280 Loss_G: 3.7916 D(x): 0.8661 D(G(z)): 0.1089 / 0.0281 [69/200][300/462] Loss_D: 0.6702 Loss_G: 2.3563 D(x): 0.6512 D(G(z)): 0.0729 / 0.1228 [69/200][350/462] Loss_D: 0.4574 Loss_G: 3.8406 D(x): 0.8099 D(G(z)): 0.0408 / 0.0314 [69/200][400/462] Loss_D: 0.5092 Loss_G: 3.9750 D(x): 0.8036 D(G(z)): 0.0651 / 0.0369 [69/200][450/462] Loss_D: 0.7490 Loss_G: 3.0601 D(x): 0.6074 D(G(z)): 0.0181 / 0.0704 [70/200][0/462] Loss_D: 0.5996 Loss_G: 4.4779 D(x): 0.8815 D(G(z)): 0.1511 / 0.0188 [70/200][50/462] Loss_D: 0.4677 Loss_G: 4.2065 D(x): 0.8501 D(G(z)): 0.0649 / 0.0236 [70/200][100/462] Loss_D: 0.4890 Loss_G: 4.2481 D(x): 0.7980 D(G(z)): 0.0381 / 0.0252 [70/200][150/462] Loss_D: 0.8256 Loss_G: 5.3479 D(x): 0.9019 D(G(z)): 0.3043 / 0.0066 [70/200][200/462] Loss_D: 0.5980 Loss_G: 3.4266 D(x): 0.6760 D(G(z)): 0.0173 / 0.0482 [70/200][250/462] Loss_D: 0.6188 Loss_G: 3.3502 D(x): 0.6776 D(G(z)): 0.0740 / 0.0506 [70/200][300/462] Loss_D: 0.5518 Loss_G: 3.3966 D(x): 0.7462 D(G(z)): 0.0378 / 0.0421 [70/200][350/462] Loss_D: 0.4384 Loss_G: 4.2932 D(x): 0.8393 D(G(z)): 0.0571 / 0.0243 [70/200][400/462] Loss_D: 0.5084 Loss_G: 3.6732 D(x): 0.7696 D(G(z)): 0.0801 / 0.0420 [70/200][450/462] Loss_D: 0.4803 Loss_G: 3.8897 D(x): 0.7519 D(G(z)): 0.0241 / 0.0311 . [71/200][0/462] Loss_D: 0.7679 Loss_G: 5.1795 D(x): 0.7762 D(G(z)): 0.2369 / 0.0090 [71/200][50/462] Loss_D: 0.4764 Loss_G: 3.2645 D(x): 0.8305 D(G(z)): 0.0526 / 0.0529 [71/200][100/462] Loss_D: 0.6486 Loss_G: 4.6245 D(x): 0.9415 D(G(z)): 0.1948 / 0.0123 [71/200][150/462] Loss_D: 0.6508 Loss_G: 2.6115 D(x): 0.6104 D(G(z)): 0.0372 / 0.1015 [71/200][200/462] Loss_D: 0.6037 Loss_G: 3.7042 D(x): 0.6452 D(G(z)): 0.0142 / 0.0360 [71/200][250/462] Loss_D: 1.1303 Loss_G: 3.5382 D(x): 0.3880 D(G(z)): 0.0049 / 0.0484 [71/200][300/462] Loss_D: 0.4948 Loss_G: 4.3490 D(x): 0.7717 D(G(z)): 0.0589 / 0.0189 [71/200][350/462] Loss_D: 0.6762 Loss_G: 4.6034 D(x): 0.9186 D(G(z)): 0.1982 / 0.0133 [71/200][400/462] Loss_D: 0.5834 Loss_G: 3.4973 D(x): 0.6941 D(G(z)): 0.0338 / 0.0393 [71/200][450/462] Loss_D: 0.5225 Loss_G: 4.6975 D(x): 0.7681 D(G(z)): 0.0636 / 0.0179 [72/200][0/462] Loss_D: 0.4932 Loss_G: 4.2533 D(x): 0.8483 D(G(z)): 0.0979 / 0.0257 [72/200][50/462] Loss_D: 0.6504 Loss_G: 5.0720 D(x): 0.8406 D(G(z)): 0.1912 / 0.0092 [72/200][100/462] Loss_D: 0.5754 Loss_G: 3.5299 D(x): 0.7448 D(G(z)): 0.0683 / 0.0411 [72/200][150/462] Loss_D: 0.8293 Loss_G: 5.4379 D(x): 0.8798 D(G(z)): 0.2726 / 0.0059 [72/200][200/462] Loss_D: 0.5151 Loss_G: 4.7834 D(x): 0.8661 D(G(z)): 0.1114 / 0.0109 [72/200][250/462] Loss_D: 0.7420 Loss_G: 4.8507 D(x): 0.9761 D(G(z)): 0.1995 / 0.0102 [72/200][300/462] Loss_D: 0.7122 Loss_G: 5.4707 D(x): 0.8857 D(G(z)): 0.2667 / 0.0062 [72/200][350/462] Loss_D: 0.5377 Loss_G: 2.4105 D(x): 0.7302 D(G(z)): 0.0463 / 0.1153 [72/200][400/462] Loss_D: 0.5011 Loss_G: 4.6334 D(x): 0.7967 D(G(z)): 0.0594 / 0.0178 [72/200][450/462] Loss_D: 0.6095 Loss_G: 4.6532 D(x): 0.6966 D(G(z)): 0.0125 / 0.0202 [73/200][0/462] Loss_D: 0.5435 Loss_G: 3.9194 D(x): 0.8325 D(G(z)): 0.1233 / 0.0276 [73/200][50/462] Loss_D: 0.6039 Loss_G: 4.5827 D(x): 0.8814 D(G(z)): 0.1973 / 0.0120 [73/200][100/462] Loss_D: 0.4504 Loss_G: 5.7785 D(x): 0.7961 D(G(z)): 0.0036 / 0.0053 [73/200][150/462] Loss_D: 0.5734 Loss_G: 3.0292 D(x): 0.7234 D(G(z)): 0.0911 / 0.0858 [73/200][200/462] Loss_D: 0.5908 Loss_G: 3.0850 D(x): 0.7092 D(G(z)): 0.1015 / 0.0615 [73/200][250/462] Loss_D: 0.6133 Loss_G: 4.2035 D(x): 0.8294 D(G(z)): 0.1576 / 0.0187 [73/200][300/462] Loss_D: 0.4726 Loss_G: 4.8195 D(x): 0.8200 D(G(z)): 0.0595 / 0.0112 [73/200][350/462] Loss_D: 0.5171 Loss_G: 2.7311 D(x): 0.8978 D(G(z)): 0.1096 / 0.0784 [73/200][400/462] Loss_D: 0.5597 Loss_G: 3.8957 D(x): 0.7162 D(G(z)): 0.0368 / 0.0464 [73/200][450/462] Loss_D: 0.5308 Loss_G: 4.9497 D(x): 0.8992 D(G(z)): 0.1333 / 0.0103 [74/200][0/462] Loss_D: 0.5634 Loss_G: 4.9898 D(x): 0.9177 D(G(z)): 0.1398 / 0.0098 [74/200][50/462] Loss_D: 0.6891 Loss_G: 2.2400 D(x): 0.6677 D(G(z)): 0.1038 / 0.1441 [74/200][100/462] Loss_D: 0.6058 Loss_G: 5.5337 D(x): 0.8819 D(G(z)): 0.1953 / 0.0062 [74/200][150/462] Loss_D: 0.5603 Loss_G: 3.2450 D(x): 0.7123 D(G(z)): 0.0216 / 0.0648 [74/200][200/462] Loss_D: 0.5445 Loss_G: 4.0020 D(x): 0.7396 D(G(z)): 0.0276 / 0.0222 [74/200][250/462] Loss_D: 0.6065 Loss_G: 3.3688 D(x): 0.6987 D(G(z)): 0.0511 / 0.0604 [74/200][300/462] Loss_D: 0.6178 Loss_G: 4.6820 D(x): 0.9628 D(G(z)): 0.1636 / 0.0104 [74/200][350/462] Loss_D: 0.5076 Loss_G: 4.6537 D(x): 0.8527 D(G(z)): 0.0461 / 0.0154 [74/200][400/462] Loss_D: 0.7740 Loss_G: 6.3069 D(x): 0.9007 D(G(z)): 0.2448 / 0.0029 [74/200][450/462] Loss_D: 0.5138 Loss_G: 4.7105 D(x): 0.8415 D(G(z)): 0.1176 / 0.0139 [75/200][0/462] Loss_D: 0.6411 Loss_G: 3.1131 D(x): 0.6611 D(G(z)): 0.0938 / 0.0558 [75/200][50/462] Loss_D: 0.5298 Loss_G: 4.1428 D(x): 0.6951 D(G(z)): 0.0377 / 0.0413 [75/200][100/462] Loss_D: 0.5876 Loss_G: 3.8122 D(x): 0.8113 D(G(z)): 0.1403 / 0.0365 [75/200][150/462] Loss_D: 0.6590 Loss_G: 3.8923 D(x): 0.7151 D(G(z)): 0.1352 / 0.0386 [75/200][200/462] Loss_D: 0.9167 Loss_G: 6.0330 D(x): 0.9367 D(G(z)): 0.3694 / 0.0032 [75/200][250/462] Loss_D: 0.5972 Loss_G: 4.7339 D(x): 0.9613 D(G(z)): 0.1764 / 0.0092 [75/200][300/462] Loss_D: 0.5302 Loss_G: 3.7721 D(x): 0.7120 D(G(z)): 0.0221 / 0.0396 [75/200][350/462] Loss_D: 0.5361 Loss_G: 4.4804 D(x): 0.8442 D(G(z)): 0.1029 / 0.0160 [75/200][400/462] Loss_D: 0.5334 Loss_G: 4.2553 D(x): 0.9265 D(G(z)): 0.1560 / 0.0155 [75/200][450/462] Loss_D: 0.4649 Loss_G: 3.2966 D(x): 0.7949 D(G(z)): 0.0412 / 0.0563 . [76/200][0/462] Loss_D: 0.6098 Loss_G: 3.2097 D(x): 0.8423 D(G(z)): 0.1170 / 0.0585 [76/200][50/462] Loss_D: 0.6856 Loss_G: 3.1172 D(x): 0.5933 D(G(z)): 0.0128 / 0.0712 [76/200][100/462] Loss_D: 0.6847 Loss_G: 3.1730 D(x): 0.6207 D(G(z)): 0.0415 / 0.0610 [76/200][150/462] Loss_D: 0.6356 Loss_G: 5.5041 D(x): 0.9033 D(G(z)): 0.1927 / 0.0060 [76/200][200/462] Loss_D: 0.5034 Loss_G: 3.7618 D(x): 0.7667 D(G(z)): 0.0676 / 0.0417 [76/200][250/462] Loss_D: 0.4658 Loss_G: 4.2098 D(x): 0.9481 D(G(z)): 0.0845 / 0.0171 [76/200][300/462] Loss_D: 0.4895 Loss_G: 4.6089 D(x): 0.7825 D(G(z)): 0.0330 / 0.0216 [76/200][350/462] Loss_D: 0.4352 Loss_G: 4.6961 D(x): 0.8384 D(G(z)): 0.0188 / 0.0181 [76/200][400/462] Loss_D: 1.0256 Loss_G: 2.8222 D(x): 0.4813 D(G(z)): 0.0188 / 0.1145 [76/200][450/462] Loss_D: 0.5257 Loss_G: 3.9446 D(x): 0.8247 D(G(z)): 0.0843 / 0.0348 [77/200][0/462] Loss_D: 0.6192 Loss_G: 3.4135 D(x): 0.7779 D(G(z)): 0.1329 / 0.0431 [77/200][50/462] Loss_D: 0.4934 Loss_G: 4.7709 D(x): 0.8827 D(G(z)): 0.0801 / 0.0130 [77/200][100/462] Loss_D: 1.0556 Loss_G: 6.0467 D(x): 0.9705 D(G(z)): 0.3852 / 0.0039 [77/200][150/462] Loss_D: 0.4065 Loss_G: 4.0984 D(x): 0.7802 D(G(z)): 0.0193 / 0.0272 [77/200][200/462] Loss_D: 0.5272 Loss_G: 4.0057 D(x): 0.8948 D(G(z)): 0.1012 / 0.0219 [77/200][250/462] Loss_D: 0.4900 Loss_G: 4.2264 D(x): 0.8252 D(G(z)): 0.0814 / 0.0183 [77/200][300/462] Loss_D: 0.5843 Loss_G: 4.8609 D(x): 0.9367 D(G(z)): 0.1776 / 0.0102 [77/200][350/462] Loss_D: 0.4239 Loss_G: 4.4582 D(x): 0.8934 D(G(z)): 0.0599 / 0.0179 [77/200][400/462] Loss_D: 0.5048 Loss_G: 5.0874 D(x): 0.7809 D(G(z)): 0.0629 / 0.0174 [77/200][450/462] Loss_D: 0.4284 Loss_G: 5.3556 D(x): 0.8955 D(G(z)): 0.0310 / 0.0133 [78/200][0/462] Loss_D: 0.5286 Loss_G: 4.0926 D(x): 0.8130 D(G(z)): 0.0623 / 0.0335 [78/200][50/462] Loss_D: 0.5879 Loss_G: 4.3809 D(x): 0.7442 D(G(z)): 0.1043 / 0.0203 [78/200][100/462] Loss_D: 0.9825 Loss_G: 2.7448 D(x): 0.4857 D(G(z)): 0.0124 / 0.1056 [78/200][150/462] Loss_D: 0.4634 Loss_G: 4.7581 D(x): 0.8585 D(G(z)): 0.0552 / 0.0128 [78/200][200/462] Loss_D: 0.4373 Loss_G: 5.5254 D(x): 0.8632 D(G(z)): 0.0278 / 0.0077 [78/200][250/462] Loss_D: 0.4474 Loss_G: 4.0564 D(x): 0.9141 D(G(z)): 0.0603 / 0.0227 [78/200][300/462] Loss_D: 0.5880 Loss_G: 4.3032 D(x): 0.9135 D(G(z)): 0.1653 / 0.0176 [78/200][350/462] Loss_D: 0.3887 Loss_G: 4.9387 D(x): 0.8696 D(G(z)): 0.0280 / 0.0120 [78/200][400/462] Loss_D: 0.4874 Loss_G: 3.3884 D(x): 0.8668 D(G(z)): 0.1022 / 0.0391 [78/200][450/462] Loss_D: 0.5209 Loss_G: 4.8081 D(x): 0.8537 D(G(z)): 0.1142 / 0.0152 [79/200][0/462] Loss_D: 0.5464 Loss_G: 4.0342 D(x): 0.7487 D(G(z)): 0.0300 / 0.0368 [79/200][50/462] Loss_D: 0.7101 Loss_G: 4.9677 D(x): 0.9613 D(G(z)): 0.2004 / 0.0106 [79/200][100/462] Loss_D: 0.4510 Loss_G: 5.3722 D(x): 0.8683 D(G(z)): 0.0664 / 0.0097 [79/200][150/462] Loss_D: 0.4816 Loss_G: 4.1545 D(x): 0.7914 D(G(z)): 0.0360 / 0.0246 [79/200][200/462] Loss_D: 0.4680 Loss_G: 3.6605 D(x): 0.8613 D(G(z)): 0.0855 / 0.0292 [79/200][250/462] Loss_D: 0.4960 Loss_G: 5.1774 D(x): 0.9367 D(G(z)): 0.1029 / 0.0098 [79/200][300/462] Loss_D: 0.5083 Loss_G: 4.0545 D(x): 0.8681 D(G(z)): 0.1067 / 0.0260 [79/200][350/462] Loss_D: 0.6566 Loss_G: 4.8257 D(x): 0.9375 D(G(z)): 0.1930 / 0.0109 [79/200][400/462] Loss_D: 0.6277 Loss_G: 5.3318 D(x): 0.8639 D(G(z)): 0.1536 / 0.0073 [79/200][450/462] Loss_D: 0.5523 Loss_G: 3.4904 D(x): 0.6882 D(G(z)): 0.0147 / 0.0408 [80/200][0/462] Loss_D: 0.5815 Loss_G: 3.6290 D(x): 0.6863 D(G(z)): 0.0416 / 0.0451 [80/200][50/462] Loss_D: 0.4085 Loss_G: 4.7254 D(x): 0.8547 D(G(z)): 0.0409 / 0.0130 [80/200][100/462] Loss_D: 0.5279 Loss_G: 3.9769 D(x): 0.9393 D(G(z)): 0.0964 / 0.0243 [80/200][150/462] Loss_D: 0.5351 Loss_G: 4.4430 D(x): 0.8801 D(G(z)): 0.1128 / 0.0161 [80/200][200/462] Loss_D: 0.4855 Loss_G: 3.7333 D(x): 0.8536 D(G(z)): 0.1014 / 0.0269 [80/200][250/462] Loss_D: 0.4780 Loss_G: 3.6270 D(x): 0.8136 D(G(z)): 0.0535 / 0.0402 [80/200][300/462] Loss_D: 0.5192 Loss_G: 3.7418 D(x): 0.8283 D(G(z)): 0.1017 / 0.0307 [80/200][350/462] Loss_D: 0.5795 Loss_G: 3.0064 D(x): 0.7690 D(G(z)): 0.1051 / 0.0785 [80/200][400/462] Loss_D: 0.4601 Loss_G: 3.9670 D(x): 0.8775 D(G(z)): 0.0489 / 0.0260 [80/200][450/462] Loss_D: 0.5407 Loss_G: 5.4548 D(x): 0.9466 D(G(z)): 0.1056 / 0.0105 . [81/200][0/462] Loss_D: 0.5440 Loss_G: 5.7766 D(x): 0.9286 D(G(z)): 0.1230 / 0.0046 [81/200][50/462] Loss_D: 0.5942 Loss_G: 2.6932 D(x): 0.6989 D(G(z)): 0.0774 / 0.0755 [81/200][100/462] Loss_D: 0.5561 Loss_G: 4.3982 D(x): 0.7986 D(G(z)): 0.1052 / 0.0194 [81/200][150/462] Loss_D: 0.7801 Loss_G: 5.2700 D(x): 0.8962 D(G(z)): 0.2680 / 0.0074 [81/200][200/462] Loss_D: 0.4397 Loss_G: 3.3211 D(x): 0.9390 D(G(z)): 0.0641 / 0.0449 [81/200][250/462] Loss_D: 0.6769 Loss_G: 5.9699 D(x): 0.9457 D(G(z)): 0.2438 / 0.0037 [81/200][300/462] Loss_D: 0.6710 Loss_G: 2.7922 D(x): 0.6339 D(G(z)): 0.0426 / 0.0973 [81/200][350/462] Loss_D: 0.7039 Loss_G: 2.8220 D(x): 0.5802 D(G(z)): 0.0208 / 0.0708 [81/200][400/462] Loss_D: 0.6448 Loss_G: 5.0358 D(x): 0.7930 D(G(z)): 0.1414 / 0.0097 [81/200][450/462] Loss_D: 0.6009 Loss_G: 5.4482 D(x): 0.8849 D(G(z)): 0.1672 / 0.0057 [82/200][0/462] Loss_D: 0.4657 Loss_G: 5.1559 D(x): 0.9689 D(G(z)): 0.0404 / 0.0079 [82/200][50/462] Loss_D: 0.5396 Loss_G: 3.4312 D(x): 0.7712 D(G(z)): 0.0377 / 0.0450 [82/200][100/462] Loss_D: 0.4827 Loss_G: 4.1758 D(x): 0.7926 D(G(z)): 0.0680 / 0.0209 [82/200][150/462] Loss_D: 0.4665 Loss_G: 4.4205 D(x): 0.8389 D(G(z)): 0.0592 / 0.0227 [82/200][200/462] Loss_D: 0.5922 Loss_G: 3.4192 D(x): 0.7249 D(G(z)): 0.0611 / 0.0462 [82/200][250/462] Loss_D: 0.5115 Loss_G: 4.4234 D(x): 0.8592 D(G(z)): 0.0971 / 0.0168 [82/200][300/462] Loss_D: 0.5318 Loss_G: 4.6474 D(x): 0.8661 D(G(z)): 0.1021 / 0.0131 [82/200][350/462] Loss_D: 0.5609 Loss_G: 5.3839 D(x): 0.8182 D(G(z)): 0.1049 / 0.0090 [82/200][400/462] Loss_D: 0.5584 Loss_G: 3.2348 D(x): 0.6967 D(G(z)): 0.0303 / 0.0596 [82/200][450/462] Loss_D: 0.5822 Loss_G: 3.3817 D(x): 0.6863 D(G(z)): 0.0171 / 0.0474 [83/200][0/462] Loss_D: 0.5032 Loss_G: 3.8452 D(x): 0.8295 D(G(z)): 0.0714 / 0.0268 [83/200][50/462] Loss_D: 0.4726 Loss_G: 3.5041 D(x): 0.7469 D(G(z)): 0.0346 / 0.0433 [83/200][100/462] Loss_D: 0.7233 Loss_G: 3.5129 D(x): 0.6614 D(G(z)): 0.1016 / 0.0409 [83/200][150/462] Loss_D: 0.7028 Loss_G: 4.0150 D(x): 0.6031 D(G(z)): 0.0090 / 0.0286 [83/200][200/462] Loss_D: 0.6493 Loss_G: 2.4615 D(x): 0.6682 D(G(z)): 0.1150 / 0.0910 [83/200][250/462] Loss_D: 0.5723 Loss_G: 3.5443 D(x): 0.9559 D(G(z)): 0.1235 / 0.0333 [83/200][300/462] Loss_D: 0.5059 Loss_G: 3.7524 D(x): 0.7283 D(G(z)): 0.0416 / 0.0363 [83/200][350/462] Loss_D: 0.4718 Loss_G: 4.4532 D(x): 0.8951 D(G(z)): 0.0615 / 0.0152 [83/200][400/462] Loss_D: 0.4446 Loss_G: 3.8054 D(x): 0.7997 D(G(z)): 0.0411 / 0.0352 [83/200][450/462] Loss_D: 0.5030 Loss_G: 3.9003 D(x): 0.8625 D(G(z)): 0.0679 / 0.0268 [84/200][0/462] Loss_D: 0.6844 Loss_G: 3.2385 D(x): 0.7029 D(G(z)): 0.1188 / 0.0538 [84/200][50/462] Loss_D: 0.6463 Loss_G: 4.7916 D(x): 0.9410 D(G(z)): 0.1944 / 0.0132 [84/200][100/462] Loss_D: 0.4496 Loss_G: 4.6707 D(x): 0.8819 D(G(z)): 0.0580 / 0.0149 [84/200][150/462] Loss_D: 0.5350 Loss_G: 4.3978 D(x): 0.9377 D(G(z)): 0.1160 / 0.0151 [84/200][200/462] Loss_D: 0.6035 Loss_G: 4.9079 D(x): 0.9256 D(G(z)): 0.1452 / 0.0109 [84/200][250/462] Loss_D: 0.7055 Loss_G: 3.4092 D(x): 0.6557 D(G(z)): 0.1028 / 0.0562 [84/200][300/462] Loss_D: 0.5944 Loss_G: 4.8100 D(x): 0.9447 D(G(z)): 0.1453 / 0.0105 [84/200][350/462] Loss_D: 0.5318 Loss_G: 3.8523 D(x): 0.7602 D(G(z)): 0.0641 / 0.0341 [84/200][400/462] Loss_D: 0.5197 Loss_G: 4.3154 D(x): 0.8467 D(G(z)): 0.0939 / 0.0197 [84/200][450/462] Loss_D: 0.4941 Loss_G: 4.6563 D(x): 0.8518 D(G(z)): 0.0516 / 0.0174 [85/200][0/462] Loss_D: 0.5548 Loss_G: 3.4151 D(x): 0.7068 D(G(z)): 0.0478 / 0.0511 [85/200][50/462] Loss_D: 0.4669 Loss_G: 3.9417 D(x): 0.8454 D(G(z)): 0.0392 / 0.0328 [85/200][100/462] Loss_D: 0.5810 Loss_G: 4.1669 D(x): 0.9517 D(G(z)): 0.1419 / 0.0205 [85/200][150/462] Loss_D: 0.5937 Loss_G: 2.8998 D(x): 0.7391 D(G(z)): 0.1205 / 0.0825 [85/200][200/462] Loss_D: 0.5330 Loss_G: 4.3116 D(x): 0.8615 D(G(z)): 0.1139 / 0.0185 [85/200][250/462] Loss_D: 0.4345 Loss_G: 3.7652 D(x): 0.9037 D(G(z)): 0.0355 / 0.0311 [85/200][300/462] Loss_D: 0.5951 Loss_G: 4.3330 D(x): 0.7204 D(G(z)): 0.0077 / 0.0187 [85/200][350/462] Loss_D: 0.4458 Loss_G: 4.6000 D(x): 0.9323 D(G(z)): 0.0764 / 0.0143 [85/200][400/462] Loss_D: 0.6136 Loss_G: 4.5388 D(x): 0.9004 D(G(z)): 0.1726 / 0.0137 [85/200][450/462] Loss_D: 0.6137 Loss_G: 5.3992 D(x): 0.8876 D(G(z)): 0.1459 / 0.0082 . [86/200][0/462] Loss_D: 0.4250 Loss_G: 4.0686 D(x): 0.8666 D(G(z)): 0.0498 / 0.0247 [86/200][50/462] Loss_D: 0.4603 Loss_G: 3.9280 D(x): 0.8676 D(G(z)): 0.0857 / 0.0289 [86/200][100/462] Loss_D: 0.5873 Loss_G: 4.0384 D(x): 0.7876 D(G(z)): 0.1418 / 0.0264 [86/200][150/462] Loss_D: 0.4894 Loss_G: 4.2050 D(x): 0.8432 D(G(z)): 0.0925 / 0.0240 [86/200][200/462] Loss_D: 0.4914 Loss_G: 4.1847 D(x): 0.8668 D(G(z)): 0.0871 / 0.0211 [86/200][250/462] Loss_D: 0.6984 Loss_G: 2.9369 D(x): 0.5926 D(G(z)): 0.0241 / 0.0784 [86/200][300/462] Loss_D: 0.5025 Loss_G: 4.4905 D(x): 0.8987 D(G(z)): 0.0972 / 0.0166 [86/200][350/462] Loss_D: 0.4714 Loss_G: 4.3007 D(x): 0.9009 D(G(z)): 0.0715 / 0.0235 [86/200][400/462] Loss_D: 0.4705 Loss_G: 3.8076 D(x): 0.7824 D(G(z)): 0.0431 / 0.0317 [86/200][450/462] Loss_D: 0.5000 Loss_G: 3.4102 D(x): 0.7151 D(G(z)): 0.0248 / 0.0691 [87/200][0/462] Loss_D: 0.5855 Loss_G: 4.5899 D(x): 0.9344 D(G(z)): 0.1610 / 0.0131 [87/200][50/462] Loss_D: 0.4354 Loss_G: 4.5610 D(x): 0.8168 D(G(z)): 0.0239 / 0.0214 [87/200][100/462] Loss_D: 0.4532 Loss_G: 4.2063 D(x): 0.8693 D(G(z)): 0.0684 / 0.0186 [87/200][150/462] Loss_D: 0.7074 Loss_G: 2.7886 D(x): 0.6084 D(G(z)): 0.0724 / 0.0948 [87/200][200/462] Loss_D: 0.4577 Loss_G: 4.0759 D(x): 0.8869 D(G(z)): 0.0895 / 0.0188 [87/200][250/462] Loss_D: 0.8780 Loss_G: 6.7114 D(x): 0.9669 D(G(z)): 0.2895 / 0.0029 [87/200][300/462] Loss_D: 0.4283 Loss_G: 3.9799 D(x): 0.8822 D(G(z)): 0.0542 / 0.0226 [87/200][350/462] Loss_D: 0.4967 Loss_G: 3.9872 D(x): 0.7213 D(G(z)): 0.0121 / 0.0300 [87/200][400/462] Loss_D: 0.5943 Loss_G: 4.6120 D(x): 0.9452 D(G(z)): 0.1662 / 0.0143 [87/200][450/462] Loss_D: 0.4277 Loss_G: 4.4349 D(x): 0.8369 D(G(z)): 0.0256 / 0.0179 [88/200][0/462] Loss_D: 0.3804 Loss_G: 5.7591 D(x): 0.8534 D(G(z)): 0.0132 / 0.0075 [88/200][50/462] Loss_D: 0.5771 Loss_G: 4.0803 D(x): 0.6588 D(G(z)): 0.0388 / 0.0257 [88/200][100/462] Loss_D: 0.4821 Loss_G: 3.9436 D(x): 0.8017 D(G(z)): 0.0500 / 0.0257 [88/200][150/462] Loss_D: 0.4003 Loss_G: 4.6834 D(x): 0.9143 D(G(z)): 0.0411 / 0.0126 [88/200][200/462] Loss_D: 0.5457 Loss_G: 5.2567 D(x): 0.8799 D(G(z)): 0.1187 / 0.0089 [88/200][250/462] Loss_D: 0.4666 Loss_G: 4.4579 D(x): 0.8071 D(G(z)): 0.0461 / 0.0246 [88/200][300/462] Loss_D: 0.6260 Loss_G: 4.2441 D(x): 0.6902 D(G(z)): 0.0415 / 0.0408 [88/200][350/462] Loss_D: 0.5690 Loss_G: 5.3491 D(x): 0.9725 D(G(z)): 0.1093 / 0.0071 [88/200][400/462] Loss_D: 0.8792 Loss_G: 3.4684 D(x): 0.4816 D(G(z)): 0.0106 / 0.0545 [88/200][450/462] Loss_D: 0.5151 Loss_G: 4.1213 D(x): 0.7507 D(G(z)): 0.0244 / 0.0277 [89/200][0/462] Loss_D: 0.4813 Loss_G: 4.5193 D(x): 0.7985 D(G(z)): 0.0260 / 0.0162 [89/200][50/462] Loss_D: 0.6493 Loss_G: 3.1905 D(x): 0.6894 D(G(z)): 0.0587 / 0.0608 [89/200][100/462] Loss_D: 0.4236 Loss_G: 5.6378 D(x): 0.8033 D(G(z)): 0.0159 / 0.0070 [89/200][150/462] Loss_D: 0.5637 Loss_G: 4.5853 D(x): 0.7866 D(G(z)): 0.1034 / 0.0125 [89/200][200/462] Loss_D: 1.0281 Loss_G: 3.2686 D(x): 0.4829 D(G(z)): 0.0136 / 0.1127 [89/200][250/462] Loss_D: 0.4488 Loss_G: 4.5173 D(x): 0.8752 D(G(z)): 0.0807 / 0.0151 [89/200][300/462] Loss_D: 0.4660 Loss_G: 4.7237 D(x): 0.9502 D(G(z)): 0.0639 / 0.0123 [89/200][350/462] Loss_D: 0.3933 Loss_G: 4.6863 D(x): 0.9182 D(G(z)): 0.0241 / 0.0158 [89/200][400/462] Loss_D: 0.4926 Loss_G: 4.4244 D(x): 0.7459 D(G(z)): 0.0191 / 0.0258 [89/200][450/462] Loss_D: 0.4335 Loss_G: 3.6198 D(x): 0.8756 D(G(z)): 0.0563 / 0.0343 [90/200][0/462] Loss_D: 0.3848 Loss_G: 5.5081 D(x): 0.8982 D(G(z)): 0.0211 / 0.0086 [90/200][50/462] Loss_D: 0.4550 Loss_G: 4.6307 D(x): 0.9289 D(G(z)): 0.0758 / 0.0160 [90/200][100/462] Loss_D: 0.4833 Loss_G: 4.3955 D(x): 0.8134 D(G(z)): 0.0282 / 0.0199 [90/200][150/462] Loss_D: 0.6276 Loss_G: 4.8171 D(x): 0.8197 D(G(z)): 0.1283 / 0.0117 [90/200][200/462] Loss_D: 0.5270 Loss_G: 4.0933 D(x): 0.9123 D(G(z)): 0.0809 / 0.0279 [90/200][250/462] Loss_D: 0.6460 Loss_G: 5.0530 D(x): 0.9175 D(G(z)): 0.1984 / 0.0112 [90/200][300/462] Loss_D: 0.6165 Loss_G: 4.7768 D(x): 0.9366 D(G(z)): 0.1731 / 0.0128 [90/200][350/462] Loss_D: 0.8340 Loss_G: 2.4659 D(x): 0.5402 D(G(z)): 0.0665 / 0.1200 [90/200][400/462] Loss_D: 0.5021 Loss_G: 4.6561 D(x): 0.9464 D(G(z)): 0.0787 / 0.0125 [90/200][450/462] Loss_D: 0.5317 Loss_G: 3.7947 D(x): 0.8090 D(G(z)): 0.0476 / 0.0341 . [91/200][0/462] Loss_D: 0.5223 Loss_G: 5.2416 D(x): 0.8889 D(G(z)): 0.1248 / 0.0076 [91/200][50/462] Loss_D: 0.6675 Loss_G: 4.9765 D(x): 0.9335 D(G(z)): 0.2141 / 0.0079 [91/200][100/462] Loss_D: 0.4676 Loss_G: 4.4572 D(x): 0.9199 D(G(z)): 0.0896 / 0.0164 [91/200][150/462] Loss_D: 0.4704 Loss_G: 4.9493 D(x): 0.9273 D(G(z)): 0.0855 / 0.0112 [91/200][200/462] Loss_D: 0.6430 Loss_G: 5.3261 D(x): 0.9639 D(G(z)): 0.1655 / 0.0087 [91/200][250/462] Loss_D: 0.4737 Loss_G: 4.9837 D(x): 0.9513 D(G(z)): 0.0912 / 0.0130 [91/200][300/462] Loss_D: 0.5035 Loss_G: 4.2050 D(x): 0.8517 D(G(z)): 0.0821 / 0.0249 [91/200][350/462] Loss_D: 0.5478 Loss_G: 4.4972 D(x): 0.9023 D(G(z)): 0.0668 / 0.0186 [91/200][400/462] Loss_D: 0.4972 Loss_G: 3.9865 D(x): 0.8627 D(G(z)): 0.0702 / 0.0255 [91/200][450/462] Loss_D: 0.4341 Loss_G: 4.1339 D(x): 0.8242 D(G(z)): 0.0293 / 0.0260 [92/200][0/462] Loss_D: 0.5203 Loss_G: 3.8987 D(x): 0.8478 D(G(z)): 0.1036 / 0.0233 [92/200][50/462] Loss_D: 0.4766 Loss_G: 4.2151 D(x): 0.7454 D(G(z)): 0.0150 / 0.0309 [92/200][100/462] Loss_D: 0.7794 Loss_G: 3.6078 D(x): 0.5641 D(G(z)): 0.0315 / 0.0880 [92/200][150/462] Loss_D: 0.7599 Loss_G: 6.7807 D(x): 0.9249 D(G(z)): 0.2549 / 0.0019 [92/200][200/462] Loss_D: 0.6529 Loss_G: 5.4855 D(x): 0.9476 D(G(z)): 0.1845 / 0.0054 [92/200][250/462] Loss_D: 0.4794 Loss_G: 4.3068 D(x): 0.7939 D(G(z)): 0.0339 / 0.0223 [92/200][300/462] Loss_D: 0.8006 Loss_G: 2.8210 D(x): 0.5968 D(G(z)): 0.0349 / 0.0874 [92/200][350/462] Loss_D: 0.4959 Loss_G: 4.3533 D(x): 0.8348 D(G(z)): 0.0675 / 0.0219 [92/200][400/462] Loss_D: 0.5887 Loss_G: 4.2908 D(x): 0.7649 D(G(z)): 0.0470 / 0.0212 [92/200][450/462] Loss_D: 0.4837 Loss_G: 4.5546 D(x): 0.7912 D(G(z)): 0.0115 / 0.0203 [93/200][0/462] Loss_D: 0.4830 Loss_G: 4.4469 D(x): 0.9296 D(G(z)): 0.0868 / 0.0154 [93/200][50/462] Loss_D: 0.4239 Loss_G: 4.1416 D(x): 0.8505 D(G(z)): 0.0294 / 0.0216 [93/200][100/462] Loss_D: 0.4246 Loss_G: 4.5828 D(x): 0.8779 D(G(z)): 0.0442 / 0.0200 [93/200][150/462] Loss_D: 0.6057 Loss_G: 5.2217 D(x): 0.8542 D(G(z)): 0.1398 / 0.0097 [93/200][200/462] Loss_D: 0.8193 Loss_G: 3.2699 D(x): 0.5156 D(G(z)): 0.0088 / 0.0904 [93/200][250/462] Loss_D: 0.4298 Loss_G: 5.2972 D(x): 0.8066 D(G(z)): 0.0065 / 0.0098 [93/200][300/462] Loss_D: 0.6458 Loss_G: 4.2854 D(x): 0.9826 D(G(z)): 0.1267 / 0.0169 [93/200][350/462] Loss_D: 0.5444 Loss_G: 4.0009 D(x): 0.7442 D(G(z)): 0.0371 / 0.0349 [93/200][400/462] Loss_D: 0.4626 Loss_G: 4.8924 D(x): 0.9575 D(G(z)): 0.0616 / 0.0113 [93/200][450/462] Loss_D: 0.4383 Loss_G: 4.2784 D(x): 0.8077 D(G(z)): 0.0159 / 0.0203 [94/200][0/462] Loss_D: 0.4415 Loss_G: 3.9680 D(x): 0.8719 D(G(z)): 0.0531 / 0.0236 [94/200][50/462] Loss_D: 0.6329 Loss_G: 3.5662 D(x): 0.6050 D(G(z)): 0.0167 / 0.0436 [94/200][100/462] Loss_D: 0.5086 Loss_G: 4.5531 D(x): 0.8069 D(G(z)): 0.0924 / 0.0176 [94/200][150/462] Loss_D: 0.6418 Loss_G: 3.7927 D(x): 0.6611 D(G(z)): 0.0258 / 0.0363 [94/200][200/462] Loss_D: 0.5488 Loss_G: 3.8645 D(x): 0.7845 D(G(z)): 0.0885 / 0.0339 [94/200][250/462] Loss_D: 0.5228 Loss_G: 5.0557 D(x): 0.9324 D(G(z)): 0.1350 / 0.0079 [94/200][300/462] Loss_D: 0.5042 Loss_G: 4.4885 D(x): 0.9071 D(G(z)): 0.0826 / 0.0161 [94/200][350/462] Loss_D: 0.5234 Loss_G: 4.9706 D(x): 0.9577 D(G(z)): 0.0767 / 0.0088 [94/200][400/462] Loss_D: 0.5420 Loss_G: 4.7844 D(x): 0.8318 D(G(z)): 0.0915 / 0.0117 [94/200][450/462] Loss_D: 0.4470 Loss_G: 4.3404 D(x): 0.9301 D(G(z)): 0.0544 / 0.0160 [95/200][0/462] Loss_D: 0.4664 Loss_G: 3.9558 D(x): 0.8845 D(G(z)): 0.0840 / 0.0255 [95/200][50/462] Loss_D: 0.7596 Loss_G: 5.5477 D(x): 0.9858 D(G(z)): 0.2202 / 0.0046 [95/200][100/462] Loss_D: 0.5738 Loss_G: 3.6479 D(x): 0.7183 D(G(z)): 0.0428 / 0.0525 [95/200][150/462] Loss_D: 0.6818 Loss_G: 5.3599 D(x): 0.9340 D(G(z)): 0.1444 / 0.0103 [95/200][200/462] Loss_D: 0.4688 Loss_G: 4.1543 D(x): 0.8590 D(G(z)): 0.0809 / 0.0234 [95/200][250/462] Loss_D: 0.8645 Loss_G: 2.6320 D(x): 0.4880 D(G(z)): 0.0155 / 0.1200 [95/200][300/462] Loss_D: 0.4614 Loss_G: 2.8742 D(x): 0.7749 D(G(z)): 0.0210 / 0.0722 [95/200][350/462] Loss_D: 0.4113 Loss_G: 5.3683 D(x): 0.9556 D(G(z)): 0.0232 / 0.0070 [95/200][400/462] Loss_D: 0.5116 Loss_G: 5.0718 D(x): 0.8808 D(G(z)): 0.0808 / 0.0109 [95/200][450/462] Loss_D: 0.4961 Loss_G: 3.9326 D(x): 0.7687 D(G(z)): 0.0273 / 0.0385 . [96/200][0/462] Loss_D: 0.5261 Loss_G: 4.3745 D(x): 0.8882 D(G(z)): 0.1135 / 0.0201 [96/200][50/462] Loss_D: 0.3936 Loss_G: 5.4228 D(x): 0.8828 D(G(z)): 0.0240 / 0.0083 [96/200][100/462] Loss_D: 0.5229 Loss_G: 4.8262 D(x): 0.8241 D(G(z)): 0.0835 / 0.0133 [96/200][150/462] Loss_D: 0.5581 Loss_G: 4.2740 D(x): 0.8341 D(G(z)): 0.1230 / 0.0184 [96/200][200/462] Loss_D: 0.4569 Loss_G: 3.7517 D(x): 0.8198 D(G(z)): 0.0531 / 0.0328 [96/200][250/462] Loss_D: 0.4221 Loss_G: 6.2072 D(x): 0.9171 D(G(z)): 0.0171 / 0.0045 [96/200][300/462] Loss_D: 0.9985 Loss_G: 4.2654 D(x): 0.4822 D(G(z)): 0.0127 / 0.0407 [96/200][350/462] Loss_D: 0.4595 Loss_G: 4.1588 D(x): 0.8742 D(G(z)): 0.0826 / 0.0204 [96/200][400/462] Loss_D: 0.4880 Loss_G: 4.4790 D(x): 0.8944 D(G(z)): 0.0840 / 0.0172 [96/200][450/462] Loss_D: 0.6499 Loss_G: 2.4972 D(x): 0.6030 D(G(z)): 0.0256 / 0.1274 [97/200][0/462] Loss_D: 0.6381 Loss_G: 4.2498 D(x): 0.6660 D(G(z)): 0.0350 / 0.0288 [97/200][50/462] Loss_D: 0.4960 Loss_G: 5.3820 D(x): 0.8830 D(G(z)): 0.1029 / 0.0084 [97/200][100/462] Loss_D: 0.4945 Loss_G: 4.6975 D(x): 0.9534 D(G(z)): 0.0914 / 0.0129 [97/200][150/462] Loss_D: 0.5672 Loss_G: 5.0334 D(x): 0.8838 D(G(z)): 0.1560 / 0.0105 [97/200][200/462] Loss_D: 0.6192 Loss_G: 3.1056 D(x): 0.7505 D(G(z)): 0.0492 / 0.0690 [97/200][250/462] Loss_D: 0.4616 Loss_G: 5.1842 D(x): 0.8758 D(G(z)): 0.0131 / 0.0139 [97/200][300/462] Loss_D: 0.4759 Loss_G: 6.1999 D(x): 0.7580 D(G(z)): 0.0096 / 0.0035 [97/200][350/462] Loss_D: 0.4664 Loss_G: 4.5551 D(x): 0.9507 D(G(z)): 0.0689 / 0.0152 [97/200][400/462] Loss_D: 0.5133 Loss_G: 3.6962 D(x): 0.7453 D(G(z)): 0.0334 / 0.0423 [97/200][450/462] Loss_D: 0.4535 Loss_G: 5.7246 D(x): 0.7715 D(G(z)): 0.0082 / 0.0100 [98/200][0/462] Loss_D: 0.4718 Loss_G: 4.1420 D(x): 0.7583 D(G(z)): 0.0161 / 0.0296 [98/200][50/462] Loss_D: 0.4391 Loss_G: 4.3226 D(x): 0.8940 D(G(z)): 0.0491 / 0.0207 [98/200][100/462] Loss_D: 0.4987 Loss_G: 4.3412 D(x): 0.8177 D(G(z)): 0.0508 / 0.0185 [98/200][150/462] Loss_D: 0.5584 Loss_G: 5.0271 D(x): 0.9305 D(G(z)): 0.1313 / 0.0099 [98/200][200/462] Loss_D: 0.4241 Loss_G: 4.9262 D(x): 0.8797 D(G(z)): 0.0504 / 0.0130 [98/200][250/462] Loss_D: 0.5541 Loss_G: 5.3469 D(x): 0.9626 D(G(z)): 0.1031 / 0.0071 [98/200][300/462] Loss_D: 0.4260 Loss_G: 5.3084 D(x): 0.9360 D(G(z)): 0.0238 / 0.0072 [98/200][350/462] Loss_D: 0.6281 Loss_G: 2.7727 D(x): 0.6595 D(G(z)): 0.0387 / 0.0957 [98/200][400/462] Loss_D: 0.4945 Loss_G: 4.7050 D(x): 0.8481 D(G(z)): 0.0723 / 0.0139 [98/200][450/462] Loss_D: 0.4956 Loss_G: 3.9438 D(x): 0.7776 D(G(z)): 0.0610 / 0.0319 [99/200][0/462] Loss_D: 0.5689 Loss_G: 3.2917 D(x): 0.6968 D(G(z)): 0.0417 / 0.0732 [99/200][50/462] Loss_D: 0.5884 Loss_G: 3.8630 D(x): 0.7895 D(G(z)): 0.0954 / 0.0338 [99/200][100/462] Loss_D: 0.4892 Loss_G: 4.2049 D(x): 0.9637 D(G(z)): 0.0547 / 0.0198 [99/200][150/462] Loss_D: 0.5059 Loss_G: 3.4469 D(x): 0.7487 D(G(z)): 0.0286 / 0.0565 [99/200][200/462] Loss_D: 0.4762 Loss_G: 3.7533 D(x): 0.8713 D(G(z)): 0.0891 / 0.0318 [99/200][250/462] Loss_D: 0.5425 Loss_G: 3.3342 D(x): 0.7689 D(G(z)): 0.0324 / 0.0568 [99/200][300/462] Loss_D: 1.1207 Loss_G: 7.0887 D(x): 0.9709 D(G(z)): 0.4045 / 0.0013 [99/200][350/462] Loss_D: 0.8755 Loss_G: 6.0424 D(x): 0.9850 D(G(z)): 0.2993 / 0.0042 [99/200][400/462] Loss_D: 0.4740 Loss_G: 3.6208 D(x): 0.8746 D(G(z)): 0.0837 / 0.0389 [99/200][450/462] Loss_D: 0.4250 Loss_G: 5.9075 D(x): 0.9409 D(G(z)): 0.0391 / 0.0038 [100/200][0/462] Loss_D: 0.5837 Loss_G: 5.3287 D(x): 0.8057 D(G(z)): 0.0876 / 0.0075 [100/200][50/462] Loss_D: 0.5210 Loss_G: 4.5685 D(x): 0.8174 D(G(z)): 0.0914 / 0.0189 [100/200][100/462] Loss_D: 0.6573 Loss_G: 2.8714 D(x): 0.6494 D(G(z)): 0.0529 / 0.0949 [100/200][150/462] Loss_D: 0.4781 Loss_G: 3.9614 D(x): 0.8281 D(G(z)): 0.0724 / 0.0320 [100/200][200/462] Loss_D: 0.4554 Loss_G: 4.1390 D(x): 0.8455 D(G(z)): 0.0354 / 0.0240 [100/200][250/462] Loss_D: 0.4498 Loss_G: 5.4217 D(x): 0.9391 D(G(z)): 0.0538 / 0.0099 [100/200][300/462] Loss_D: 0.4633 Loss_G: 4.2467 D(x): 0.8362 D(G(z)): 0.0874 / 0.0185 [100/200][350/462] Loss_D: 0.4324 Loss_G: 4.0875 D(x): 0.9007 D(G(z)): 0.0389 / 0.0341 [100/200][400/462] Loss_D: 0.4310 Loss_G: 4.0420 D(x): 0.8766 D(G(z)): 0.0636 / 0.0252 [100/200][450/462] Loss_D: 0.4727 Loss_G: 5.1929 D(x): 0.9097 D(G(z)): 0.0570 / 0.0095 . [101/200][0/462] Loss_D: 0.4103 Loss_G: 5.1335 D(x): 0.8860 D(G(z)): 0.0360 / 0.0121 [101/200][50/462] Loss_D: 0.4454 Loss_G: 4.2532 D(x): 0.8908 D(G(z)): 0.0652 / 0.0207 [101/200][100/462] Loss_D: 0.4836 Loss_G: 4.6021 D(x): 0.9175 D(G(z)): 0.0553 / 0.0141 [101/200][150/462] Loss_D: 0.4589 Loss_G: 4.2504 D(x): 0.8289 D(G(z)): 0.0183 / 0.0191 [101/200][200/462] Loss_D: 0.5106 Loss_G: 4.8361 D(x): 0.9415 D(G(z)): 0.1078 / 0.0093 [101/200][250/462] Loss_D: 0.6056 Loss_G: 4.6786 D(x): 0.9454 D(G(z)): 0.1692 / 0.0141 [101/200][300/462] Loss_D: 0.5204 Loss_G: 4.7087 D(x): 0.8557 D(G(z)): 0.0877 / 0.0151 [101/200][350/462] Loss_D: 0.4248 Loss_G: 5.0137 D(x): 0.8320 D(G(z)): 0.0243 / 0.0120 [101/200][400/462] Loss_D: 0.5650 Loss_G: 3.0860 D(x): 0.6639 D(G(z)): 0.0307 / 0.0662 [101/200][450/462] Loss_D: 0.4462 Loss_G: 5.0021 D(x): 0.8464 D(G(z)): 0.0449 / 0.0120 [102/200][0/462] Loss_D: 0.4323 Loss_G: 3.9251 D(x): 0.9067 D(G(z)): 0.0375 / 0.0261 [102/200][50/462] Loss_D: 0.5050 Loss_G: 3.6849 D(x): 0.7989 D(G(z)): 0.1012 / 0.0325 [102/200][100/462] Loss_D: 0.6082 Loss_G: 4.8111 D(x): 0.9236 D(G(z)): 0.1453 / 0.0115 [102/200][150/462] Loss_D: 0.4499 Loss_G: 4.7479 D(x): 0.8255 D(G(z)): 0.0725 / 0.0127 [102/200][200/462] Loss_D: 0.5916 Loss_G: 3.3263 D(x): 0.7132 D(G(z)): 0.0754 / 0.0443 [102/200][250/462] Loss_D: 0.5688 Loss_G: 3.6349 D(x): 0.6808 D(G(z)): 0.0374 / 0.0482 [102/200][300/462] Loss_D: 0.4898 Loss_G: 4.6620 D(x): 0.8475 D(G(z)): 0.0382 / 0.0142 [102/200][350/462] Loss_D: 0.4612 Loss_G: 5.3680 D(x): 0.7950 D(G(z)): 0.0252 / 0.0133 [102/200][400/462] Loss_D: 0.4428 Loss_G: 4.6643 D(x): 0.9320 D(G(z)): 0.0598 / 0.0133 [102/200][450/462] Loss_D: 0.4235 Loss_G: 4.4786 D(x): 0.8941 D(G(z)): 0.0581 / 0.0182 [103/200][0/462] Loss_D: 0.5216 Loss_G: 4.2248 D(x): 0.7911 D(G(z)): 0.0783 / 0.0221 [103/200][50/462] Loss_D: 0.4892 Loss_G: 4.9378 D(x): 0.9114 D(G(z)): 0.0800 / 0.0096 [103/200][100/462] Loss_D: 0.6474 Loss_G: 3.4116 D(x): 0.6176 D(G(z)): 0.0143 / 0.0442 [103/200][150/462] Loss_D: 0.4164 Loss_G: 4.9417 D(x): 0.8621 D(G(z)): 0.0554 / 0.0118 [103/200][200/462] Loss_D: 0.4191 Loss_G: 4.2020 D(x): 0.8682 D(G(z)): 0.0348 / 0.0191 [103/200][250/462] Loss_D: 0.4424 Loss_G: 5.1635 D(x): 0.8764 D(G(z)): 0.0282 / 0.0098 [103/200][300/462] Loss_D: 0.4516 Loss_G: 4.3820 D(x): 0.8607 D(G(z)): 0.0716 / 0.0172 [103/200][350/462] Loss_D: 0.4406 Loss_G: 5.3260 D(x): 0.9600 D(G(z)): 0.0366 / 0.0089 [103/200][400/462] Loss_D: 0.4799 Loss_G: 3.6252 D(x): 0.8296 D(G(z)): 0.0761 / 0.0343 [103/200][450/462] Loss_D: 0.5098 Loss_G: 4.3837 D(x): 0.7687 D(G(z)): 0.0204 / 0.0264 [104/200][0/462] Loss_D: 0.5831 Loss_G: 5.5061 D(x): 0.9175 D(G(z)): 0.1804 / 0.0064 [104/200][50/462] Loss_D: 0.6931 Loss_G: 5.5083 D(x): 0.9711 D(G(z)): 0.2292 / 0.0046 [104/200][100/462] Loss_D: 0.4615 Loss_G: 4.5030 D(x): 0.8356 D(G(z)): 0.0268 / 0.0176 [104/200][150/462] Loss_D: 0.4759 Loss_G: 3.8928 D(x): 0.8918 D(G(z)): 0.0872 / 0.0258 [104/200][200/462] Loss_D: 0.4155 Loss_G: 4.4536 D(x): 0.8667 D(G(z)): 0.0384 / 0.0200 [104/200][250/462] Loss_D: 0.5656 Loss_G: 5.0344 D(x): 0.7187 D(G(z)): 0.0149 / 0.0126 [104/200][300/462] Loss_D: 0.5032 Loss_G: 4.9535 D(x): 0.8838 D(G(z)): 0.1116 / 0.0108 [104/200][350/462] Loss_D: 0.5174 Loss_G: 4.2938 D(x): 0.7380 D(G(z)): 0.0354 / 0.0235 [104/200][400/462] Loss_D: 0.9406 Loss_G: 3.0307 D(x): 0.4986 D(G(z)): 0.0159 / 0.1193 [104/200][450/462] Loss_D: 0.3993 Loss_G: 5.8000 D(x): 0.8780 D(G(z)): 0.0414 / 0.0060 [105/200][0/462] Loss_D: 0.5884 Loss_G: 4.4722 D(x): 0.8293 D(G(z)): 0.0815 / 0.0157 [105/200][50/462] Loss_D: 0.4419 Loss_G: 4.1565 D(x): 0.8890 D(G(z)): 0.0630 / 0.0215 [105/200][100/462] Loss_D: 0.4769 Loss_G: 5.3612 D(x): 0.8089 D(G(z)): 0.0462 / 0.0111 [105/200][150/462] Loss_D: 0.5443 Loss_G: 4.2397 D(x): 0.6878 D(G(z)): 0.0109 / 0.0273 [105/200][200/462] Loss_D: 0.7077 Loss_G: 4.6794 D(x): 0.8801 D(G(z)): 0.2355 / 0.0113 [105/200][250/462] Loss_D: 0.7236 Loss_G: 6.3301 D(x): 0.9735 D(G(z)): 0.2195 / 0.0024 [105/200][300/462] Loss_D: 0.5034 Loss_G: 4.9167 D(x): 0.8685 D(G(z)): 0.1087 / 0.0090 [105/200][350/462] Loss_D: 0.5140 Loss_G: 4.1150 D(x): 0.7772 D(G(z)): 0.0323 / 0.0287 [105/200][400/462] Loss_D: 0.5446 Loss_G: 5.4952 D(x): 0.9535 D(G(z)): 0.0990 / 0.0051 [105/200][450/462] Loss_D: 0.4431 Loss_G: 4.6688 D(x): 0.8058 D(G(z)): 0.0126 / 0.0160 . [106/200][0/462] Loss_D: 0.6333 Loss_G: 4.9681 D(x): 0.9689 D(G(z)): 0.1859 / 0.0085 [106/200][50/462] Loss_D: 0.5838 Loss_G: 3.8735 D(x): 0.7256 D(G(z)): 0.0544 / 0.0281 [106/200][100/462] Loss_D: 0.4873 Loss_G: 4.6812 D(x): 0.9483 D(G(z)): 0.0647 / 0.0120 [106/200][150/462] Loss_D: 0.4110 Loss_G: 4.3246 D(x): 0.8491 D(G(z)): 0.0332 / 0.0179 [106/200][200/462] Loss_D: 0.9049 Loss_G: 2.6501 D(x): 0.4569 D(G(z)): 0.0069 / 0.1008 [106/200][250/462] Loss_D: 0.5924 Loss_G: 3.8550 D(x): 0.8607 D(G(z)): 0.1180 / 0.0277 [106/200][300/462] Loss_D: 0.4527 Loss_G: 4.8654 D(x): 0.8907 D(G(z)): 0.0525 / 0.0116 [106/200][350/462] Loss_D: 0.4774 Loss_G: 4.4497 D(x): 0.8795 D(G(z)): 0.0445 / 0.0179 [106/200][400/462] Loss_D: 0.5455 Loss_G: 4.2359 D(x): 0.9224 D(G(z)): 0.1296 / 0.0198 [106/200][450/462] Loss_D: 0.6601 Loss_G: 6.7560 D(x): 0.8996 D(G(z)): 0.2088 / 0.0017 [107/200][0/462] Loss_D: 0.5576 Loss_G: 4.1022 D(x): 0.7636 D(G(z)): 0.0987 / 0.0256 [107/200][50/462] Loss_D: 0.4878 Loss_G: 4.9620 D(x): 0.8867 D(G(z)): 0.0891 / 0.0114 [107/200][100/462] Loss_D: 0.4457 Loss_G: 4.3574 D(x): 0.8310 D(G(z)): 0.0588 / 0.0176 [107/200][150/462] Loss_D: 0.4417 Loss_G: 4.7430 D(x): 0.8214 D(G(z)): 0.0179 / 0.0150 [107/200][200/462] Loss_D: 0.4688 Loss_G: 5.5815 D(x): 0.9370 D(G(z)): 0.0623 / 0.0082 [107/200][250/462] Loss_D: 0.5182 Loss_G: 4.9392 D(x): 0.9594 D(G(z)): 0.1133 / 0.0112 [107/200][300/462] Loss_D: 0.5706 Loss_G: 5.3105 D(x): 0.9001 D(G(z)): 0.1575 / 0.0078 [107/200][350/462] Loss_D: 0.4924 Loss_G: 3.6342 D(x): 0.8362 D(G(z)): 0.0739 / 0.0321 [107/200][400/462] Loss_D: 0.3802 Loss_G: 4.8603 D(x): 0.9030 D(G(z)): 0.0311 / 0.0144 [107/200][450/462] Loss_D: 0.4928 Loss_G: 4.7272 D(x): 0.8688 D(G(z)): 0.0755 / 0.0143 [108/200][0/462] Loss_D: 0.3889 Loss_G: 5.1090 D(x): 0.9139 D(G(z)): 0.0280 / 0.0114 [108/200][50/462] Loss_D: 0.4883 Loss_G: 5.2057 D(x): 0.8619 D(G(z)): 0.0942 / 0.0114 [108/200][100/462] Loss_D: 0.4390 Loss_G: 4.2869 D(x): 0.8885 D(G(z)): 0.0580 / 0.0208 [108/200][150/462] Loss_D: 0.5137 Loss_G: 4.7209 D(x): 0.8243 D(G(z)): 0.0776 / 0.0137 [108/200][200/462] Loss_D: 0.4234 Loss_G: 4.1099 D(x): 0.8472 D(G(z)): 0.0366 / 0.0293 [108/200][250/462] Loss_D: 0.4231 Loss_G: 4.6661 D(x): 0.8904 D(G(z)): 0.0359 / 0.0133 [108/200][300/462] Loss_D: 0.6811 Loss_G: 3.1615 D(x): 0.6071 D(G(z)): 0.0433 / 0.0809 [108/200][350/462] Loss_D: 0.4465 Loss_G: 4.6644 D(x): 0.8735 D(G(z)): 0.0396 / 0.0183 [108/200][400/462] Loss_D: 0.4708 Loss_G: 3.6564 D(x): 0.7627 D(G(z)): 0.0232 / 0.0365 [108/200][450/462] Loss_D: 0.4290 Loss_G: 4.8447 D(x): 0.8690 D(G(z)): 0.0392 / 0.0127 [109/200][0/462] Loss_D: 0.4203 Loss_G: 4.1164 D(x): 0.8215 D(G(z)): 0.0262 / 0.0269 [109/200][50/462] Loss_D: 0.5768 Loss_G: 4.2537 D(x): 0.9007 D(G(z)): 0.1424 / 0.0162 [109/200][100/462] Loss_D: 0.4706 Loss_G: 4.2922 D(x): 0.7775 D(G(z)): 0.0321 / 0.0200 [109/200][150/462] Loss_D: 0.5739 Loss_G: 5.1321 D(x): 0.9548 D(G(z)): 0.1327 / 0.0083 [109/200][200/462] Loss_D: 0.4764 Loss_G: 4.2946 D(x): 0.8440 D(G(z)): 0.0560 / 0.0190 [109/200][250/462] Loss_D: 0.5102 Loss_G: 3.9546 D(x): 0.7545 D(G(z)): 0.0775 / 0.0327 [109/200][300/462] Loss_D: 0.6168 Loss_G: 4.5989 D(x): 0.9856 D(G(z)): 0.1022 / 0.0135 [109/200][350/462] Loss_D: 0.4531 Loss_G: 3.9996 D(x): 0.7580 D(G(z)): 0.0166 / 0.0266 [109/200][400/462] Loss_D: 0.5082 Loss_G: 4.3098 D(x): 0.7576 D(G(z)): 0.0402 / 0.0334 [109/200][450/462] Loss_D: 0.4859 Loss_G: 4.6289 D(x): 0.9391 D(G(z)): 0.1070 / 0.0172 [110/200][0/462] Loss_D: 0.4711 Loss_G: 4.8816 D(x): 0.9237 D(G(z)): 0.0764 / 0.0098 [110/200][50/462] Loss_D: 0.6761 Loss_G: 4.8270 D(x): 0.8518 D(G(z)): 0.2083 / 0.0137 [110/200][100/462] Loss_D: 0.5276 Loss_G: 4.8656 D(x): 0.7513 D(G(z)): 0.0558 / 0.0126 [110/200][150/462] Loss_D: 0.4943 Loss_G: 3.5078 D(x): 0.7573 D(G(z)): 0.0518 / 0.0399 [110/200][200/462] Loss_D: 0.4283 Loss_G: 4.7066 D(x): 0.9489 D(G(z)): 0.0376 / 0.0124 [110/200][250/462] Loss_D: 0.5466 Loss_G: 4.4336 D(x): 0.9581 D(G(z)): 0.1119 / 0.0138 [110/200][300/462] Loss_D: 0.5489 Loss_G: 5.0095 D(x): 0.9060 D(G(z)): 0.1427 / 0.0096 [110/200][350/462] Loss_D: 0.5628 Loss_G: 6.0506 D(x): 0.9907 D(G(z)): 0.0429 / 0.0048 [110/200][400/462] Loss_D: 0.6001 Loss_G: 3.7249 D(x): 0.6303 D(G(z)): 0.0122 / 0.0436 [110/200][450/462] Loss_D: 0.8349 Loss_G: 3.0172 D(x): 0.4934 D(G(z)): 0.0236 / 0.0958 . [111/200][0/462] Loss_D: 0.4858 Loss_G: 4.7463 D(x): 0.8683 D(G(z)): 0.0196 / 0.0149 [111/200][50/462] Loss_D: 0.6802 Loss_G: 3.2851 D(x): 0.6403 D(G(z)): 0.0789 / 0.0556 [111/200][100/462] Loss_D: 0.4294 Loss_G: 5.3121 D(x): 0.9262 D(G(z)): 0.0221 / 0.0099 [111/200][150/462] Loss_D: 0.4783 Loss_G: 4.9629 D(x): 0.8668 D(G(z)): 0.0652 / 0.0123 [111/200][200/462] Loss_D: 0.5031 Loss_G: 4.3793 D(x): 0.9456 D(G(z)): 0.1069 / 0.0159 [111/200][250/462] Loss_D: 0.4342 Loss_G: 3.8650 D(x): 0.8381 D(G(z)): 0.0430 / 0.0247 [111/200][300/462] Loss_D: 0.6383 Loss_G: 3.0920 D(x): 0.6590 D(G(z)): 0.0341 / 0.0765 [111/200][350/462] Loss_D: 0.4256 Loss_G: 4.5304 D(x): 0.8926 D(G(z)): 0.0607 / 0.0133 [111/200][400/462] Loss_D: 0.4658 Loss_G: 4.2827 D(x): 0.8212 D(G(z)): 0.0371 / 0.0210 [111/200][450/462] Loss_D: 0.4360 Loss_G: 4.6984 D(x): 0.8306 D(G(z)): 0.0333 / 0.0182 [112/200][0/462] Loss_D: 0.4275 Loss_G: 3.4437 D(x): 0.8019 D(G(z)): 0.0384 / 0.0548 [112/200][50/462] Loss_D: 0.4856 Loss_G: 4.3481 D(x): 0.9027 D(G(z)): 0.0622 / 0.0194 [112/200][100/462] Loss_D: 0.4483 Loss_G: 4.5670 D(x): 0.8576 D(G(z)): 0.0481 / 0.0171 [112/200][150/462] Loss_D: 0.5387 Loss_G: 5.1105 D(x): 0.9371 D(G(z)): 0.1194 / 0.0076 [112/200][200/462] Loss_D: 0.4297 Loss_G: 4.0445 D(x): 0.9262 D(G(z)): 0.0591 / 0.0235 [112/200][250/462] Loss_D: 0.4685 Loss_G: 4.8142 D(x): 0.9072 D(G(z)): 0.0762 / 0.0127 [112/200][300/462] Loss_D: 0.6194 Loss_G: 4.1838 D(x): 0.9567 D(G(z)): 0.1367 / 0.0167 [112/200][350/462] Loss_D: 0.7415 Loss_G: 6.1355 D(x): 0.9589 D(G(z)): 0.2051 / 0.0033 [112/200][400/462] Loss_D: 0.6267 Loss_G: 6.5851 D(x): 0.9767 D(G(z)): 0.1281 / 0.0024 [112/200][450/462] Loss_D: 0.4065 Loss_G: 4.8826 D(x): 0.8693 D(G(z)): 0.0227 / 0.0125 [113/200][0/462] Loss_D: 0.5862 Loss_G: 4.9545 D(x): 0.9663 D(G(z)): 0.1362 / 0.0080 [113/200][50/462] Loss_D: 0.6086 Loss_G: 5.0524 D(x): 0.7899 D(G(z)): 0.1425 / 0.0104 [113/200][100/462] Loss_D: 0.6758 Loss_G: 5.7840 D(x): 0.9630 D(G(z)): 0.2014 / 0.0048 [113/200][150/462] Loss_D: 0.4785 Loss_G: 4.2317 D(x): 0.9297 D(G(z)): 0.0926 / 0.0190 [113/200][200/462] Loss_D: 0.5388 Loss_G: 3.9969 D(x): 0.8243 D(G(z)): 0.0914 / 0.0272 [113/200][250/462] Loss_D: 0.4973 Loss_G: 4.5201 D(x): 0.8115 D(G(z)): 0.0863 / 0.0194 [113/200][300/462] Loss_D: 0.5017 Loss_G: 4.0629 D(x): 0.8456 D(G(z)): 0.1001 / 0.0231 [113/200][350/462] Loss_D: 0.4328 Loss_G: 4.1267 D(x): 0.8593 D(G(z)): 0.0513 / 0.0231 [113/200][400/462] Loss_D: 0.5056 Loss_G: 3.3039 D(x): 0.8273 D(G(z)): 0.0823 / 0.0464 [113/200][450/462] Loss_D: 0.4666 Loss_G: 4.3905 D(x): 0.9285 D(G(z)): 0.0368 / 0.0211 [114/200][0/462] Loss_D: 0.4632 Loss_G: 3.6125 D(x): 0.7622 D(G(z)): 0.0254 / 0.0483 [114/200][50/462] Loss_D: 0.4663 Loss_G: 4.7326 D(x): 0.7810 D(G(z)): 0.0282 / 0.0157 [114/200][100/462] Loss_D: 0.5664 Loss_G: 4.0829 D(x): 0.8580 D(G(z)): 0.1325 / 0.0314 [114/200][150/462] Loss_D: 0.6211 Loss_G: 5.4139 D(x): 0.9372 D(G(z)): 0.1822 / 0.0064 [114/200][200/462] Loss_D: 0.5670 Loss_G: 3.7385 D(x): 0.6910 D(G(z)): 0.0192 / 0.0382 [114/200][250/462] Loss_D: 0.6542 Loss_G: 5.7721 D(x): 0.9244 D(G(z)): 0.1903 / 0.0045 [114/200][300/462] Loss_D: 0.4408 Loss_G: 3.5478 D(x): 0.7948 D(G(z)): 0.0305 / 0.0413 [114/200][350/462] Loss_D: 0.4276 Loss_G: 3.8682 D(x): 0.9334 D(G(z)): 0.0495 / 0.0234 [114/200][400/462] Loss_D: 0.5070 Loss_G: 4.5002 D(x): 0.7602 D(G(z)): 0.0081 / 0.0203 [114/200][450/462] Loss_D: 0.4290 Loss_G: 4.9776 D(x): 0.8643 D(G(z)): 0.0196 / 0.0119 [115/200][0/462] Loss_D: 0.4328 Loss_G: 4.4012 D(x): 0.8949 D(G(z)): 0.0548 / 0.0178 [115/200][50/462] Loss_D: 0.4536 Loss_G: 4.1776 D(x): 0.9032 D(G(z)): 0.0580 / 0.0214 [115/200][100/462] Loss_D: 0.4454 Loss_G: 4.4017 D(x): 0.7844 D(G(z)): 0.0155 / 0.0212 [115/200][150/462] Loss_D: 0.6077 Loss_G: 3.8337 D(x): 0.7244 D(G(z)): 0.0694 / 0.0460 [115/200][200/462] Loss_D: 0.3848 Loss_G: 4.2642 D(x): 0.8300 D(G(z)): 0.0169 / 0.0186 [115/200][250/462] Loss_D: 0.4171 Loss_G: 4.0477 D(x): 0.8576 D(G(z)): 0.0454 / 0.0279 [115/200][300/462] Loss_D: 0.4472 Loss_G: 4.1825 D(x): 0.8018 D(G(z)): 0.0144 / 0.0217 [115/200][350/462] Loss_D: 0.4276 Loss_G: 4.3348 D(x): 0.8134 D(G(z)): 0.0341 / 0.0215 [115/200][400/462] Loss_D: 0.4023 Loss_G: 4.0142 D(x): 0.8717 D(G(z)): 0.0239 / 0.0241 [115/200][450/462] Loss_D: 0.4007 Loss_G: 4.9480 D(x): 0.9254 D(G(z)): 0.0256 / 0.0098 . [116/200][0/462] Loss_D: 0.4951 Loss_G: 3.6158 D(x): 0.8048 D(G(z)): 0.0466 / 0.0399 [116/200][50/462] Loss_D: 0.6585 Loss_G: 3.9583 D(x): 0.7701 D(G(z)): 0.1537 / 0.0265 [116/200][100/462] Loss_D: 0.4142 Loss_G: 5.8855 D(x): 0.8508 D(G(z)): 0.0143 / 0.0070 [116/200][150/462] Loss_D: 0.4433 Loss_G: 4.2508 D(x): 0.8145 D(G(z)): 0.0323 / 0.0207 [116/200][200/462] Loss_D: 0.4281 Loss_G: 3.8489 D(x): 0.8420 D(G(z)): 0.0253 / 0.0268 [116/200][250/462] Loss_D: 0.4219 Loss_G: 4.2892 D(x): 0.9502 D(G(z)): 0.0220 / 0.0158 [116/200][300/462] Loss_D: 0.4912 Loss_G: 5.8245 D(x): 0.7698 D(G(z)): 0.0075 / 0.0095 [116/200][350/462] Loss_D: 0.4520 Loss_G: 3.5425 D(x): 0.8956 D(G(z)): 0.0761 / 0.0347 [116/200][400/462] Loss_D: 0.5146 Loss_G: 4.8857 D(x): 0.9202 D(G(z)): 0.0934 / 0.0113 [116/200][450/462] Loss_D: 0.4546 Loss_G: 4.6088 D(x): 0.7946 D(G(z)): 0.0196 / 0.0213 [117/200][0/462] Loss_D: 0.4815 Loss_G: 4.0908 D(x): 0.8306 D(G(z)): 0.0552 / 0.0295 [117/200][50/462] Loss_D: 0.5280 Loss_G: 4.2485 D(x): 0.8185 D(G(z)): 0.0512 / 0.0201 [117/200][100/462] Loss_D: 0.6066 Loss_G: 3.8390 D(x): 0.7436 D(G(z)): 0.0498 / 0.0326 [117/200][150/462] Loss_D: 0.4791 Loss_G: 4.3245 D(x): 0.9261 D(G(z)): 0.0781 / 0.0160 [117/200][200/462] Loss_D: 0.4842 Loss_G: 4.7854 D(x): 0.7464 D(G(z)): 0.0067 / 0.0171 [117/200][250/462] Loss_D: 0.4464 Loss_G: 4.6048 D(x): 0.8371 D(G(z)): 0.0196 / 0.0166 [117/200][300/462] Loss_D: 0.4611 Loss_G: 4.9339 D(x): 0.8447 D(G(z)): 0.0773 / 0.0100 [117/200][350/462] Loss_D: 0.6206 Loss_G: 3.7287 D(x): 0.6463 D(G(z)): 0.0087 / 0.0399 [117/200][400/462] Loss_D: 0.4061 Loss_G: 5.4914 D(x): 0.8605 D(G(z)): 0.0168 / 0.0065 [117/200][450/462] Loss_D: 0.6088 Loss_G: 6.0890 D(x): 0.9500 D(G(z)): 0.1565 / 0.0035 [118/200][0/462] Loss_D: 0.5215 Loss_G: 4.0086 D(x): 0.7350 D(G(z)): 0.0121 / 0.0320 [118/200][50/462] Loss_D: 0.5132 Loss_G: 5.3332 D(x): 0.8821 D(G(z)): 0.0962 / 0.0065 [118/200][100/462] Loss_D: 0.5928 Loss_G: 5.4312 D(x): 0.9525 D(G(z)): 0.1285 / 0.0058 [118/200][150/462] Loss_D: 0.5760 Loss_G: 3.3687 D(x): 0.6631 D(G(z)): 0.0156 / 0.0733 [118/200][200/462] Loss_D: 0.5917 Loss_G: 3.9235 D(x): 0.6779 D(G(z)): 0.0177 / 0.0403 [118/200][250/462] Loss_D: 0.5916 Loss_G: 3.4572 D(x): 0.6936 D(G(z)): 0.0201 / 0.0548 [118/200][300/462] Loss_D: 0.4997 Loss_G: 4.2355 D(x): 0.7786 D(G(z)): 0.0371 / 0.0275 [118/200][350/462] Loss_D: 0.4206 Loss_G: 5.5730 D(x): 0.9088 D(G(z)): 0.0173 / 0.0077 [118/200][400/462] Loss_D: 0.8271 Loss_G: 6.2855 D(x): 0.9076 D(G(z)): 0.3008 / 0.0029 [118/200][450/462] Loss_D: 0.4617 Loss_G: 4.0812 D(x): 0.9126 D(G(z)): 0.0658 / 0.0233 [119/200][0/462] Loss_D: 0.4197 Loss_G: 5.2655 D(x): 0.8215 D(G(z)): 0.0124 / 0.0096 [119/200][50/462] Loss_D: 0.4265 Loss_G: 4.1296 D(x): 0.9105 D(G(z)): 0.0449 / 0.0363 [119/200][100/462] Loss_D: 0.4955 Loss_G: 4.8502 D(x): 0.9350 D(G(z)): 0.1021 / 0.0102 [119/200][150/462] Loss_D: 0.3841 Loss_G: 5.8125 D(x): 0.9024 D(G(z)): 0.0155 / 0.0052 [119/200][200/462] Loss_D: 0.5417 Loss_G: 5.2290 D(x): 0.9701 D(G(z)): 0.0894 / 0.0065 [119/200][250/462] Loss_D: 0.4519 Loss_G: 4.5513 D(x): 0.8669 D(G(z)): 0.0522 / 0.0153 [119/200][300/462] Loss_D: 0.3799 Loss_G: 4.6449 D(x): 0.8927 D(G(z)): 0.0252 / 0.0198 [119/200][350/462] Loss_D: 0.4662 Loss_G: 4.4244 D(x): 0.8641 D(G(z)): 0.0849 / 0.0163 [119/200][400/462] Loss_D: 0.6471 Loss_G: 5.8118 D(x): 0.9341 D(G(z)): 0.2206 / 0.0036 [119/200][450/462] Loss_D: 0.4095 Loss_G: 4.0862 D(x): 0.8548 D(G(z)): 0.0217 / 0.0199 [120/200][0/462] Loss_D: 0.4126 Loss_G: 4.6215 D(x): 0.8238 D(G(z)): 0.0229 / 0.0155 [120/200][50/462] Loss_D: 0.5253 Loss_G: 3.6444 D(x): 0.7797 D(G(z)): 0.0500 / 0.0526 [120/200][100/462] Loss_D: 0.5490 Loss_G: 4.9805 D(x): 0.9164 D(G(z)): 0.1491 / 0.0088 [120/200][150/462] Loss_D: 0.4357 Loss_G: 5.1792 D(x): 0.8608 D(G(z)): 0.0155 / 0.0106 [120/200][200/462] Loss_D: 0.4203 Loss_G: 4.4860 D(x): 0.8946 D(G(z)): 0.0451 / 0.0191 [120/200][250/462] Loss_D: 0.5483 Loss_G: 4.6411 D(x): 0.8649 D(G(z)): 0.1064 / 0.0129 [120/200][300/462] Loss_D: 0.4715 Loss_G: 5.5549 D(x): 0.9526 D(G(z)): 0.0907 / 0.0074 [120/200][350/462] Loss_D: 0.4573 Loss_G: 4.0272 D(x): 0.8363 D(G(z)): 0.0511 / 0.0302 [120/200][400/462] Loss_D: 0.4515 Loss_G: 3.2972 D(x): 0.8329 D(G(z)): 0.0637 / 0.0477 [120/200][450/462] Loss_D: 0.6164 Loss_G: 3.3640 D(x): 0.6380 D(G(z)): 0.0291 / 0.0545 . [121/200][0/462] Loss_D: 0.6368 Loss_G: 5.4966 D(x): 0.8968 D(G(z)): 0.1787 / 0.0060 [121/200][50/462] Loss_D: 0.8134 Loss_G: 3.7463 D(x): 0.5703 D(G(z)): 0.0138 / 0.0414 [121/200][100/462] Loss_D: 0.5071 Loss_G: 4.1631 D(x): 0.8675 D(G(z)): 0.1148 / 0.0185 [121/200][150/462] Loss_D: 0.4826 Loss_G: 3.3815 D(x): 0.7473 D(G(z)): 0.0420 / 0.0507 [121/200][200/462] Loss_D: 0.4410 Loss_G: 4.3266 D(x): 0.9485 D(G(z)): 0.0618 / 0.0194 [121/200][250/462] Loss_D: 0.4026 Loss_G: 5.2223 D(x): 0.9134 D(G(z)): 0.0295 / 0.0135 [121/200][300/462] Loss_D: 0.4623 Loss_G: 4.5336 D(x): 0.8425 D(G(z)): 0.0552 / 0.0176 [121/200][350/462] Loss_D: 1.0661 Loss_G: 8.9427 D(x): 0.9740 D(G(z)): 0.3597 / 0.0003 [121/200][400/462] Loss_D: 0.7736 Loss_G: 2.6353 D(x): 0.5793 D(G(z)): 0.0195 / 0.1314 [121/200][450/462] Loss_D: 0.4904 Loss_G: 4.8866 D(x): 0.9450 D(G(z)): 0.0643 / 0.0108 [122/200][0/462] Loss_D: 0.4322 Loss_G: 4.4131 D(x): 0.8909 D(G(z)): 0.0406 / 0.0163 [122/200][50/462] Loss_D: 0.5233 Loss_G: 4.5927 D(x): 0.8824 D(G(z)): 0.0312 / 0.0183 [122/200][100/462] Loss_D: 0.4549 Loss_G: 3.3565 D(x): 0.8811 D(G(z)): 0.0773 / 0.0436 [122/200][150/462] Loss_D: 0.5109 Loss_G: 4.7780 D(x): 0.8722 D(G(z)): 0.1047 / 0.0136 [122/200][200/462] Loss_D: 0.4349 Loss_G: 3.9818 D(x): 0.8978 D(G(z)): 0.0521 / 0.0265 [122/200][250/462] Loss_D: 0.3758 Loss_G: 6.5488 D(x): 0.8460 D(G(z)): 0.0055 / 0.0036 [122/200][300/462] Loss_D: 0.5687 Loss_G: 3.5169 D(x): 0.6941 D(G(z)): 0.0344 / 0.0476 [122/200][350/462] Loss_D: 0.6329 Loss_G: 2.8930 D(x): 0.6945 D(G(z)): 0.0724 / 0.0899 [122/200][400/462] Loss_D: 0.6786 Loss_G: 3.8221 D(x): 0.5836 D(G(z)): 0.0116 / 0.0639 [122/200][450/462] Loss_D: 0.5221 Loss_G: 3.7276 D(x): 0.8427 D(G(z)): 0.0705 / 0.0386 [123/200][0/462] Loss_D: 0.3984 Loss_G: 4.3330 D(x): 0.8853 D(G(z)): 0.0315 / 0.0178 [123/200][50/462] Loss_D: 0.5891 Loss_G: 4.7578 D(x): 0.9174 D(G(z)): 0.1300 / 0.0131 [123/200][100/462] Loss_D: 0.4848 Loss_G: 3.5707 D(x): 0.7869 D(G(z)): 0.0455 / 0.0425 [123/200][150/462] Loss_D: 0.4673 Loss_G: 4.5921 D(x): 0.8464 D(G(z)): 0.0236 / 0.0198 [123/200][200/462] Loss_D: 0.5443 Loss_G: 2.7342 D(x): 0.7188 D(G(z)): 0.0531 / 0.0778 [123/200][250/462] Loss_D: 0.4173 Loss_G: 4.7986 D(x): 0.9390 D(G(z)): 0.0233 / 0.0152 [123/200][300/462] Loss_D: 0.4115 Loss_G: 5.4678 D(x): 0.9328 D(G(z)): 0.0353 / 0.0067 [123/200][350/462] Loss_D: 0.4785 Loss_G: 4.8136 D(x): 0.9067 D(G(z)): 0.0854 / 0.0113 [123/200][400/462] Loss_D: 0.5215 Loss_G: 5.2851 D(x): 0.9294 D(G(z)): 0.0924 / 0.0074 [123/200][450/462] Loss_D: 0.5029 Loss_G: 3.5964 D(x): 0.7484 D(G(z)): 0.0322 / 0.0473 [124/200][0/462] Loss_D: 0.5548 Loss_G: 4.1697 D(x): 0.7233 D(G(z)): 0.0813 / 0.0213 [124/200][50/462] Loss_D: 0.4848 Loss_G: 4.0631 D(x): 0.9277 D(G(z)): 0.0678 / 0.0249 [124/200][100/462] Loss_D: 0.4209 Loss_G: 4.4039 D(x): 0.8104 D(G(z)): 0.0159 / 0.0204 [124/200][150/462] Loss_D: 0.7563 Loss_G: 5.0444 D(x): 0.9897 D(G(z)): 0.1787 / 0.0078 [124/200][200/462] Loss_D: 0.4923 Loss_G: 4.0457 D(x): 0.9252 D(G(z)): 0.0730 / 0.0219 [124/200][250/462] Loss_D: 0.5405 Loss_G: 4.0336 D(x): 0.8009 D(G(z)): 0.0752 / 0.0206 [124/200][300/462] Loss_D: 0.5837 Loss_G: 5.8291 D(x): 0.9382 D(G(z)): 0.1289 / 0.0057 [124/200][350/462] Loss_D: 0.5163 Loss_G: 3.2745 D(x): 0.7570 D(G(z)): 0.0497 / 0.0495 [124/200][400/462] Loss_D: 0.4534 Loss_G: 4.5305 D(x): 0.8994 D(G(z)): 0.0272 / 0.0136 [124/200][450/462] Loss_D: 0.4786 Loss_G: 4.3052 D(x): 0.8050 D(G(z)): 0.0300 / 0.0268 [125/200][0/462] Loss_D: 0.6115 Loss_G: 4.5771 D(x): 0.9310 D(G(z)): 0.1678 / 0.0128 [125/200][50/462] Loss_D: 0.5506 Loss_G: 3.9636 D(x): 0.6941 D(G(z)): 0.0156 / 0.0310 [125/200][100/462] Loss_D: 0.6361 Loss_G: 3.5342 D(x): 0.7004 D(G(z)): 0.1238 / 0.0361 [125/200][150/462] Loss_D: 0.4837 Loss_G: 3.1515 D(x): 0.7676 D(G(z)): 0.0319 / 0.0545 [125/200][200/462] Loss_D: 0.4256 Loss_G: 4.9667 D(x): 0.8262 D(G(z)): 0.0172 / 0.0133 [125/200][250/462] Loss_D: 0.5437 Loss_G: 4.8244 D(x): 0.9415 D(G(z)): 0.1264 / 0.0100 [125/200][300/462] Loss_D: 0.5345 Loss_G: 4.4523 D(x): 0.9286 D(G(z)): 0.1259 / 0.0148 [125/200][350/462] Loss_D: 0.4282 Loss_G: 4.9672 D(x): 0.9185 D(G(z)): 0.0638 / 0.0116 [125/200][400/462] Loss_D: 0.4507 Loss_G: 4.7747 D(x): 0.8913 D(G(z)): 0.0605 / 0.0116 [125/200][450/462] Loss_D: 0.4140 Loss_G: 4.5651 D(x): 0.8042 D(G(z)): 0.0133 / 0.0133 . [126/200][0/462] Loss_D: 0.4417 Loss_G: 4.8374 D(x): 0.8502 D(G(z)): 0.0162 / 0.0142 [126/200][50/462] Loss_D: 0.4557 Loss_G: 5.1148 D(x): 0.8950 D(G(z)): 0.0699 / 0.0093 [126/200][100/462] Loss_D: 0.7003 Loss_G: 3.5553 D(x): 0.6071 D(G(z)): 0.0294 / 0.0741 [126/200][150/462] Loss_D: 0.4220 Loss_G: 4.7987 D(x): 0.9585 D(G(z)): 0.0150 / 0.0126 [126/200][200/462] Loss_D: 0.4504 Loss_G: 4.7838 D(x): 0.7709 D(G(z)): 0.0074 / 0.0115 [126/200][250/462] Loss_D: 0.4674 Loss_G: 4.6326 D(x): 0.9417 D(G(z)): 0.0868 / 0.0119 [126/200][300/462] Loss_D: 0.4938 Loss_G: 3.9310 D(x): 0.8727 D(G(z)): 0.0945 / 0.0252 [126/200][350/462] Loss_D: 0.4525 Loss_G: 4.0060 D(x): 0.7973 D(G(z)): 0.0145 / 0.0246 [126/200][400/462] Loss_D: 0.4711 Loss_G: 4.0377 D(x): 0.9224 D(G(z)): 0.0790 / 0.0221 [126/200][450/462] Loss_D: 0.3977 Loss_G: 6.2283 D(x): 0.9316 D(G(z)): 0.0209 / 0.0047 [127/200][0/462] Loss_D: 0.8941 Loss_G: 2.5003 D(x): 0.4905 D(G(z)): 0.0250 / 0.1099 [127/200][50/462] Loss_D: 0.4196 Loss_G: 4.8853 D(x): 0.8308 D(G(z)): 0.0218 / 0.0118 [127/200][100/462] Loss_D: 0.4102 Loss_G: 4.8150 D(x): 0.8586 D(G(z)): 0.0272 / 0.0155 [127/200][150/462] Loss_D: 0.4447 Loss_G: 4.4655 D(x): 0.8105 D(G(z)): 0.0396 / 0.0182 [127/200][200/462] Loss_D: 0.3833 Loss_G: 5.8422 D(x): 0.8767 D(G(z)): 0.0080 / 0.0058 [127/200][250/462] Loss_D: 0.4791 Loss_G: 3.8345 D(x): 0.8280 D(G(z)): 0.0658 / 0.0333 [127/200][300/462] Loss_D: 0.4191 Loss_G: 4.3795 D(x): 0.8885 D(G(z)): 0.0545 / 0.0165 [127/200][350/462] Loss_D: 0.4438 Loss_G: 4.1718 D(x): 0.8794 D(G(z)): 0.0381 / 0.0220 [127/200][400/462] Loss_D: 0.5879 Loss_G: 3.3169 D(x): 0.6378 D(G(z)): 0.0126 / 0.0534 [127/200][450/462] Loss_D: 0.6670 Loss_G: 4.8483 D(x): 0.5883 D(G(z)): 0.0142 / 0.0222 [128/200][0/462] Loss_D: 0.3802 Loss_G: 7.0403 D(x): 0.8759 D(G(z)): 0.0026 / 0.0017 [128/200][50/462] Loss_D: 0.5266 Loss_G: 5.5005 D(x): 0.9426 D(G(z)): 0.0891 / 0.0051 [128/200][100/462] Loss_D: 0.5011 Loss_G: 4.6277 D(x): 0.9642 D(G(z)): 0.0642 / 0.0125 [128/200][150/462] Loss_D: 0.4145 Loss_G: 5.0999 D(x): 0.9091 D(G(z)): 0.0129 / 0.0092 [128/200][200/462] Loss_D: 0.4942 Loss_G: 4.7617 D(x): 0.8091 D(G(z)): 0.0431 / 0.0120 [128/200][250/462] Loss_D: 0.5822 Loss_G: 4.7894 D(x): 0.8463 D(G(z)): 0.1476 / 0.0115 [128/200][300/462] Loss_D: 0.5329 Loss_G: 2.9939 D(x): 0.7196 D(G(z)): 0.0330 / 0.0637 [128/200][350/462] Loss_D: 0.5004 Loss_G: 3.5006 D(x): 0.7337 D(G(z)): 0.0361 / 0.0438 [128/200][400/462] Loss_D: 0.4684 Loss_G: 5.0777 D(x): 0.9707 D(G(z)): 0.0494 / 0.0124 [128/200][450/462] Loss_D: 0.4855 Loss_G: 4.5335 D(x): 0.9759 D(G(z)): 0.0401 / 0.0129 [129/200][0/462] Loss_D: 0.4839 Loss_G: 4.1064 D(x): 0.8295 D(G(z)): 0.0722 / 0.0233 [129/200][50/462] Loss_D: 0.6959 Loss_G: 6.2334 D(x): 0.9555 D(G(z)): 0.2123 / 0.0022 [129/200][100/462] Loss_D: 0.4593 Loss_G: 4.4918 D(x): 0.9255 D(G(z)): 0.0611 / 0.0163 [129/200][150/462] Loss_D: 0.5939 Loss_G: 2.9812 D(x): 0.6987 D(G(z)): 0.0465 / 0.0848 [129/200][200/462] Loss_D: 0.6107 Loss_G: 4.8304 D(x): 0.9204 D(G(z)): 0.1146 / 0.0133 [129/200][250/462] Loss_D: 0.4757 Loss_G: 4.0070 D(x): 0.8820 D(G(z)): 0.0891 / 0.0229 [129/200][300/462] Loss_D: 0.4868 Loss_G: 3.9621 D(x): 0.8634 D(G(z)): 0.0739 / 0.0310 [129/200][350/462] Loss_D: 0.4435 Loss_G: 4.6988 D(x): 0.8980 D(G(z)): 0.0622 / 0.0153 [129/200][400/462] Loss_D: 0.4367 Loss_G: 4.9474 D(x): 0.8382 D(G(z)): 0.0581 / 0.0164 [129/200][450/462] Loss_D: 0.4506 Loss_G: 4.8447 D(x): 0.7988 D(G(z)): 0.0249 / 0.0182 [130/200][0/462] Loss_D: 0.4166 Loss_G: 5.4872 D(x): 0.9495 D(G(z)): 0.0198 / 0.0069 [130/200][50/462] Loss_D: 0.5598 Loss_G: 3.2941 D(x): 0.8490 D(G(z)): 0.0857 / 0.0508 [130/200][100/462] Loss_D: 0.4277 Loss_G: 5.5086 D(x): 0.8202 D(G(z)): 0.0111 / 0.0081 [130/200][150/462] Loss_D: 0.4220 Loss_G: 4.6761 D(x): 0.8767 D(G(z)): 0.0523 / 0.0164 [130/200][200/462] Loss_D: 0.4523 Loss_G: 4.0725 D(x): 0.9010 D(G(z)): 0.0762 / 0.0234 [130/200][250/462] Loss_D: 0.4743 Loss_G: 4.5324 D(x): 0.8958 D(G(z)): 0.0709 / 0.0206 [130/200][300/462] Loss_D: 0.4991 Loss_G: 4.1747 D(x): 0.7414 D(G(z)): 0.0189 / 0.0247 [130/200][350/462] Loss_D: 0.4341 Loss_G: 4.3228 D(x): 0.8986 D(G(z)): 0.0431 / 0.0183 [130/200][400/462] Loss_D: 0.4683 Loss_G: 4.7140 D(x): 0.8755 D(G(z)): 0.0799 / 0.0118 [130/200][450/462] Loss_D: 0.4390 Loss_G: 4.7184 D(x): 0.9219 D(G(z)): 0.0251 / 0.0143 . [131/200][0/462] Loss_D: 0.4898 Loss_G: 4.7564 D(x): 0.9274 D(G(z)): 0.0509 / 0.0130 [131/200][50/462] Loss_D: 0.4626 Loss_G: 4.3951 D(x): 0.8877 D(G(z)): 0.0579 / 0.0183 [131/200][100/462] Loss_D: 0.4429 Loss_G: 4.0251 D(x): 0.8445 D(G(z)): 0.0528 / 0.0243 [131/200][150/462] Loss_D: 0.4840 Loss_G: 4.1384 D(x): 0.7559 D(G(z)): 0.0136 / 0.0314 [131/200][200/462] Loss_D: 0.4457 Loss_G: 4.0922 D(x): 0.8238 D(G(z)): 0.0417 / 0.0271 [131/200][250/462] Loss_D: 0.4745 Loss_G: 4.8194 D(x): 0.8861 D(G(z)): 0.0573 / 0.0160 [131/200][300/462] Loss_D: 0.4468 Loss_G: 5.5294 D(x): 0.9289 D(G(z)): 0.0323 / 0.0075 [131/200][350/462] Loss_D: 0.5544 Loss_G: 4.5364 D(x): 0.9151 D(G(z)): 0.1355 / 0.0154 [131/200][400/462] Loss_D: 0.3917 Loss_G: 4.9497 D(x): 0.8783 D(G(z)): 0.0225 / 0.0115 [131/200][450/462] Loss_D: 0.4063 Loss_G: 5.0497 D(x): 0.8724 D(G(z)): 0.0212 / 0.0100 [132/200][0/462] Loss_D: 0.3771 Loss_G: 5.3170 D(x): 0.8898 D(G(z)): 0.0134 / 0.0082 [132/200][50/462] Loss_D: 0.4455 Loss_G: 3.0297 D(x): 0.8182 D(G(z)): 0.0433 / 0.0715 [132/200][100/462] Loss_D: 0.4372 Loss_G: 5.2309 D(x): 0.9281 D(G(z)): 0.0351 / 0.0089 [132/200][150/462] Loss_D: 0.4395 Loss_G: 4.7429 D(x): 0.8179 D(G(z)): 0.0212 / 0.0171 [132/200][200/462] Loss_D: 1.0114 Loss_G: 2.9364 D(x): 0.4983 D(G(z)): 0.0303 / 0.0940 [132/200][250/462] Loss_D: 0.4681 Loss_G: 4.8653 D(x): 0.8159 D(G(z)): 0.0253 / 0.0111 [132/200][300/462] Loss_D: 0.4313 Loss_G: 4.8714 D(x): 0.8542 D(G(z)): 0.0441 / 0.0149 [132/200][350/462] Loss_D: 0.5248 Loss_G: 3.2371 D(x): 0.7306 D(G(z)): 0.0457 / 0.0536 [132/200][400/462] Loss_D: 0.5550 Loss_G: 3.9193 D(x): 0.8546 D(G(z)): 0.1278 / 0.0236 [132/200][450/462] Loss_D: 0.3997 Loss_G: 5.2755 D(x): 0.9415 D(G(z)): 0.0249 / 0.0105 [133/200][0/462] Loss_D: 0.3872 Loss_G: 5.2385 D(x): 0.9277 D(G(z)): 0.0231 / 0.0082 [133/200][50/462] Loss_D: 0.3856 Loss_G: 5.2211 D(x): 0.8815 D(G(z)): 0.0152 / 0.0085 [133/200][100/462] Loss_D: 0.4384 Loss_G: 4.6686 D(x): 0.8505 D(G(z)): 0.0239 / 0.0143 [133/200][150/462] Loss_D: 0.3825 Loss_G: 4.7797 D(x): 0.8766 D(G(z)): 0.0211 / 0.0119 [133/200][200/462] Loss_D: 0.5532 Loss_G: 6.2237 D(x): 0.9185 D(G(z)): 0.1170 / 0.0038 [133/200][250/462] Loss_D: 0.4268 Loss_G: 4.8175 D(x): 0.8740 D(G(z)): 0.0309 / 0.0138 [133/200][300/462] Loss_D: 0.4122 Loss_G: 4.8675 D(x): 0.9310 D(G(z)): 0.0464 / 0.0119 [133/200][350/462] Loss_D: 0.5857 Loss_G: 3.3804 D(x): 0.7229 D(G(z)): 0.0550 / 0.0602 [133/200][400/462] Loss_D: 0.5753 Loss_G: 5.6274 D(x): 0.9642 D(G(z)): 0.1209 / 0.0056 [133/200][450/462] Loss_D: 0.4704 Loss_G: 4.3559 D(x): 0.7400 D(G(z)): 0.0189 / 0.0245 [134/200][0/462] Loss_D: 0.4361 Loss_G: 3.8222 D(x): 0.9256 D(G(z)): 0.0537 / 0.0265 [134/200][50/462] Loss_D: 0.5624 Loss_G: 3.3889 D(x): 0.6793 D(G(z)): 0.0197 / 0.0542 [134/200][100/462] Loss_D: 0.4398 Loss_G: 3.9890 D(x): 0.9309 D(G(z)): 0.0338 / 0.0241 [134/200][150/462] Loss_D: 0.5480 Loss_G: 5.0055 D(x): 0.9248 D(G(z)): 0.1193 / 0.0088 [134/200][200/462] Loss_D: 0.5413 Loss_G: 5.0339 D(x): 0.9618 D(G(z)): 0.0834 / 0.0085 [134/200][250/462] Loss_D: 0.4398 Loss_G: 3.7446 D(x): 0.8101 D(G(z)): 0.0370 / 0.0358 [134/200][300/462] Loss_D: 0.4416 Loss_G: 4.4793 D(x): 0.9385 D(G(z)): 0.0598 / 0.0141 [134/200][350/462] Loss_D: 0.5158 Loss_G: 4.1086 D(x): 0.9309 D(G(z)): 0.0696 / 0.0228 [134/200][400/462] Loss_D: 0.4084 Loss_G: 3.4970 D(x): 0.8404 D(G(z)): 0.0296 / 0.0407 [134/200][450/462] Loss_D: 0.4234 Loss_G: 4.9691 D(x): 0.8710 D(G(z)): 0.0130 / 0.0124 [135/200][0/462] Loss_D: 0.4967 Loss_G: 4.5153 D(x): 0.7187 D(G(z)): 0.0107 / 0.0245 [135/200][50/462] Loss_D: 0.4183 Loss_G: 4.5666 D(x): 0.9195 D(G(z)): 0.0573 / 0.0159 [135/200][100/462] Loss_D: 0.4555 Loss_G: 4.2091 D(x): 0.8627 D(G(z)): 0.0326 / 0.0198 [135/200][150/462] Loss_D: 0.4475 Loss_G: 4.0771 D(x): 0.7771 D(G(z)): 0.0267 / 0.0291 [135/200][200/462] Loss_D: 0.6545 Loss_G: 5.0273 D(x): 0.9658 D(G(z)): 0.1590 / 0.0076 [135/200][250/462] Loss_D: 0.4816 Loss_G: 4.3103 D(x): 0.9094 D(G(z)): 0.1034 / 0.0276 [135/200][300/462] Loss_D: 0.6145 Loss_G: 4.4225 D(x): 0.9422 D(G(z)): 0.1548 / 0.0146 [135/200][350/462] Loss_D: 0.3930 Loss_G: 4.8090 D(x): 0.9094 D(G(z)): 0.0165 / 0.0153 [135/200][400/462] Loss_D: 0.4549 Loss_G: 4.6777 D(x): 0.8627 D(G(z)): 0.0236 / 0.0160 [135/200][450/462] Loss_D: 0.4844 Loss_G: 4.0290 D(x): 0.7486 D(G(z)): 0.0271 / 0.0297 . [136/200][0/462] Loss_D: 0.4371 Loss_G: 4.7493 D(x): 0.8445 D(G(z)): 0.0189 / 0.0132 [136/200][50/462] Loss_D: 0.5540 Loss_G: 4.4217 D(x): 0.6689 D(G(z)): 0.0056 / 0.0200 [136/200][100/462] Loss_D: 0.3912 Loss_G: 4.8662 D(x): 0.9266 D(G(z)): 0.0221 / 0.0111 [136/200][150/462] Loss_D: 0.4200 Loss_G: 5.4677 D(x): 0.9066 D(G(z)): 0.0312 / 0.0107 [136/200][200/462] Loss_D: 0.4421 Loss_G: 3.8264 D(x): 0.8686 D(G(z)): 0.0392 / 0.0338 [136/200][250/462] Loss_D: 0.4117 Loss_G: 4.3242 D(x): 0.8850 D(G(z)): 0.0322 / 0.0195 [136/200][300/462] Loss_D: 0.5200 Loss_G: 3.3306 D(x): 0.8562 D(G(z)): 0.0969 / 0.0472 [136/200][350/462] Loss_D: 0.4212 Loss_G: 5.1372 D(x): 0.9102 D(G(z)): 0.0333 / 0.0092 [136/200][400/462] Loss_D: 0.4568 Loss_G: 5.1289 D(x): 0.9134 D(G(z)): 0.0810 / 0.0101 [136/200][450/462] Loss_D: 0.4799 Loss_G: 4.7789 D(x): 0.9307 D(G(z)): 0.0573 / 0.0108 [137/200][0/462] Loss_D: 0.4243 Loss_G: 5.1362 D(x): 0.9183 D(G(z)): 0.0182 / 0.0082 [137/200][50/462] Loss_D: 0.4412 Loss_G: 5.5251 D(x): 0.8322 D(G(z)): 0.0065 / 0.0071 [137/200][100/462] Loss_D: 0.4362 Loss_G: 4.9623 D(x): 0.9337 D(G(z)): 0.0577 / 0.0172 [137/200][150/462] Loss_D: 0.4973 Loss_G: 3.5115 D(x): 0.7917 D(G(z)): 0.0420 / 0.0492 [137/200][200/462] Loss_D: 0.4979 Loss_G: 3.3858 D(x): 0.7727 D(G(z)): 0.0237 / 0.0531 [137/200][250/462] Loss_D: 0.3578 Loss_G: 4.8992 D(x): 0.8572 D(G(z)): 0.0112 / 0.0122 [137/200][300/462] Loss_D: 1.0203 Loss_G: 1.5633 D(x): 0.4092 D(G(z)): 0.0189 / 0.2718 [137/200][350/462] Loss_D: 0.4182 Loss_G: 4.0973 D(x): 0.8662 D(G(z)): 0.0325 / 0.0249 [137/200][400/462] Loss_D: 0.4823 Loss_G: 4.4557 D(x): 0.7488 D(G(z)): 0.0201 / 0.0243 [137/200][450/462] Loss_D: 0.4661 Loss_G: 4.1949 D(x): 0.7205 D(G(z)): 0.0121 / 0.0296 [138/200][0/462] Loss_D: 0.4517 Loss_G: 4.6795 D(x): 0.8974 D(G(z)): 0.0754 / 0.0172 [138/200][50/462] Loss_D: 0.5046 Loss_G: 4.6220 D(x): 0.7643 D(G(z)): 0.0339 / 0.0211 [138/200][100/462] Loss_D: 0.6548 Loss_G: 3.1467 D(x): 0.6655 D(G(z)): 0.0558 / 0.0646 [138/200][150/462] Loss_D: 0.4321 Loss_G: 4.6673 D(x): 0.9074 D(G(z)): 0.0462 / 0.0141 [138/200][200/462] Loss_D: 0.3686 Loss_G: 4.4366 D(x): 0.8983 D(G(z)): 0.0209 / 0.0227 [138/200][250/462] Loss_D: 0.4806 Loss_G: 4.8484 D(x): 0.8443 D(G(z)): 0.0390 / 0.0154 [138/200][300/462] Loss_D: 0.4696 Loss_G: 5.4341 D(x): 0.9203 D(G(z)): 0.0434 / 0.0061 [138/200][350/462] Loss_D: 0.3800 Loss_G: 5.3595 D(x): 0.9460 D(G(z)): 0.0140 / 0.0078 [138/200][400/462] Loss_D: 0.4873 Loss_G: 5.2313 D(x): 0.9226 D(G(z)): 0.0947 / 0.0087 [138/200][450/462] Loss_D: 0.4048 Loss_G: 4.8857 D(x): 0.9288 D(G(z)): 0.0198 / 0.0143 [139/200][0/462] Loss_D: 0.4485 Loss_G: 5.0153 D(x): 0.8573 D(G(z)): 0.0268 / 0.0114 [139/200][50/462] Loss_D: 0.5067 Loss_G: 4.0325 D(x): 0.7469 D(G(z)): 0.0339 / 0.0252 [139/200][100/462] Loss_D: 0.4976 Loss_G: 4.0948 D(x): 0.7558 D(G(z)): 0.0175 / 0.0270 [139/200][150/462] Loss_D: 0.3963 Loss_G: 4.4750 D(x): 0.9018 D(G(z)): 0.0274 / 0.0159 [139/200][200/462] Loss_D: 0.4475 Loss_G: 5.2491 D(x): 0.9219 D(G(z)): 0.0415 / 0.0074 [139/200][250/462] Loss_D: 0.3995 Loss_G: 4.9672 D(x): 0.8462 D(G(z)): 0.0116 / 0.0110 [139/200][300/462] Loss_D: 0.4267 Loss_G: 5.4295 D(x): 0.8596 D(G(z)): 0.0114 / 0.0095 [139/200][350/462] Loss_D: 0.4037 Loss_G: 4.8184 D(x): 0.9026 D(G(z)): 0.0413 / 0.0106 [139/200][400/462] Loss_D: 0.4470 Loss_G: 4.0549 D(x): 0.8471 D(G(z)): 0.0293 / 0.0249 [139/200][450/462] Loss_D: 0.5042 Loss_G: 5.3011 D(x): 0.9609 D(G(z)): 0.0600 / 0.0079 [140/200][0/462] Loss_D: 0.4655 Loss_G: 4.7942 D(x): 0.7748 D(G(z)): 0.0073 / 0.0171 [140/200][50/462] Loss_D: 0.5971 Loss_G: 5.4747 D(x): 0.9836 D(G(z)): 0.1023 / 0.0070 [140/200][100/462] Loss_D: 0.5290 Loss_G: 5.5499 D(x): 0.9720 D(G(z)): 0.0653 / 0.0066 [140/200][150/462] Loss_D: 0.5069 Loss_G: 3.9186 D(x): 0.7225 D(G(z)): 0.0222 / 0.0316 [140/200][200/462] Loss_D: 0.5088 Loss_G: 3.3919 D(x): 0.7319 D(G(z)): 0.0305 / 0.0554 [140/200][250/462] Loss_D: 0.4183 Loss_G: 5.0393 D(x): 0.9296 D(G(z)): 0.0507 / 0.0081 [140/200][300/462] Loss_D: 0.4387 Loss_G: 4.7524 D(x): 0.9230 D(G(z)): 0.0465 / 0.0128 [140/200][350/462] Loss_D: 0.4308 Loss_G: 4.5337 D(x): 0.8683 D(G(z)): 0.0318 / 0.0151 [140/200][400/462] Loss_D: 0.4367 Loss_G: 3.8782 D(x): 0.8608 D(G(z)): 0.0263 / 0.0365 [140/200][450/462] Loss_D: 0.4286 Loss_G: 4.5611 D(x): 0.9277 D(G(z)): 0.0331 / 0.0170 . [141/200][0/462] Loss_D: 0.4363 Loss_G: 4.7818 D(x): 0.8468 D(G(z)): 0.0273 / 0.0156 [141/200][50/462] Loss_D: 0.3907 Loss_G: 4.8020 D(x): 0.9281 D(G(z)): 0.0273 / 0.0128 [141/200][100/462] Loss_D: 0.5358 Loss_G: 4.6839 D(x): 0.9490 D(G(z)): 0.1150 / 0.0112 [141/200][150/462] Loss_D: 0.4704 Loss_G: 5.3137 D(x): 0.9545 D(G(z)): 0.0681 / 0.0066 [141/200][200/462] Loss_D: 0.4869 Loss_G: 3.0891 D(x): 0.8275 D(G(z)): 0.0632 / 0.0591 [141/200][250/462] Loss_D: 0.5423 Loss_G: 3.9428 D(x): 0.7945 D(G(z)): 0.0633 / 0.0267 [141/200][300/462] Loss_D: 0.4100 Loss_G: 5.2206 D(x): 0.8945 D(G(z)): 0.0245 / 0.0088 [141/200][350/462] Loss_D: 0.4011 Loss_G: 5.1008 D(x): 0.8751 D(G(z)): 0.0251 / 0.0111 [141/200][400/462] Loss_D: 0.8044 Loss_G: 3.3070 D(x): 0.5696 D(G(z)): 0.0110 / 0.0704 [141/200][450/462] Loss_D: 0.4099 Loss_G: 5.0999 D(x): 0.8784 D(G(z)): 0.0335 / 0.0097 [142/200][0/462] Loss_D: 0.3969 Loss_G: 5.2313 D(x): 0.9194 D(G(z)): 0.0242 / 0.0086 [142/200][50/462] Loss_D: 0.4155 Loss_G: 4.0490 D(x): 0.8634 D(G(z)): 0.0314 / 0.0245 [142/200][100/462] Loss_D: 0.6314 Loss_G: 2.4667 D(x): 0.6359 D(G(z)): 0.0321 / 0.1298 [142/200][150/462] Loss_D: 0.4121 Loss_G: 5.0027 D(x): 0.9401 D(G(z)): 0.0324 / 0.0115 [142/200][200/462] Loss_D: 0.4950 Loss_G: 4.5008 D(x): 0.8721 D(G(z)): 0.0830 / 0.0214 [142/200][250/462] Loss_D: 0.4184 Loss_G: 4.6719 D(x): 0.8678 D(G(z)): 0.0120 / 0.0139 [142/200][300/462] Loss_D: 0.5329 Loss_G: 4.6489 D(x): 0.9561 D(G(z)): 0.1085 / 0.0116 [142/200][350/462] Loss_D: 0.4651 Loss_G: 3.8856 D(x): 0.8019 D(G(z)): 0.0643 / 0.0318 [142/200][400/462] Loss_D: 0.4368 Loss_G: 4.7741 D(x): 0.9022 D(G(z)): 0.0392 / 0.0129 [142/200][450/462] Loss_D: 0.4554 Loss_G: 4.3718 D(x): 0.7546 D(G(z)): 0.0123 / 0.0187 [143/200][0/462] Loss_D: 0.5017 Loss_G: 4.8085 D(x): 0.8526 D(G(z)): 0.0336 / 0.0130 [143/200][50/462] Loss_D: 0.3920 Loss_G: 4.6866 D(x): 0.8986 D(G(z)): 0.0259 / 0.0138 [143/200][100/462] Loss_D: 0.4641 Loss_G: 4.6495 D(x): 0.7408 D(G(z)): 0.0120 / 0.0131 [143/200][150/462] Loss_D: 0.4387 Loss_G: 3.6868 D(x): 0.8701 D(G(z)): 0.0608 / 0.0359 [143/200][200/462] Loss_D: 0.4938 Loss_G: 3.5243 D(x): 0.7294 D(G(z)): 0.0169 / 0.0500 [143/200][250/462] Loss_D: 0.4565 Loss_G: 4.6620 D(x): 0.9247 D(G(z)): 0.0673 / 0.0134 [143/200][300/462] Loss_D: 0.4631 Loss_G: 3.7747 D(x): 0.8092 D(G(z)): 0.0259 / 0.0363 [143/200][350/462] Loss_D: 0.4364 Loss_G: 5.7071 D(x): 0.8972 D(G(z)): 0.0102 / 0.0053 [143/200][400/462] Loss_D: 0.4273 Loss_G: 4.3489 D(x): 0.8451 D(G(z)): 0.0350 / 0.0205 [143/200][450/462] Loss_D: 0.3784 Loss_G: 5.2095 D(x): 0.9079 D(G(z)): 0.0160 / 0.0084 [144/200][0/462] Loss_D: 0.5022 Loss_G: 5.2703 D(x): 0.9297 D(G(z)): 0.1010 / 0.0076 [144/200][50/462] Loss_D: 0.4273 Loss_G: 4.5547 D(x): 0.8775 D(G(z)): 0.0453 / 0.0141 [144/200][100/462] Loss_D: 0.4431 Loss_G: 5.3512 D(x): 0.8589 D(G(z)): 0.0113 / 0.0085 [144/200][150/462] Loss_D: 0.4531 Loss_G: 4.1704 D(x): 0.7595 D(G(z)): 0.0169 / 0.0325 [144/200][200/462] Loss_D: 0.3974 Loss_G: 5.6602 D(x): 0.9339 D(G(z)): 0.0099 / 0.0068 [144/200][250/462] Loss_D: 0.4108 Loss_G: 4.6247 D(x): 0.8507 D(G(z)): 0.0193 / 0.0150 [144/200][300/462] Loss_D: 0.4130 Loss_G: 4.1357 D(x): 0.8186 D(G(z)): 0.0324 / 0.0229 [144/200][350/462] Loss_D: 0.4292 Loss_G: 4.9116 D(x): 0.9081 D(G(z)): 0.0611 / 0.0134 [144/200][400/462] Loss_D: 0.4726 Loss_G: 5.0032 D(x): 0.9217 D(G(z)): 0.0816 / 0.0093 [144/200][450/462] Loss_D: 0.4534 Loss_G: 3.9008 D(x): 0.8444 D(G(z)): 0.0438 / 0.0354 [145/200][0/462] Loss_D: 0.4415 Loss_G: 4.7777 D(x): 0.8629 D(G(z)): 0.0431 / 0.0150 [145/200][50/462] Loss_D: 0.4402 Loss_G: 4.1934 D(x): 0.8854 D(G(z)): 0.0588 / 0.0210 [145/200][100/462] Loss_D: 0.4524 Loss_G: 4.7248 D(x): 0.8622 D(G(z)): 0.0165 / 0.0125 [145/200][150/462] Loss_D: 0.4577 Loss_G: 4.1398 D(x): 0.8164 D(G(z)): 0.0358 / 0.0227 [145/200][200/462] Loss_D: 0.7371 Loss_G: 4.6708 D(x): 0.6088 D(G(z)): 0.0046 / 0.0177 [145/200][250/462] Loss_D: 0.4820 Loss_G: 3.3479 D(x): 0.7706 D(G(z)): 0.0589 / 0.0651 [145/200][300/462] Loss_D: 0.4461 Loss_G: 3.9966 D(x): 0.9076 D(G(z)): 0.0567 / 0.0272 [145/200][350/462] Loss_D: 0.4403 Loss_G: 4.8119 D(x): 0.8996 D(G(z)): 0.0249 / 0.0142 [145/200][400/462] Loss_D: 0.4437 Loss_G: 4.7791 D(x): 0.9353 D(G(z)): 0.0622 / 0.0122 [145/200][450/462] Loss_D: 0.4749 Loss_G: 4.0668 D(x): 0.8300 D(G(z)): 0.0352 / 0.0238 . [146/200][0/462] Loss_D: 0.6744 Loss_G: 5.1743 D(x): 0.9609 D(G(z)): 0.1433 / 0.0068 [146/200][50/462] Loss_D: 0.4083 Loss_G: 4.3528 D(x): 0.8518 D(G(z)): 0.0131 / 0.0186 [146/200][100/462] Loss_D: 0.4336 Loss_G: 4.8769 D(x): 0.9074 D(G(z)): 0.0588 / 0.0141 [146/200][150/462] Loss_D: 0.4242 Loss_G: 5.1864 D(x): 0.8179 D(G(z)): 0.0051 / 0.0091 [146/200][200/462] Loss_D: 0.4813 Loss_G: 5.5189 D(x): 0.9542 D(G(z)): 0.0638 / 0.0061 [146/200][250/462] Loss_D: 0.4650 Loss_G: 4.9683 D(x): 0.8773 D(G(z)): 0.0447 / 0.0109 [146/200][300/462] Loss_D: 0.4203 Loss_G: 5.4376 D(x): 0.8249 D(G(z)): 0.0204 / 0.0124 [146/200][350/462] Loss_D: 0.5025 Loss_G: 4.1811 D(x): 0.7846 D(G(z)): 0.0414 / 0.0208 [146/200][400/462] Loss_D: 0.4588 Loss_G: 4.6855 D(x): 0.9437 D(G(z)): 0.0785 / 0.0154 [146/200][450/462] Loss_D: 0.4155 Loss_G: 4.7360 D(x): 0.9076 D(G(z)): 0.0374 / 0.0213 [147/200][0/462] Loss_D: 0.4220 Loss_G: 4.2291 D(x): 0.8743 D(G(z)): 0.0234 / 0.0187 [147/200][50/462] Loss_D: 0.4515 Loss_G: 4.2486 D(x): 0.9103 D(G(z)): 0.0872 / 0.0156 [147/200][100/462] Loss_D: 0.4621 Loss_G: 4.0174 D(x): 0.8403 D(G(z)): 0.0732 / 0.0228 [147/200][150/462] Loss_D: 0.4902 Loss_G: 5.1979 D(x): 0.7601 D(G(z)): 0.0093 / 0.0179 [147/200][200/462] Loss_D: 0.4490 Loss_G: 4.9511 D(x): 0.9574 D(G(z)): 0.0292 / 0.0115 [147/200][250/462] Loss_D: 0.4198 Loss_G: 5.1713 D(x): 0.9073 D(G(z)): 0.0563 / 0.0084 [147/200][300/462] Loss_D: 0.4688 Loss_G: 5.3374 D(x): 0.9436 D(G(z)): 0.0523 / 0.0074 [147/200][350/462] Loss_D: 0.4248 Loss_G: 5.2082 D(x): 0.9334 D(G(z)): 0.0247 / 0.0095 [147/200][400/462] Loss_D: 0.4320 Loss_G: 3.5122 D(x): 0.8711 D(G(z)): 0.0549 / 0.0383 [147/200][450/462] Loss_D: 0.4865 Loss_G: 4.1360 D(x): 0.7419 D(G(z)): 0.0216 / 0.0231 [148/200][0/462] Loss_D: 0.6878 Loss_G: 4.7214 D(x): 0.9434 D(G(z)): 0.2016 / 0.0108 [148/200][50/462] Loss_D: 0.4766 Loss_G: 5.1187 D(x): 0.8975 D(G(z)): 0.0967 / 0.0100 [148/200][100/462] Loss_D: 0.3927 Loss_G: 4.6307 D(x): 0.8688 D(G(z)): 0.0175 / 0.0138 [148/200][150/462] Loss_D: 0.5401 Loss_G: 4.3482 D(x): 0.7195 D(G(z)): 0.0147 / 0.0273 [148/200][200/462] Loss_D: 0.3916 Loss_G: 5.1711 D(x): 0.9013 D(G(z)): 0.0196 / 0.0097 [148/200][250/462] Loss_D: 0.5367 Loss_G: 3.8764 D(x): 0.7535 D(G(z)): 0.0679 / 0.0302 [148/200][300/462] Loss_D: 0.4367 Loss_G: 4.1494 D(x): 0.8568 D(G(z)): 0.0669 / 0.0253 [148/200][350/462] Loss_D: 0.5385 Loss_G: 3.0774 D(x): 0.7421 D(G(z)): 0.0480 / 0.0635 [148/200][400/462] Loss_D: 0.4334 Loss_G: 4.1656 D(x): 0.7810 D(G(z)): 0.0317 / 0.0298 [148/200][450/462] Loss_D: 0.3893 Loss_G: 5.3779 D(x): 0.9395 D(G(z)): 0.0230 / 0.0086 [149/200][0/462] Loss_D: 0.6042 Loss_G: 2.7292 D(x): 0.6202 D(G(z)): 0.0193 / 0.0951 [149/200][50/462] Loss_D: 0.3994 Loss_G: 3.8140 D(x): 0.8264 D(G(z)): 0.0361 / 0.0298 [149/200][100/462] Loss_D: 0.4353 Loss_G: 4.9220 D(x): 0.7712 D(G(z)): 0.0102 / 0.0141 [149/200][150/462] Loss_D: 0.4348 Loss_G: 5.3614 D(x): 0.9450 D(G(z)): 0.0141 / 0.0063 [149/200][200/462] Loss_D: 0.4681 Loss_G: 5.1373 D(x): 0.8648 D(G(z)): 0.0345 / 0.0136 [149/200][250/462] Loss_D: 0.5906 Loss_G: 5.3209 D(x): 0.9747 D(G(z)): 0.0831 / 0.0067 [149/200][300/462] Loss_D: 0.4516 Loss_G: 4.0485 D(x): 0.8283 D(G(z)): 0.0209 / 0.0300 [149/200][350/462] Loss_D: 0.4679 Loss_G: 4.2465 D(x): 0.8865 D(G(z)): 0.0376 / 0.0192 [149/200][400/462] Loss_D: 0.4615 Loss_G: 3.4998 D(x): 0.7962 D(G(z)): 0.0330 / 0.0460 [149/200][450/462] Loss_D: 0.5191 Loss_G: 3.5552 D(x): 0.7983 D(G(z)): 0.0347 / 0.0400 [150/200][0/462] Loss_D: 0.4326 Loss_G: 4.2938 D(x): 0.8118 D(G(z)): 0.0170 / 0.0280 [150/200][50/462] Loss_D: 0.4761 Loss_G: 4.8274 D(x): 0.8219 D(G(z)): 0.0317 / 0.0178 [150/200][100/462] Loss_D: 0.4641 Loss_G: 5.0658 D(x): 0.8648 D(G(z)): 0.0328 / 0.0089 [150/200][150/462] Loss_D: 0.4072 Loss_G: 5.3256 D(x): 0.9160 D(G(z)): 0.0191 / 0.0110 [150/200][200/462] Loss_D: 0.4762 Loss_G: 3.2992 D(x): 0.7743 D(G(z)): 0.0543 / 0.0494 [150/200][250/462] Loss_D: 0.7140 Loss_G: 5.4703 D(x): 0.9844 D(G(z)): 0.1967 / 0.0060 [150/200][300/462] Loss_D: 0.4796 Loss_G: 4.5457 D(x): 0.7956 D(G(z)): 0.0143 / 0.0173 [150/200][350/462] Loss_D: 0.4525 Loss_G: 4.7335 D(x): 0.9593 D(G(z)): 0.0482 / 0.0119 [150/200][400/462] Loss_D: 0.5173 Loss_G: 3.7924 D(x): 0.7114 D(G(z)): 0.0256 / 0.0436 [150/200][450/462] Loss_D: 0.4085 Loss_G: 4.6042 D(x): 0.9174 D(G(z)): 0.0352 / 0.0178 . [151/200][0/462] Loss_D: 0.5679 Loss_G: 4.8094 D(x): 0.6494 D(G(z)): 0.0086 / 0.0177 [151/200][50/462] Loss_D: 0.4394 Loss_G: 4.3965 D(x): 0.9566 D(G(z)): 0.0567 / 0.0168 [151/200][100/462] Loss_D: 0.5170 Loss_G: 5.0603 D(x): 0.9603 D(G(z)): 0.0881 / 0.0078 [151/200][150/462] Loss_D: 0.3998 Loss_G: 5.1636 D(x): 0.9127 D(G(z)): 0.0313 / 0.0088 [151/200][200/462] Loss_D: 0.4148 Loss_G: 5.2256 D(x): 0.9164 D(G(z)): 0.0496 / 0.0094 [151/200][250/462] Loss_D: 0.6806 Loss_G: 2.7171 D(x): 0.6029 D(G(z)): 0.0140 / 0.1122 [151/200][300/462] Loss_D: 0.4648 Loss_G: 4.3702 D(x): 0.7865 D(G(z)): 0.0295 / 0.0303 [151/200][350/462] Loss_D: 0.4581 Loss_G: 4.1713 D(x): 0.8161 D(G(z)): 0.0550 / 0.0327 [151/200][400/462] Loss_D: 0.4146 Loss_G: 4.8734 D(x): 0.8541 D(G(z)): 0.0197 / 0.0110 [151/200][450/462] Loss_D: 0.4923 Loss_G: 3.8552 D(x): 0.7628 D(G(z)): 0.0189 / 0.0313 [152/200][0/462] Loss_D: 0.3869 Loss_G: 5.9360 D(x): 0.9034 D(G(z)): 0.0137 / 0.0055 [152/200][50/462] Loss_D: 0.5199 Loss_G: 3.7993 D(x): 0.9487 D(G(z)): 0.1085 / 0.0244 [152/200][100/462] Loss_D: 0.4461 Loss_G: 4.7823 D(x): 0.9265 D(G(z)): 0.0507 / 0.0118 [152/200][150/462] Loss_D: 0.4398 Loss_G: 4.9267 D(x): 0.9622 D(G(z)): 0.0436 / 0.0126 [152/200][200/462] Loss_D: 0.4935 Loss_G: 3.7920 D(x): 0.8376 D(G(z)): 0.0575 / 0.0330 [152/200][250/462] Loss_D: 0.4371 Loss_G: 4.7773 D(x): 0.9272 D(G(z)): 0.0372 / 0.0155 [152/200][300/462] Loss_D: 0.3871 Loss_G: 4.8248 D(x): 0.9065 D(G(z)): 0.0280 / 0.0130 [152/200][350/462] Loss_D: 0.5019 Loss_G: 3.8874 D(x): 0.7536 D(G(z)): 0.0100 / 0.0283 [152/200][400/462] Loss_D: 0.4330 Loss_G: 5.0214 D(x): 0.9322 D(G(z)): 0.0415 / 0.0091 [152/200][450/462] Loss_D: 0.5810 Loss_G: 4.3450 D(x): 0.6577 D(G(z)): 0.0101 / 0.0227 [153/200][0/462] Loss_D: 0.4216 Loss_G: 4.6424 D(x): 0.9455 D(G(z)): 0.0259 / 0.0133 [153/200][50/462] Loss_D: 0.8808 Loss_G: 1.9315 D(x): 0.5019 D(G(z)): 0.0615 / 0.1865 [153/200][100/462] Loss_D: 0.4033 Loss_G: 5.2514 D(x): 0.9248 D(G(z)): 0.0315 / 0.0100 [153/200][150/462] Loss_D: 0.4748 Loss_G: 5.1333 D(x): 0.9446 D(G(z)): 0.0806 / 0.0089 [153/200][200/462] Loss_D: 0.4311 Loss_G: 4.0645 D(x): 0.8541 D(G(z)): 0.0409 / 0.0235 [153/200][250/462] Loss_D: 0.4460 Loss_G: 4.2383 D(x): 0.7945 D(G(z)): 0.0123 / 0.0233 [153/200][300/462] Loss_D: 0.4011 Loss_G: 4.8484 D(x): 0.9279 D(G(z)): 0.0264 / 0.0127 [153/200][350/462] Loss_D: 0.5118 Loss_G: 4.4520 D(x): 0.8881 D(G(z)): 0.1099 / 0.0145 [153/200][400/462] Loss_D: 0.4376 Loss_G: 4.9243 D(x): 0.8829 D(G(z)): 0.0602 / 0.0090 [153/200][450/462] Loss_D: 0.4467 Loss_G: 5.9020 D(x): 0.9128 D(G(z)): 0.0106 / 0.0041 [154/200][0/462] Loss_D: 0.3864 Loss_G: 5.6120 D(x): 0.9352 D(G(z)): 0.0175 / 0.0048 [154/200][50/462] Loss_D: 0.4193 Loss_G: 5.7777 D(x): 0.9257 D(G(z)): 0.0085 / 0.0059 [154/200][100/462] Loss_D: 0.4572 Loss_G: 4.8212 D(x): 0.9452 D(G(z)): 0.0684 / 0.0099 [154/200][150/462] Loss_D: 0.4223 Loss_G: 4.7531 D(x): 0.8322 D(G(z)): 0.0131 / 0.0123 [154/200][200/462] Loss_D: 0.4753 Loss_G: 5.4605 D(x): 0.9525 D(G(z)): 0.0834 / 0.0070 [154/200][250/462] Loss_D: 0.3942 Loss_G: 5.4798 D(x): 0.9129 D(G(z)): 0.0399 / 0.0076 [154/200][300/462] Loss_D: 0.4417 Loss_G: 3.8950 D(x): 0.8734 D(G(z)): 0.0513 / 0.0245 [154/200][350/462] Loss_D: 0.4782 Loss_G: 4.5373 D(x): 0.9049 D(G(z)): 0.0660 / 0.0132 [154/200][400/462] Loss_D: 0.4064 Loss_G: 5.0321 D(x): 0.9050 D(G(z)): 0.0249 / 0.0128 [154/200][450/462] Loss_D: 0.4289 Loss_G: 5.5003 D(x): 0.9236 D(G(z)): 0.0242 / 0.0054 [155/200][0/462] Loss_D: 0.4825 Loss_G: 5.6311 D(x): 0.8915 D(G(z)): 0.0912 / 0.0047 [155/200][50/462] Loss_D: 0.3796 Loss_G: 4.7267 D(x): 0.8951 D(G(z)): 0.0368 / 0.0130 [155/200][100/462] Loss_D: 0.3909 Loss_G: 4.5406 D(x): 0.8580 D(G(z)): 0.0186 / 0.0154 [155/200][150/462] Loss_D: 0.4435 Loss_G: 4.7412 D(x): 0.8743 D(G(z)): 0.0341 / 0.0131 [155/200][200/462] Loss_D: 0.4277 Loss_G: 4.4528 D(x): 0.8573 D(G(z)): 0.0270 / 0.0203 [155/200][250/462] Loss_D: 0.3898 Loss_G: 5.5127 D(x): 0.9419 D(G(z)): 0.0202 / 0.0053 [155/200][300/462] Loss_D: 0.4085 Loss_G: 4.8697 D(x): 0.8342 D(G(z)): 0.0162 / 0.0129 [155/200][350/462] Loss_D: 0.4175 Loss_G: 4.4536 D(x): 0.9188 D(G(z)): 0.0458 / 0.0181 [155/200][400/462] Loss_D: 0.4396 Loss_G: 4.2764 D(x): 0.8440 D(G(z)): 0.0675 / 0.0200 [155/200][450/462] Loss_D: 0.5779 Loss_G: 5.7832 D(x): 0.9523 D(G(z)): 0.1473 / 0.0046 . [156/200][0/462] Loss_D: 0.4719 Loss_G: 4.9321 D(x): 0.9094 D(G(z)): 0.0611 / 0.0101 [156/200][50/462] Loss_D: 0.4889 Loss_G: 4.0263 D(x): 0.8149 D(G(z)): 0.0447 / 0.0234 [156/200][100/462] Loss_D: 0.5304 Loss_G: 3.5050 D(x): 0.6951 D(G(z)): 0.0167 / 0.0497 [156/200][150/462] Loss_D: 0.6168 Loss_G: 3.1997 D(x): 0.6142 D(G(z)): 0.0113 / 0.0544 [156/200][200/462] Loss_D: 0.4181 Loss_G: 5.5539 D(x): 0.8379 D(G(z)): 0.0079 / 0.0073 [156/200][250/462] Loss_D: 0.4211 Loss_G: 4.6443 D(x): 0.9211 D(G(z)): 0.0626 / 0.0163 [156/200][300/462] Loss_D: 0.3963 Loss_G: 4.3686 D(x): 0.8870 D(G(z)): 0.0167 / 0.0152 [156/200][350/462] Loss_D: 1.3741 Loss_G: 2.0658 D(x): 0.3455 D(G(z)): 0.0161 / 0.1777 [156/200][400/462] Loss_D: 0.4784 Loss_G: 3.7021 D(x): 0.7892 D(G(z)): 0.0476 / 0.0422 [156/200][450/462] Loss_D: 0.4394 Loss_G: 4.4441 D(x): 0.9084 D(G(z)): 0.0719 / 0.0159 [157/200][0/462] Loss_D: 0.4278 Loss_G: 4.8167 D(x): 0.8983 D(G(z)): 0.0438 / 0.0109 [157/200][50/462] Loss_D: 0.5849 Loss_G: 4.1146 D(x): 0.7660 D(G(z)): 0.0278 / 0.0229 [157/200][100/462] Loss_D: 0.4586 Loss_G: 5.5202 D(x): 0.9271 D(G(z)): 0.0452 / 0.0060 [157/200][150/462] Loss_D: 0.4084 Loss_G: 5.2779 D(x): 0.9135 D(G(z)): 0.0380 / 0.0085 [157/200][200/462] Loss_D: 0.4536 Loss_G: 4.5330 D(x): 0.8018 D(G(z)): 0.0486 / 0.0167 [157/200][250/462] Loss_D: 0.6038 Loss_G: 4.0591 D(x): 0.6385 D(G(z)): 0.0279 / 0.0397 [157/200][300/462] Loss_D: 0.4760 Loss_G: 4.4068 D(x): 0.8481 D(G(z)): 0.0602 / 0.0157 [157/200][350/462] Loss_D: 0.6542 Loss_G: 4.6650 D(x): 0.9710 D(G(z)): 0.1049 / 0.0144 [157/200][400/462] Loss_D: 0.4684 Loss_G: 4.3639 D(x): 0.7793 D(G(z)): 0.0152 / 0.0230 [157/200][450/462] Loss_D: 0.4270 Loss_G: 5.3053 D(x): 0.8087 D(G(z)): 0.0064 / 0.0092 [158/200][0/462] Loss_D: 0.5019 Loss_G: 5.3046 D(x): 0.9591 D(G(z)): 0.0622 / 0.0063 [158/200][50/462] Loss_D: 0.4501 Loss_G: 5.3301 D(x): 0.8860 D(G(z)): 0.0304 / 0.0107 [158/200][100/462] Loss_D: 0.4880 Loss_G: 4.9803 D(x): 0.9200 D(G(z)): 0.0876 / 0.0114 [158/200][150/462] Loss_D: 0.5323 Loss_G: 5.0970 D(x): 0.9754 D(G(z)): 0.0652 / 0.0088 [158/200][200/462] Loss_D: 0.4413 Loss_G: 4.6031 D(x): 0.8491 D(G(z)): 0.0223 / 0.0160 [158/200][250/462] Loss_D: 0.4509 Loss_G: 4.4870 D(x): 0.9233 D(G(z)): 0.0725 / 0.0139 [158/200][300/462] Loss_D: 0.3979 Loss_G: 4.9688 D(x): 0.8921 D(G(z)): 0.0180 / 0.0102 [158/200][350/462] Loss_D: 0.5240 Loss_G: 4.5575 D(x): 0.8355 D(G(z)): 0.0320 / 0.0155 [158/200][400/462] Loss_D: 0.4371 Loss_G: 5.0445 D(x): 0.9431 D(G(z)): 0.0427 / 0.0094 [158/200][450/462] Loss_D: 0.3826 Loss_G: 5.6327 D(x): 0.9243 D(G(z)): 0.0133 / 0.0064 [159/200][0/462] Loss_D: 0.5918 Loss_G: 4.6223 D(x): 0.9338 D(G(z)): 0.1726 / 0.0159 [159/200][50/462] Loss_D: 0.4450 Loss_G: 4.5156 D(x): 0.8625 D(G(z)): 0.0227 / 0.0215 [159/200][100/462] Loss_D: 0.4273 Loss_G: 3.8311 D(x): 0.8521 D(G(z)): 0.0346 / 0.0275 [159/200][150/462] Loss_D: 0.4581 Loss_G: 3.9544 D(x): 0.8345 D(G(z)): 0.0368 / 0.0319 [159/200][200/462] Loss_D: 0.4846 Loss_G: 3.9039 D(x): 0.9293 D(G(z)): 0.0758 / 0.0247 [159/200][250/462] Loss_D: 0.6297 Loss_G: 4.1813 D(x): 0.9517 D(G(z)): 0.1390 / 0.0179 [159/200][300/462] Loss_D: 0.5064 Loss_G: 3.4311 D(x): 0.7632 D(G(z)): 0.0621 / 0.0506 [159/200][350/462] Loss_D: 0.4314 Loss_G: 4.9004 D(x): 0.9099 D(G(z)): 0.0289 / 0.0101 [159/200][400/462] Loss_D: 0.4123 Loss_G: 4.9693 D(x): 0.9325 D(G(z)): 0.0366 / 0.0098 [159/200][450/462] Loss_D: 0.4402 Loss_G: 4.7863 D(x): 0.7613 D(G(z)): 0.0081 / 0.0131 [160/200][0/462] Loss_D: 0.4810 Loss_G: 4.3580 D(x): 0.8710 D(G(z)): 0.0694 / 0.0193 [160/200][50/462] Loss_D: 0.3834 Loss_G: 5.4263 D(x): 0.8711 D(G(z)): 0.0069 / 0.0072 [160/200][100/462] Loss_D: 0.4056 Loss_G: 4.8738 D(x): 0.8469 D(G(z)): 0.0157 / 0.0151 [160/200][150/462] Loss_D: 0.4330 Loss_G: 4.0697 D(x): 0.8925 D(G(z)): 0.0556 / 0.0228 [160/200][200/462] Loss_D: 0.6777 Loss_G: 3.9101 D(x): 0.6037 D(G(z)): 0.0118 / 0.0391 [160/200][250/462] Loss_D: 0.3952 Loss_G: 4.9218 D(x): 0.8888 D(G(z)): 0.0260 / 0.0160 [160/200][300/462] Loss_D: 0.4085 Loss_G: 5.3417 D(x): 0.8510 D(G(z)): 0.0105 / 0.0063 [160/200][350/462] Loss_D: 0.5234 Loss_G: 4.2822 D(x): 0.7424 D(G(z)): 0.0147 / 0.0234 [160/200][400/462] Loss_D: 0.4974 Loss_G: 3.8226 D(x): 0.9264 D(G(z)): 0.1001 / 0.0299 [160/200][450/462] Loss_D: 0.4138 Loss_G: 5.5554 D(x): 0.9350 D(G(z)): 0.0245 / 0.0064 . [161/200][0/462] Loss_D: 0.5044 Loss_G: 3.6976 D(x): 0.7341 D(G(z)): 0.0171 / 0.0389 [161/200][50/462] Loss_D: 0.4156 Loss_G: 5.2087 D(x): 0.8837 D(G(z)): 0.0230 / 0.0087 [161/200][100/462] Loss_D: 0.4266 Loss_G: 4.0829 D(x): 0.9121 D(G(z)): 0.0284 / 0.0255 [161/200][150/462] Loss_D: 0.4184 Loss_G: 4.8514 D(x): 0.9329 D(G(z)): 0.0392 / 0.0104 [161/200][200/462] Loss_D: 0.4236 Loss_G: 5.7561 D(x): 0.9371 D(G(z)): 0.0195 / 0.0055 [161/200][250/462] Loss_D: 0.4596 Loss_G: 4.0076 D(x): 0.7956 D(G(z)): 0.0556 / 0.0277 [161/200][300/462] Loss_D: 0.4906 Loss_G: 5.3797 D(x): 0.9713 D(G(z)): 0.0443 / 0.0078 [161/200][350/462] Loss_D: 0.4733 Loss_G: 4.3541 D(x): 0.8988 D(G(z)): 0.0538 / 0.0185 [161/200][400/462] Loss_D: 0.4825 Loss_G: 3.3407 D(x): 0.7146 D(G(z)): 0.0160 / 0.0412 [161/200][450/462] Loss_D: 0.4275 Loss_G: 4.9989 D(x): 0.9343 D(G(z)): 0.0417 / 0.0105 [162/200][0/462] Loss_D: 0.6608 Loss_G: 3.0831 D(x): 0.6475 D(G(z)): 0.0173 / 0.0571 [162/200][50/462] Loss_D: 0.5258 Loss_G: 5.3441 D(x): 0.9440 D(G(z)): 0.0896 / 0.0070 [162/200][100/462] Loss_D: 0.4980 Loss_G: 4.1235 D(x): 0.7457 D(G(z)): 0.0321 / 0.0246 [162/200][150/462] Loss_D: 0.4066 Loss_G: 4.1167 D(x): 0.8444 D(G(z)): 0.0157 / 0.0237 [162/200][200/462] Loss_D: 0.5101 Loss_G: 5.1780 D(x): 0.9666 D(G(z)): 0.0828 / 0.0072 [162/200][250/462] Loss_D: 0.4151 Loss_G: 3.7770 D(x): 0.8745 D(G(z)): 0.0346 / 0.0393 [162/200][300/462] Loss_D: 0.5283 Loss_G: 4.3064 D(x): 0.8975 D(G(z)): 0.1386 / 0.0181 [162/200][350/462] Loss_D: 0.4666 Loss_G: 5.3815 D(x): 0.9594 D(G(z)): 0.0553 / 0.0088 [162/200][400/462] Loss_D: 0.5109 Loss_G: 4.7058 D(x): 0.9355 D(G(z)): 0.0932 / 0.0105 [162/200][450/462] Loss_D: 0.3918 Loss_G: 4.7371 D(x): 0.8857 D(G(z)): 0.0361 / 0.0178 [163/200][0/462] Loss_D: 0.4651 Loss_G: 5.2617 D(x): 0.9308 D(G(z)): 0.0542 / 0.0069 [163/200][50/462] Loss_D: 0.4131 Loss_G: 4.0112 D(x): 0.9134 D(G(z)): 0.0323 / 0.0338 [163/200][100/462] Loss_D: 0.3958 Loss_G: 4.8321 D(x): 0.8247 D(G(z)): 0.0302 / 0.0159 [163/200][150/462] Loss_D: 0.4048 Loss_G: 5.4565 D(x): 0.9215 D(G(z)): 0.0248 / 0.0078 [163/200][200/462] Loss_D: 0.5750 Loss_G: 5.2853 D(x): 0.9547 D(G(z)): 0.1345 / 0.0073 [163/200][250/462] Loss_D: 0.5328 Loss_G: 3.6423 D(x): 0.7124 D(G(z)): 0.0256 / 0.0518 [163/200][300/462] Loss_D: 0.5776 Loss_G: 3.7936 D(x): 0.6450 D(G(z)): 0.0141 / 0.0481 [163/200][350/462] Loss_D: 0.4446 Loss_G: 4.9022 D(x): 0.9350 D(G(z)): 0.0666 / 0.0113 [163/200][400/462] Loss_D: 0.4526 Loss_G: 4.9462 D(x): 0.8942 D(G(z)): 0.0542 / 0.0099 [163/200][450/462] Loss_D: 0.4507 Loss_G: 4.4844 D(x): 0.9499 D(G(z)): 0.0411 / 0.0147 [164/200][0/462] Loss_D: 0.3797 Loss_G: 4.7420 D(x): 0.8867 D(G(z)): 0.0221 / 0.0161 [164/200][50/462] Loss_D: 0.4052 Loss_G: 4.3636 D(x): 0.8582 D(G(z)): 0.0304 / 0.0223 [164/200][100/462] Loss_D: 0.5962 Loss_G: 5.9207 D(x): 0.9569 D(G(z)): 0.1611 / 0.0033 [164/200][150/462] Loss_D: 0.4235 Loss_G: 5.3730 D(x): 0.9286 D(G(z)): 0.0588 / 0.0080 [164/200][200/462] Loss_D: 0.5144 Loss_G: 3.0247 D(x): 0.7692 D(G(z)): 0.0644 / 0.0697 [164/200][250/462] Loss_D: 0.4270 Loss_G: 5.1248 D(x): 0.7731 D(G(z)): 0.0078 / 0.0115 [164/200][300/462] Loss_D: 0.4122 Loss_G: 5.2262 D(x): 0.8801 D(G(z)): 0.0261 / 0.0107 [164/200][350/462] Loss_D: 0.3803 Loss_G: 4.7329 D(x): 0.8336 D(G(z)): 0.0198 / 0.0167 [164/200][400/462] Loss_D: 0.4855 Loss_G: 3.9922 D(x): 0.8353 D(G(z)): 0.0819 / 0.0279 [164/200][450/462] Loss_D: 0.4822 Loss_G: 3.5834 D(x): 0.7327 D(G(z)): 0.0294 / 0.0386 [165/200][0/462] Loss_D: 0.4118 Loss_G: 5.2543 D(x): 0.9458 D(G(z)): 0.0290 / 0.0108 [165/200][50/462] Loss_D: 0.4450 Loss_G: 3.8973 D(x): 0.8056 D(G(z)): 0.0282 / 0.0278 [165/200][100/462] Loss_D: 0.4567 Loss_G: 4.1753 D(x): 0.8934 D(G(z)): 0.0941 / 0.0176 [165/200][150/462] Loss_D: 0.5308 Loss_G: 3.5852 D(x): 0.8792 D(G(z)): 0.0566 / 0.0359 [165/200][200/462] Loss_D: 0.4215 Loss_G: 4.8562 D(x): 0.7913 D(G(z)): 0.0081 / 0.0119 [165/200][250/462] Loss_D: 0.4711 Loss_G: 4.7582 D(x): 0.9731 D(G(z)): 0.0332 / 0.0124 [165/200][300/462] Loss_D: 0.4149 Loss_G: 5.3968 D(x): 0.8965 D(G(z)): 0.0311 / 0.0075 [165/200][350/462] Loss_D: 0.3978 Loss_G: 4.6536 D(x): 0.8690 D(G(z)): 0.0172 / 0.0183 [165/200][400/462] Loss_D: 0.3925 Loss_G: 4.1413 D(x): 0.8951 D(G(z)): 0.0210 / 0.0273 [165/200][450/462] Loss_D: 0.3999 Loss_G: 4.5667 D(x): 0.8582 D(G(z)): 0.0300 / 0.0166 . [166/200][0/462] Loss_D: 0.4191 Loss_G: 5.0034 D(x): 0.8683 D(G(z)): 0.0269 / 0.0108 [166/200][50/462] Loss_D: 0.4172 Loss_G: 4.0731 D(x): 0.7975 D(G(z)): 0.0222 / 0.0273 [166/200][100/462] Loss_D: 0.3821 Loss_G: 4.4962 D(x): 0.9181 D(G(z)): 0.0218 / 0.0159 [166/200][150/462] Loss_D: 0.5853 Loss_G: 3.8156 D(x): 0.6806 D(G(z)): 0.0115 / 0.0357 [166/200][200/462] Loss_D: 0.4037 Loss_G: 4.4545 D(x): 0.8336 D(G(z)): 0.0127 / 0.0158 [166/200][250/462] Loss_D: 0.4225 Loss_G: 6.4805 D(x): 0.9470 D(G(z)): 0.0081 / 0.0030 [166/200][300/462] Loss_D: 0.4782 Loss_G: 4.5405 D(x): 0.8479 D(G(z)): 0.0339 / 0.0168 [166/200][350/462] Loss_D: 0.4037 Loss_G: 4.8432 D(x): 0.8058 D(G(z)): 0.0195 / 0.0183 [166/200][400/462] Loss_D: 0.5168 Loss_G: 3.9423 D(x): 0.7139 D(G(z)): 0.0297 / 0.0354 [166/200][450/462] Loss_D: 0.4343 Loss_G: 4.5313 D(x): 0.8933 D(G(z)): 0.0643 / 0.0156 [167/200][0/462] Loss_D: 0.3747 Loss_G: 5.5465 D(x): 0.9091 D(G(z)): 0.0097 / 0.0064 [167/200][50/462] Loss_D: 0.3959 Loss_G: 5.2147 D(x): 0.8229 D(G(z)): 0.0116 / 0.0106 [167/200][100/462] Loss_D: 0.4584 Loss_G: 3.8896 D(x): 0.8158 D(G(z)): 0.0253 / 0.0344 [167/200][150/462] Loss_D: 0.4858 Loss_G: 4.6193 D(x): 0.8467 D(G(z)): 0.0394 / 0.0164 [167/200][200/462] Loss_D: 0.5427 Loss_G: 4.0281 D(x): 0.7777 D(G(z)): 0.0532 / 0.0279 [167/200][250/462] Loss_D: 0.4678 Loss_G: 4.7094 D(x): 0.7977 D(G(z)): 0.0312 / 0.0109 [167/200][300/462] Loss_D: 0.6586 Loss_G: 6.3963 D(x): 0.9782 D(G(z)): 0.1732 / 0.0022 [167/200][350/462] Loss_D: 0.4164 Loss_G: 5.0492 D(x): 0.8339 D(G(z)): 0.0270 / 0.0143 [167/200][400/462] Loss_D: 0.4493 Loss_G: 4.8739 D(x): 0.8530 D(G(z)): 0.0369 / 0.0145 [167/200][450/462] Loss_D: 0.5969 Loss_G: 4.2297 D(x): 0.7936 D(G(z)): 0.1407 / 0.0205 [168/200][0/462] Loss_D: 0.4025 Loss_G: 5.8532 D(x): 0.9527 D(G(z)): 0.0172 / 0.0053 [168/200][50/462] Loss_D: 0.4089 Loss_G: 5.3639 D(x): 0.9323 D(G(z)): 0.0353 / 0.0080 [168/200][100/462] Loss_D: 0.4987 Loss_G: 3.8503 D(x): 0.7468 D(G(z)): 0.0214 / 0.0301 [168/200][150/462] Loss_D: 0.5615 Loss_G: 4.5435 D(x): 0.6976 D(G(z)): 0.0093 / 0.0155 [168/200][200/462] Loss_D: 0.4358 Loss_G: 4.6871 D(x): 0.8920 D(G(z)): 0.0249 / 0.0116 [168/200][250/462] Loss_D: 0.5245 Loss_G: 5.4584 D(x): 0.9694 D(G(z)): 0.0918 / 0.0051 [168/200][300/462] Loss_D: 0.4561 Loss_G: 3.9225 D(x): 0.8921 D(G(z)): 0.0404 / 0.0251 [168/200][350/462] Loss_D: 0.4600 Loss_G: 4.9763 D(x): 0.9508 D(G(z)): 0.0453 / 0.0125 [168/200][400/462] Loss_D: 0.4021 Loss_G: 4.8540 D(x): 0.8773 D(G(z)): 0.0233 / 0.0136 [168/200][450/462] Loss_D: 0.4615 Loss_G: 4.1290 D(x): 0.8575 D(G(z)): 0.0297 / 0.0238 [169/200][0/462] Loss_D: 0.5387 Loss_G: 4.8010 D(x): 0.8417 D(G(z)): 0.1027 / 0.0129 [169/200][50/462] Loss_D: 0.4077 Loss_G: 4.5809 D(x): 0.9033 D(G(z)): 0.0406 / 0.0178 [169/200][100/462] Loss_D: 0.4146 Loss_G: 5.1691 D(x): 0.8569 D(G(z)): 0.0112 / 0.0109 [169/200][150/462] Loss_D: 0.4393 Loss_G: 5.0191 D(x): 0.9561 D(G(z)): 0.0336 / 0.0098 [169/200][200/462] Loss_D: 0.4406 Loss_G: 4.9456 D(x): 0.8644 D(G(z)): 0.0191 / 0.0109 [169/200][250/462] Loss_D: 0.4451 Loss_G: 3.3504 D(x): 0.8861 D(G(z)): 0.0718 / 0.0392 [169/200][300/462] Loss_D: 0.3795 Loss_G: 4.9623 D(x): 0.8925 D(G(z)): 0.0267 / 0.0144 [169/200][350/462] Loss_D: 0.4229 Loss_G: 5.2243 D(x): 0.9115 D(G(z)): 0.0245 / 0.0084 [169/200][400/462] Loss_D: 0.4082 Loss_G: 5.3069 D(x): 0.8930 D(G(z)): 0.0457 / 0.0089 [169/200][450/462] Loss_D: 0.3849 Loss_G: 5.4604 D(x): 0.8862 D(G(z)): 0.0130 / 0.0072 [170/200][0/462] Loss_D: 0.5382 Loss_G: 4.0687 D(x): 0.7574 D(G(z)): 0.0555 / 0.0249 [170/200][50/462] Loss_D: 0.4574 Loss_G: 4.0537 D(x): 0.7752 D(G(z)): 0.0298 / 0.0242 [170/200][100/462] Loss_D: 0.4979 Loss_G: 5.7035 D(x): 0.9689 D(G(z)): 0.0481 / 0.0066 [170/200][150/462] Loss_D: 0.4391 Loss_G: 4.6299 D(x): 0.7768 D(G(z)): 0.0143 / 0.0183 [170/200][200/462] Loss_D: 0.4684 Loss_G: 4.9535 D(x): 0.8955 D(G(z)): 0.0575 / 0.0124 [170/200][250/462] Loss_D: 0.4720 Loss_G: 4.9462 D(x): 0.7780 D(G(z)): 0.0137 / 0.0142 [170/200][300/462] Loss_D: 0.5403 Loss_G: 3.6932 D(x): 0.6611 D(G(z)): 0.0096 / 0.0371 [170/200][350/462] Loss_D: 0.3631 Loss_G: 6.0507 D(x): 0.9116 D(G(z)): 0.0124 / 0.0058 [170/200][400/462] Loss_D: 0.4059 Loss_G: 4.7609 D(x): 0.8229 D(G(z)): 0.0133 / 0.0150 [170/200][450/462] Loss_D: 0.4473 Loss_G: 5.2088 D(x): 0.9342 D(G(z)): 0.0618 / 0.0096 . [171/200][0/462] Loss_D: 0.4016 Loss_G: 5.0244 D(x): 0.8636 D(G(z)): 0.0386 / 0.0149 [171/200][50/462] Loss_D: 0.3879 Loss_G: 5.3223 D(x): 0.9430 D(G(z)): 0.0227 / 0.0077 [171/200][100/462] Loss_D: 0.4511 Loss_G: 4.3122 D(x): 0.8321 D(G(z)): 0.0292 / 0.0197 [171/200][150/462] Loss_D: 0.4491 Loss_G: 5.2834 D(x): 0.9449 D(G(z)): 0.0559 / 0.0079 [171/200][200/462] Loss_D: 0.4574 Loss_G: 4.0361 D(x): 0.8340 D(G(z)): 0.0256 / 0.0288 [171/200][250/462] Loss_D: 0.4870 Loss_G: 4.7224 D(x): 0.9537 D(G(z)): 0.0652 / 0.0137 [171/200][300/462] Loss_D: 0.4222 Loss_G: 5.2490 D(x): 0.8186 D(G(z)): 0.0221 / 0.0124 [171/200][350/462] Loss_D: 0.4730 Loss_G: 4.1295 D(x): 0.8193 D(G(z)): 0.0437 / 0.0252 [171/200][400/462] Loss_D: 0.4942 Loss_G: 4.9761 D(x): 0.9732 D(G(z)): 0.0722 / 0.0086 [171/200][450/462] Loss_D: 0.4594 Loss_G: 4.4417 D(x): 0.8714 D(G(z)): 0.0808 / 0.0143 [172/200][0/462] Loss_D: 0.4257 Loss_G: 4.1641 D(x): 0.9046 D(G(z)): 0.0342 / 0.0220 [172/200][50/462] Loss_D: 0.4518 Loss_G: 4.8637 D(x): 0.8347 D(G(z)): 0.0363 / 0.0110 [172/200][100/462] Loss_D: 0.4977 Loss_G: 4.6603 D(x): 0.8614 D(G(z)): 0.0548 / 0.0205 [172/200][150/462] Loss_D: 0.4394 Loss_G: 4.2402 D(x): 0.8344 D(G(z)): 0.0313 / 0.0209 [172/200][200/462] Loss_D: 0.5113 Loss_G: 5.7851 D(x): 0.9471 D(G(z)): 0.0758 / 0.0072 [172/200][250/462] Loss_D: 0.3914 Loss_G: 4.7111 D(x): 0.8365 D(G(z)): 0.0238 / 0.0136 [172/200][300/462] Loss_D: 0.4930 Loss_G: 5.5949 D(x): 0.9750 D(G(z)): 0.0329 / 0.0058 [172/200][350/462] Loss_D: 0.3997 Loss_G: 4.7054 D(x): 0.9278 D(G(z)): 0.0208 / 0.0120 [172/200][400/462] Loss_D: 0.5527 Loss_G: 4.2389 D(x): 0.9495 D(G(z)): 0.1408 / 0.0181 [172/200][450/462] Loss_D: 0.3904 Loss_G: 4.0507 D(x): 0.8837 D(G(z)): 0.0380 / 0.0258 [173/200][0/462] Loss_D: 0.4402 Loss_G: 4.6951 D(x): 0.9253 D(G(z)): 0.0288 / 0.0149 [173/200][50/462] Loss_D: 0.4376 Loss_G: 5.0295 D(x): 0.9700 D(G(z)): 0.0330 / 0.0101 [173/200][100/462] Loss_D: 0.4114 Loss_G: 4.4315 D(x): 0.8583 D(G(z)): 0.0156 / 0.0197 [173/200][150/462] Loss_D: 0.5308 Loss_G: 5.0530 D(x): 0.9772 D(G(z)): 0.0536 / 0.0112 [173/200][200/462] Loss_D: 0.4207 Loss_G: 4.7106 D(x): 0.8352 D(G(z)): 0.0181 / 0.0176 [173/200][250/462] Loss_D: 0.3807 Loss_G: 4.9027 D(x): 0.9132 D(G(z)): 0.0217 / 0.0121 [173/200][300/462] Loss_D: 0.4326 Loss_G: 5.0811 D(x): 0.9335 D(G(z)): 0.0157 / 0.0106 [173/200][350/462] Loss_D: 0.4922 Loss_G: 4.5119 D(x): 0.8995 D(G(z)): 0.0876 / 0.0140 [173/200][400/462] Loss_D: 0.5382 Loss_G: 3.7560 D(x): 0.7267 D(G(z)): 0.0159 / 0.0446 [173/200][450/462] Loss_D: 0.4089 Loss_G: 4.4718 D(x): 0.8396 D(G(z)): 0.0153 / 0.0202 [174/200][0/462] Loss_D: 0.4215 Loss_G: 4.7110 D(x): 0.9447 D(G(z)): 0.0344 / 0.0105 [174/200][50/462] Loss_D: 0.4309 Loss_G: 4.2911 D(x): 0.9361 D(G(z)): 0.0480 / 0.0187 [174/200][100/462] Loss_D: 0.4181 Loss_G: 5.1331 D(x): 0.8177 D(G(z)): 0.0168 / 0.0108 [174/200][150/462] Loss_D: 0.4058 Loss_G: 4.1944 D(x): 0.8659 D(G(z)): 0.0341 / 0.0241 [174/200][200/462] Loss_D: 0.4080 Loss_G: 4.1007 D(x): 0.9121 D(G(z)): 0.0502 / 0.0239 [174/200][250/462] Loss_D: 0.3778 Loss_G: 4.5947 D(x): 0.8665 D(G(z)): 0.0246 / 0.0168 [174/200][300/462] Loss_D: 0.5709 Loss_G: 5.2558 D(x): 0.9605 D(G(z)): 0.1432 / 0.0066 [174/200][350/462] Loss_D: 0.4780 Loss_G: 3.9965 D(x): 0.7729 D(G(z)): 0.0190 / 0.0298 [174/200][400/462] Loss_D: 0.4290 Loss_G: 4.6459 D(x): 0.8197 D(G(z)): 0.0167 / 0.0162 [174/200][450/462] Loss_D: 0.4450 Loss_G: 4.7144 D(x): 0.8247 D(G(z)): 0.0176 / 0.0149 [175/200][0/462] Loss_D: 0.4555 Loss_G: 4.3472 D(x): 0.8607 D(G(z)): 0.0312 / 0.0233 [175/200][50/462] Loss_D: 0.5941 Loss_G: 3.9494 D(x): 0.6252 D(G(z)): 0.0107 / 0.0338 [175/200][100/462] Loss_D: 0.4908 Loss_G: 4.2106 D(x): 0.8047 D(G(z)): 0.0505 / 0.0246 [175/200][150/462] Loss_D: 0.3770 Loss_G: 4.6432 D(x): 0.9280 D(G(z)): 0.0232 / 0.0112 [175/200][200/462] Loss_D: 0.4344 Loss_G: 4.6071 D(x): 0.9233 D(G(z)): 0.0399 / 0.0167 [175/200][250/462] Loss_D: 0.4063 Loss_G: 4.3525 D(x): 0.8654 D(G(z)): 0.0255 / 0.0187 [175/200][300/462] Loss_D: 0.6171 Loss_G: 4.1961 D(x): 0.9773 D(G(z)): 0.1345 / 0.0170 [175/200][350/462] Loss_D: 0.3859 Loss_G: 4.8608 D(x): 0.8745 D(G(z)): 0.0299 / 0.0120 [175/200][400/462] Loss_D: 0.4685 Loss_G: 5.6278 D(x): 0.9306 D(G(z)): 0.0705 / 0.0044 [175/200][450/462] Loss_D: 0.3942 Loss_G: 5.0575 D(x): 0.8462 D(G(z)): 0.0114 / 0.0087 . [176/200][0/462] Loss_D: 0.4113 Loss_G: 4.9922 D(x): 0.8097 D(G(z)): 0.0180 / 0.0145 [176/200][50/462] Loss_D: 0.4203 Loss_G: 4.5682 D(x): 0.9292 D(G(z)): 0.0586 / 0.0133 [176/200][100/462] Loss_D: 0.3997 Loss_G: 4.5792 D(x): 0.8174 D(G(z)): 0.0109 / 0.0162 [176/200][150/462] Loss_D: 0.3821 Loss_G: 5.8332 D(x): 0.9176 D(G(z)): 0.0312 / 0.0077 [176/200][200/462] Loss_D: 0.4085 Loss_G: 4.5564 D(x): 0.8690 D(G(z)): 0.0359 / 0.0162 [176/200][250/462] Loss_D: 0.3932 Loss_G: 4.1558 D(x): 0.8709 D(G(z)): 0.0351 / 0.0237 [176/200][300/462] Loss_D: 0.5168 Loss_G: 4.3533 D(x): 0.6725 D(G(z)): 0.0074 / 0.0186 [176/200][350/462] Loss_D: 0.4334 Loss_G: 5.0603 D(x): 0.7898 D(G(z)): 0.0174 / 0.0114 [176/200][400/462] Loss_D: 0.4617 Loss_G: 4.3482 D(x): 0.8442 D(G(z)): 0.0583 / 0.0189 [176/200][450/462] Loss_D: 0.5335 Loss_G: 4.3389 D(x): 0.8593 D(G(z)): 0.0757 / 0.0157 [177/200][0/462] Loss_D: 0.5034 Loss_G: 4.4623 D(x): 0.7344 D(G(z)): 0.0183 / 0.0221 [177/200][50/462] Loss_D: 0.4705 Loss_G: 4.2292 D(x): 0.7580 D(G(z)): 0.0168 / 0.0223 [177/200][100/462] Loss_D: 0.3873 Loss_G: 5.4390 D(x): 0.8955 D(G(z)): 0.0163 / 0.0073 [177/200][150/462] Loss_D: 0.4744 Loss_G: 4.6562 D(x): 0.9191 D(G(z)): 0.1015 / 0.0124 [177/200][200/462] Loss_D: 0.5153 Loss_G: 4.1066 D(x): 0.7500 D(G(z)): 0.0162 / 0.0301 [177/200][250/462] Loss_D: 0.4240 Loss_G: 4.3303 D(x): 0.7996 D(G(z)): 0.0113 / 0.0285 [177/200][300/462] Loss_D: 0.4298 Loss_G: 4.5815 D(x): 0.8463 D(G(z)): 0.0161 / 0.0162 [177/200][350/462] Loss_D: 0.4074 Loss_G: 4.6032 D(x): 0.8811 D(G(z)): 0.0289 / 0.0164 [177/200][400/462] Loss_D: 0.4751 Loss_G: 3.6977 D(x): 0.8210 D(G(z)): 0.0427 / 0.0295 [177/200][450/462] Loss_D: 0.4304 Loss_G: 5.0221 D(x): 0.9543 D(G(z)): 0.0243 / 0.0138 [178/200][0/462] Loss_D: 0.4199 Loss_G: 4.4606 D(x): 0.8978 D(G(z)): 0.0191 / 0.0141 [178/200][50/462] Loss_D: 0.4257 Loss_G: 5.3902 D(x): 0.9639 D(G(z)): 0.0112 / 0.0058 [178/200][100/462] Loss_D: 0.3832 Loss_G: 4.6522 D(x): 0.8823 D(G(z)): 0.0203 / 0.0149 [178/200][150/462] Loss_D: 0.3911 Loss_G: 4.0592 D(x): 0.9025 D(G(z)): 0.0409 / 0.0261 [178/200][200/462] Loss_D: 0.4602 Loss_G: 4.5002 D(x): 0.7848 D(G(z)): 0.0272 / 0.0225 [178/200][250/462] Loss_D: 0.4694 Loss_G: 4.8091 D(x): 0.7510 D(G(z)): 0.0129 / 0.0205 [178/200][300/462] Loss_D: 0.5285 Loss_G: 4.3020 D(x): 0.7013 D(G(z)): 0.0100 / 0.0224 [178/200][350/462] Loss_D: 0.5424 Loss_G: 3.9819 D(x): 0.6820 D(G(z)): 0.0149 / 0.0309 [178/200][400/462] Loss_D: 0.4761 Loss_G: 4.1369 D(x): 0.8782 D(G(z)): 0.0857 / 0.0257 [178/200][450/462] Loss_D: 0.4580 Loss_G: 4.0392 D(x): 0.7869 D(G(z)): 0.0162 / 0.0276 [179/200][0/462] Loss_D: 0.4325 Loss_G: 3.8805 D(x): 0.7970 D(G(z)): 0.0285 / 0.0275 [179/200][50/462] Loss_D: 0.4838 Loss_G: 4.0002 D(x): 0.8029 D(G(z)): 0.0370 / 0.0240 [179/200][100/462] Loss_D: 0.3776 Loss_G: 5.4156 D(x): 0.8710 D(G(z)): 0.0092 / 0.0069 [179/200][150/462] Loss_D: 0.4098 Loss_G: 4.2811 D(x): 0.8039 D(G(z)): 0.0194 / 0.0273 [179/200][200/462] Loss_D: 0.3774 Loss_G: 6.3536 D(x): 0.9329 D(G(z)): 0.0051 / 0.0025 [179/200][250/462] Loss_D: 0.4094 Loss_G: 5.7871 D(x): 0.8662 D(G(z)): 0.0047 / 0.0045 [179/200][300/462] Loss_D: 0.4266 Loss_G: 6.3020 D(x): 0.8942 D(G(z)): 0.0060 / 0.0034 [179/200][350/462] Loss_D: 0.4867 Loss_G: 4.6508 D(x): 0.9759 D(G(z)): 0.0544 / 0.0132 [179/200][400/462] Loss_D: 0.4040 Loss_G: 4.9098 D(x): 0.9201 D(G(z)): 0.0172 / 0.0099 [179/200][450/462] Loss_D: 0.5287 Loss_G: 4.3294 D(x): 0.7554 D(G(z)): 0.0197 / 0.0230 [180/200][0/462] Loss_D: 0.4000 Loss_G: 5.0578 D(x): 0.9308 D(G(z)): 0.0215 / 0.0121 [180/200][50/462] Loss_D: 0.4095 Loss_G: 5.5967 D(x): 0.9282 D(G(z)): 0.0122 / 0.0069 [180/200][100/462] Loss_D: 0.3960 Loss_G: 4.9167 D(x): 0.8493 D(G(z)): 0.0271 / 0.0136 [180/200][150/462] Loss_D: 0.4188 Loss_G: 4.4175 D(x): 0.8618 D(G(z)): 0.0310 / 0.0230 [180/200][200/462] Loss_D: 0.4044 Loss_G: 6.1413 D(x): 0.8857 D(G(z)): 0.0061 / 0.0030 [180/200][250/462] Loss_D: 0.3758 Loss_G: 5.5002 D(x): 0.9291 D(G(z)): 0.0208 / 0.0062 [180/200][300/462] Loss_D: 0.4020 Loss_G: 4.8827 D(x): 0.9396 D(G(z)): 0.0295 / 0.0114 [180/200][350/462] Loss_D: 0.5145 Loss_G: 4.0976 D(x): 0.9751 D(G(z)): 0.0510 / 0.0221 [180/200][400/462] Loss_D: 0.4251 Loss_G: 4.1071 D(x): 0.8480 D(G(z)): 0.0368 / 0.0264 [180/200][450/462] Loss_D: 0.3937 Loss_G: 5.4053 D(x): 0.8734 D(G(z)): 0.0184 / 0.0081 . [181/200][0/462] Loss_D: 0.3956 Loss_G: 4.9739 D(x): 0.9195 D(G(z)): 0.0208 / 0.0120 [181/200][50/462] Loss_D: 0.3809 Loss_G: 5.2654 D(x): 0.9290 D(G(z)): 0.0235 / 0.0087 [181/200][100/462] Loss_D: 0.4327 Loss_G: 5.1296 D(x): 0.9570 D(G(z)): 0.0362 / 0.0084 [181/200][150/462] Loss_D: 0.4433 Loss_G: 4.1999 D(x): 0.7760 D(G(z)): 0.0273 / 0.0252 [181/200][200/462] Loss_D: 0.5726 Loss_G: 5.1489 D(x): 0.9449 D(G(z)): 0.1439 / 0.0071 [181/200][250/462] Loss_D: 0.4229 Loss_G: 4.6581 D(x): 0.8883 D(G(z)): 0.0334 / 0.0120 [181/200][300/462] Loss_D: 0.4068 Loss_G: 4.6416 D(x): 0.8260 D(G(z)): 0.0145 / 0.0165 [181/200][350/462] Loss_D: 0.4991 Loss_G: 4.3121 D(x): 0.6955 D(G(z)): 0.0123 / 0.0275 [181/200][400/462] Loss_D: 0.4709 Loss_G: 3.9962 D(x): 0.7772 D(G(z)): 0.0267 / 0.0288 [181/200][450/462] Loss_D: 0.4515 Loss_G: 4.3622 D(x): 0.7897 D(G(z)): 0.0199 / 0.0218 [182/200][0/462] Loss_D: 0.4347 Loss_G: 3.5844 D(x): 0.7899 D(G(z)): 0.0275 / 0.0351 [182/200][50/462] Loss_D: 0.4130 Loss_G: 4.6580 D(x): 0.8549 D(G(z)): 0.0431 / 0.0184 [182/200][100/462] Loss_D: 0.4462 Loss_G: 4.1335 D(x): 0.8852 D(G(z)): 0.0364 / 0.0218 [182/200][150/462] Loss_D: 0.3950 Loss_G: 4.4513 D(x): 0.8898 D(G(z)): 0.0338 / 0.0143 [182/200][200/462] Loss_D: 0.4774 Loss_G: 3.7487 D(x): 0.9240 D(G(z)): 0.0846 / 0.0335 [182/200][250/462] Loss_D: 0.3817 Loss_G: 4.5607 D(x): 0.8554 D(G(z)): 0.0216 / 0.0188 [182/200][300/462] Loss_D: 0.4911 Loss_G: 4.8379 D(x): 0.7447 D(G(z)): 0.0052 / 0.0124 [182/200][350/462] Loss_D: 0.4348 Loss_G: 5.4748 D(x): 0.9629 D(G(z)): 0.0314 / 0.0063 [182/200][400/462] Loss_D: 0.5759 Loss_G: 3.3566 D(x): 0.6604 D(G(z)): 0.0296 / 0.0622 [182/200][450/462] Loss_D: 0.4278 Loss_G: 5.2176 D(x): 0.9411 D(G(z)): 0.0407 / 0.0097 [183/200][0/462] Loss_D: 0.5111 Loss_G: 4.5579 D(x): 0.7538 D(G(z)): 0.0196 / 0.0177 [183/200][50/462] Loss_D: 0.3939 Loss_G: 4.8140 D(x): 0.9225 D(G(z)): 0.0340 / 0.0106 [183/200][100/462] Loss_D: 0.4209 Loss_G: 4.6838 D(x): 0.8314 D(G(z)): 0.0165 / 0.0149 [183/200][150/462] Loss_D: 0.4831 Loss_G: 4.3104 D(x): 0.8768 D(G(z)): 0.0597 / 0.0192 [183/200][200/462] Loss_D: 0.4414 Loss_G: 4.0281 D(x): 0.7906 D(G(z)): 0.0389 / 0.0280 [183/200][250/462] Loss_D: 0.4447 Loss_G: 5.1969 D(x): 0.9625 D(G(z)): 0.0488 / 0.0081 [183/200][300/462] Loss_D: 0.5228 Loss_G: 5.1900 D(x): 0.9781 D(G(z)): 0.0271 / 0.0074 [183/200][350/462] Loss_D: 0.5603 Loss_G: 5.0966 D(x): 0.9600 D(G(z)): 0.1147 / 0.0075 [183/200][400/462] Loss_D: 0.4164 Loss_G: 5.1687 D(x): 0.9232 D(G(z)): 0.0261 / 0.0081 [183/200][450/462] Loss_D: 0.4247 Loss_G: 4.6260 D(x): 0.8858 D(G(z)): 0.0311 / 0.0131 [184/200][0/462] Loss_D: 0.3974 Loss_G: 4.6362 D(x): 0.9239 D(G(z)): 0.0268 / 0.0149 [184/200][50/462] Loss_D: 0.4342 Loss_G: 3.9174 D(x): 0.9248 D(G(z)): 0.0387 / 0.0401 [184/200][100/462] Loss_D: 0.5100 Loss_G: 4.6980 D(x): 0.9451 D(G(z)): 0.0936 / 0.0142 [184/200][150/462] Loss_D: 0.3991 Loss_G: 4.4395 D(x): 0.9049 D(G(z)): 0.0286 / 0.0163 [184/200][200/462] Loss_D: 0.3891 Loss_G: 5.0447 D(x): 0.8616 D(G(z)): 0.0083 / 0.0092 [184/200][250/462] Loss_D: 0.4811 Loss_G: 4.1188 D(x): 0.9392 D(G(z)): 0.0772 / 0.0212 [184/200][300/462] Loss_D: 0.4225 Loss_G: 5.0637 D(x): 0.9450 D(G(z)): 0.0286 / 0.0104 [184/200][350/462] Loss_D: 0.4300 Loss_G: 5.6051 D(x): 0.8297 D(G(z)): 0.0042 / 0.0086 [184/200][400/462] Loss_D: 0.5611 Loss_G: 6.3470 D(x): 0.9387 D(G(z)): 0.1532 / 0.0028 [184/200][450/462] Loss_D: 0.4967 Loss_G: 4.9876 D(x): 0.9391 D(G(z)): 0.0891 / 0.0102 [185/200][0/462] Loss_D: 0.4104 Loss_G: 5.3879 D(x): 0.9418 D(G(z)): 0.0212 / 0.0077 [185/200][50/462] Loss_D: 0.6220 Loss_G: 4.1361 D(x): 0.6818 D(G(z)): 0.0163 / 0.0292 [185/200][100/462] Loss_D: 0.5583 Loss_G: 2.8602 D(x): 0.6938 D(G(z)): 0.0373 / 0.0685 [185/200][150/462] Loss_D: 0.3926 Loss_G: 5.4515 D(x): 0.8100 D(G(z)): 0.0062 / 0.0102 [185/200][200/462] Loss_D: 0.4440 Loss_G: 4.2907 D(x): 0.7488 D(G(z)): 0.0118 / 0.0252 [185/200][250/462] Loss_D: 0.5041 Loss_G: 3.7371 D(x): 0.7152 D(G(z)): 0.0080 / 0.0341 [185/200][300/462] Loss_D: 0.3975 Loss_G: 4.4002 D(x): 0.8971 D(G(z)): 0.0335 / 0.0185 [185/200][350/462] Loss_D: 0.3779 Loss_G: 5.0757 D(x): 0.9047 D(G(z)): 0.0159 / 0.0102 [185/200][400/462] Loss_D: 0.4613 Loss_G: 4.5535 D(x): 0.9606 D(G(z)): 0.0524 / 0.0153 [185/200][450/462] Loss_D: 0.4279 Loss_G: 4.1531 D(x): 0.8655 D(G(z)): 0.0282 / 0.0224 . [186/200][0/462] Loss_D: 0.4059 Loss_G: 4.8780 D(x): 0.8980 D(G(z)): 0.0163 / 0.0129 [186/200][50/462] Loss_D: 0.3907 Loss_G: 5.9008 D(x): 0.9450 D(G(z)): 0.0144 / 0.0048 [186/200][100/462] Loss_D: 0.4425 Loss_G: 5.1829 D(x): 0.9020 D(G(z)): 0.0255 / 0.0117 [186/200][150/462] Loss_D: 0.4105 Loss_G: 4.9977 D(x): 0.8197 D(G(z)): 0.0114 / 0.0103 [186/200][200/462] Loss_D: 0.3681 Loss_G: 5.8258 D(x): 0.9287 D(G(z)): 0.0088 / 0.0056 [186/200][250/462] Loss_D: 0.4765 Loss_G: 4.0059 D(x): 0.7836 D(G(z)): 0.0146 / 0.0293 [186/200][300/462] Loss_D: 0.3730 Loss_G: 5.2316 D(x): 0.9205 D(G(z)): 0.0134 / 0.0079 [186/200][350/462] Loss_D: 0.4325 Loss_G: 3.9687 D(x): 0.8290 D(G(z)): 0.0164 / 0.0267 [186/200][400/462] Loss_D: 0.3952 Loss_G: 3.4205 D(x): 0.8828 D(G(z)): 0.0313 / 0.0376 [186/200][450/462] Loss_D: 0.3946 Loss_G: 4.3978 D(x): 0.9016 D(G(z)): 0.0249 / 0.0145 [187/200][0/462] Loss_D: 0.3756 Loss_G: 5.0413 D(x): 0.8762 D(G(z)): 0.0171 / 0.0102 [187/200][50/462] Loss_D: 0.4264 Loss_G: 5.0251 D(x): 0.8226 D(G(z)): 0.0139 / 0.0124 [187/200][100/462] Loss_D: 0.3821 Loss_G: 4.4874 D(x): 0.9330 D(G(z)): 0.0269 / 0.0154 [187/200][150/462] Loss_D: 0.4305 Loss_G: 5.0227 D(x): 0.9469 D(G(z)): 0.0281 / 0.0120 [187/200][200/462] Loss_D: 0.3891 Loss_G: 5.7424 D(x): 0.8871 D(G(z)): 0.0065 / 0.0063 [187/200][250/462] Loss_D: 0.4587 Loss_G: 4.8695 D(x): 0.8385 D(G(z)): 0.0358 / 0.0144 [187/200][300/462] Loss_D: 0.3800 Loss_G: 5.2145 D(x): 0.9028 D(G(z)): 0.0266 / 0.0093 [187/200][350/462] Loss_D: 0.4169 Loss_G: 4.9582 D(x): 0.9091 D(G(z)): 0.0196 / 0.0112 [187/200][400/462] Loss_D: 0.6051 Loss_G: 2.7552 D(x): 0.6550 D(G(z)): 0.0365 / 0.0885 [187/200][450/462] Loss_D: 0.3900 Loss_G: 4.7849 D(x): 0.8574 D(G(z)): 0.0168 / 0.0126 [188/200][0/462] Loss_D: 0.4339 Loss_G: 4.5318 D(x): 0.8084 D(G(z)): 0.0153 / 0.0216 [188/200][50/462] Loss_D: 0.4671 Loss_G: 5.4754 D(x): 0.9685 D(G(z)): 0.0480 / 0.0072 [188/200][100/462] Loss_D: 0.3778 Loss_G: 4.3102 D(x): 0.8785 D(G(z)): 0.0163 / 0.0216 [188/200][150/462] Loss_D: 0.3836 Loss_G: 5.6433 D(x): 0.8654 D(G(z)): 0.0075 / 0.0076 [188/200][200/462] Loss_D: 0.4271 Loss_G: 4.3076 D(x): 0.8853 D(G(z)): 0.0592 / 0.0173 [188/200][250/462] Loss_D: 0.4797 Loss_G: 4.6658 D(x): 0.9534 D(G(z)): 0.0736 / 0.0125 [188/200][300/462] Loss_D: 0.3991 Loss_G: 4.4394 D(x): 0.8923 D(G(z)): 0.0368 / 0.0198 [188/200][350/462] Loss_D: 0.3850 Loss_G: 5.2201 D(x): 0.8722 D(G(z)): 0.0222 / 0.0118 [188/200][400/462] Loss_D: 0.4691 Loss_G: 4.7427 D(x): 0.8949 D(G(z)): 0.0893 / 0.0116 [188/200][450/462] Loss_D: 0.4830 Loss_G: 3.3758 D(x): 0.7630 D(G(z)): 0.0307 / 0.0509 [189/200][0/462] Loss_D: 0.4994 Loss_G: 5.9910 D(x): 0.7529 D(G(z)): 0.0069 / 0.0084 [189/200][50/462] Loss_D: 0.4147 Loss_G: 3.9605 D(x): 0.8902 D(G(z)): 0.0273 / 0.0270 [189/200][100/462] Loss_D: 0.4788 Loss_G: 5.3023 D(x): 0.7985 D(G(z)): 0.0107 / 0.0118 [189/200][150/462] Loss_D: 0.4038 Loss_G: 4.2337 D(x): 0.8878 D(G(z)): 0.0373 / 0.0165 [189/200][200/462] Loss_D: 0.3861 Loss_G: 4.4227 D(x): 0.8721 D(G(z)): 0.0267 / 0.0211 [189/200][250/462] Loss_D: 0.4201 Loss_G: 4.2693 D(x): 0.7823 D(G(z)): 0.0146 / 0.0227 [189/200][300/462] Loss_D: 0.3978 Loss_G: 5.0016 D(x): 0.9092 D(G(z)): 0.0330 / 0.0131 [189/200][350/462] Loss_D: 0.3988 Loss_G: 4.4063 D(x): 0.8355 D(G(z)): 0.0137 / 0.0182 [189/200][400/462] Loss_D: 0.5578 Loss_G: 4.1180 D(x): 0.6847 D(G(z)): 0.0134 / 0.0244 [189/200][450/462] Loss_D: 0.4020 Loss_G: 4.9377 D(x): 0.8237 D(G(z)): 0.0151 / 0.0125 [190/200][0/462] Loss_D: 0.3846 Loss_G: 4.9048 D(x): 0.8586 D(G(z)): 0.0138 / 0.0123 [190/200][50/462] Loss_D: 0.4138 Loss_G: 4.7230 D(x): 0.9116 D(G(z)): 0.0348 / 0.0137 [190/200][100/462] Loss_D: 0.3883 Loss_G: 4.9547 D(x): 0.8488 D(G(z)): 0.0121 / 0.0089 [190/200][150/462] Loss_D: 0.4525 Loss_G: 4.9612 D(x): 0.9630 D(G(z)): 0.0306 / 0.0107 [190/200][200/462] Loss_D: 0.3935 Loss_G: 4.8178 D(x): 0.9354 D(G(z)): 0.0234 / 0.0100 [190/200][250/462] Loss_D: 0.4317 Loss_G: 5.1951 D(x): 0.9309 D(G(z)): 0.0372 / 0.0102 [190/200][300/462] Loss_D: 0.4205 Loss_G: 4.8584 D(x): 0.8314 D(G(z)): 0.0324 / 0.0196 [190/200][350/462] Loss_D: 0.6259 Loss_G: 4.2665 D(x): 0.5941 D(G(z)): 0.0057 / 0.0233 [190/200][400/462] Loss_D: 0.4693 Loss_G: 5.0867 D(x): 0.9241 D(G(z)): 0.0896 / 0.0081 [190/200][450/462] Loss_D: 0.4392 Loss_G: 4.9223 D(x): 0.9630 D(G(z)): 0.0495 / 0.0089 . [191/200][0/462] Loss_D: 0.4624 Loss_G: 5.1247 D(x): 0.9620 D(G(z)): 0.0501 / 0.0084 [191/200][50/462] Loss_D: 0.5479 Loss_G: 3.5580 D(x): 0.7137 D(G(z)): 0.0229 / 0.0422 [191/200][100/462] Loss_D: 0.4351 Loss_G: 5.0241 D(x): 0.9535 D(G(z)): 0.0268 / 0.0106 [191/200][150/462] Loss_D: 0.4576 Loss_G: 3.4568 D(x): 0.7879 D(G(z)): 0.0277 / 0.0563 [191/200][200/462] Loss_D: 0.4037 Loss_G: 3.5609 D(x): 0.8101 D(G(z)): 0.0152 / 0.0375 [191/200][250/462] Loss_D: 0.4443 Loss_G: 4.4942 D(x): 0.9279 D(G(z)): 0.0550 / 0.0172 [191/200][300/462] Loss_D: 0.4635 Loss_G: 4.4201 D(x): 0.8148 D(G(z)): 0.0238 / 0.0160 [191/200][350/462] Loss_D: 0.4435 Loss_G: 4.6552 D(x): 0.9125 D(G(z)): 0.0459 / 0.0138 [191/200][400/462] Loss_D: 0.4255 Loss_G: 4.2694 D(x): 0.8097 D(G(z)): 0.0236 / 0.0253 [191/200][450/462] Loss_D: 0.3791 Loss_G: 4.7578 D(x): 0.8859 D(G(z)): 0.0199 / 0.0132 [192/200][0/462] Loss_D: 0.3865 Loss_G: 4.3545 D(x): 0.8398 D(G(z)): 0.0148 / 0.0168 [192/200][50/462] Loss_D: 0.5897 Loss_G: 4.2011 D(x): 0.6437 D(G(z)): 0.0101 / 0.0432 [192/200][100/462] Loss_D: 0.4088 Loss_G: 4.8861 D(x): 0.8712 D(G(z)): 0.0245 / 0.0106 [192/200][150/462] Loss_D: 0.5923 Loss_G: 3.9208 D(x): 0.6345 D(G(z)): 0.0071 / 0.0338 [192/200][200/462] Loss_D: 0.4115 Loss_G: 4.5688 D(x): 0.8214 D(G(z)): 0.0130 / 0.0176 [192/200][250/462] Loss_D: 0.3835 Loss_G: 4.8302 D(x): 0.9062 D(G(z)): 0.0281 / 0.0128 [192/200][300/462] Loss_D: 0.4649 Loss_G: 4.6532 D(x): 0.9289 D(G(z)): 0.0806 / 0.0124 [192/200][350/462] Loss_D: 0.3947 Loss_G: 6.1121 D(x): 0.9098 D(G(z)): 0.0082 / 0.0042 [192/200][400/462] Loss_D: 0.4613 Loss_G: 4.8367 D(x): 0.9570 D(G(z)): 0.0679 / 0.0101 [192/200][450/462] Loss_D: 0.5320 Loss_G: 4.8708 D(x): 0.6802 D(G(z)): 0.0059 / 0.0152 [193/200][0/462] Loss_D: 0.4707 Loss_G: 4.6488 D(x): 0.9607 D(G(z)): 0.0683 / 0.0128 [193/200][50/462] Loss_D: 0.4196 Loss_G: 6.3930 D(x): 0.8867 D(G(z)): 0.0042 / 0.0029 [193/200][100/462] Loss_D: 0.4033 Loss_G: 4.7851 D(x): 0.9096 D(G(z)): 0.0552 / 0.0125 [193/200][150/462] Loss_D: 0.3956 Loss_G: 5.5313 D(x): 0.8903 D(G(z)): 0.0069 / 0.0068 [193/200][200/462] Loss_D: 0.3957 Loss_G: 4.5738 D(x): 0.8627 D(G(z)): 0.0131 / 0.0136 [193/200][250/462] Loss_D: 0.3782 Loss_G: 4.4711 D(x): 0.9043 D(G(z)): 0.0248 / 0.0153 [193/200][300/462] Loss_D: 0.4048 Loss_G: 5.1073 D(x): 0.8538 D(G(z)): 0.0310 / 0.0122 [193/200][350/462] Loss_D: 0.4441 Loss_G: 5.3682 D(x): 0.9649 D(G(z)): 0.0347 / 0.0066 [193/200][400/462] Loss_D: 0.4012 Loss_G: 4.5990 D(x): 0.8520 D(G(z)): 0.0353 / 0.0129 [193/200][450/462] Loss_D: 0.3869 Loss_G: 4.8461 D(x): 0.9215 D(G(z)): 0.0183 / 0.0129 [194/200][0/462] Loss_D: 0.4169 Loss_G: 5.4151 D(x): 0.8412 D(G(z)): 0.0137 / 0.0118 [194/200][50/462] Loss_D: 0.4525 Loss_G: 5.5761 D(x): 0.9074 D(G(z)): 0.0219 / 0.0061 [194/200][100/462] Loss_D: 0.4169 Loss_G: 4.7616 D(x): 0.8655 D(G(z)): 0.0229 / 0.0147 [194/200][150/462] Loss_D: 0.3956 Loss_G: 5.0547 D(x): 0.8445 D(G(z)): 0.0176 / 0.0094 [194/200][200/462] Loss_D: 0.4266 Loss_G: 4.1551 D(x): 0.9450 D(G(z)): 0.0522 / 0.0185 [194/200][250/462] Loss_D: 0.4055 Loss_G: 5.0556 D(x): 0.8481 D(G(z)): 0.0429 / 0.0100 [194/200][300/462] Loss_D: 0.3853 Loss_G: 4.5730 D(x): 0.9026 D(G(z)): 0.0277 / 0.0149 [194/200][350/462] Loss_D: 0.4387 Loss_G: 5.0056 D(x): 0.9457 D(G(z)): 0.0398 / 0.0095 [194/200][400/462] Loss_D: 0.4757 Loss_G: 4.1521 D(x): 0.9481 D(G(z)): 0.0590 / 0.0205 [194/200][450/462] Loss_D: 0.4170 Loss_G: 4.1468 D(x): 0.9495 D(G(z)): 0.0397 / 0.0225 [195/200][0/462] Loss_D: 0.4355 Loss_G: 4.8113 D(x): 0.9575 D(G(z)): 0.0607 / 0.0114 [195/200][50/462] Loss_D: 0.4107 Loss_G: 5.1909 D(x): 0.9232 D(G(z)): 0.0242 / 0.0122 [195/200][100/462] Loss_D: 0.4177 Loss_G: 5.0336 D(x): 0.8303 D(G(z)): 0.0135 / 0.0121 [195/200][150/462] Loss_D: 0.4350 Loss_G: 5.0939 D(x): 0.8792 D(G(z)): 0.0160 / 0.0095 [195/200][200/462] Loss_D: 0.7897 Loss_G: 4.2230 D(x): 0.5336 D(G(z)): 0.0126 / 0.0386 [195/200][250/462] Loss_D: 0.3932 Loss_G: 4.7439 D(x): 0.8643 D(G(z)): 0.0201 / 0.0150 [195/200][300/462] Loss_D: 0.5016 Loss_G: 4.3132 D(x): 0.9628 D(G(z)): 0.0942 / 0.0172 [195/200][350/462] Loss_D: 0.4313 Loss_G: 5.0349 D(x): 0.9675 D(G(z)): 0.0353 / 0.0109 [195/200][400/462] Loss_D: 0.4611 Loss_G: 4.6858 D(x): 0.7787 D(G(z)): 0.0306 / 0.0223 [195/200][450/462] Loss_D: 0.4526 Loss_G: 4.2600 D(x): 0.7885 D(G(z)): 0.0163 / 0.0206 . [196/200][0/462] Loss_D: 0.4789 Loss_G: 3.9972 D(x): 0.8493 D(G(z)): 0.0852 / 0.0252 [196/200][50/462] Loss_D: 0.5057 Loss_G: 3.9106 D(x): 0.7152 D(G(z)): 0.0241 / 0.0387 [196/200][100/462] Loss_D: 0.4006 Loss_G: 4.8380 D(x): 0.8811 D(G(z)): 0.0228 / 0.0128 [196/200][150/462] Loss_D: 0.3786 Loss_G: 4.9380 D(x): 0.8764 D(G(z)): 0.0120 / 0.0112 [196/200][200/462] Loss_D: 0.4305 Loss_G: 5.5376 D(x): 0.8959 D(G(z)): 0.0261 / 0.0090 [196/200][250/462] Loss_D: 0.4301 Loss_G: 4.4070 D(x): 0.8649 D(G(z)): 0.0591 / 0.0184 [196/200][300/462] Loss_D: 0.4882 Loss_G: 3.4915 D(x): 0.7571 D(G(z)): 0.0242 / 0.0528 [196/200][350/462] Loss_D: 0.3706 Loss_G: 5.1451 D(x): 0.8730 D(G(z)): 0.0220 / 0.0152 [196/200][400/462] Loss_D: 0.4280 Loss_G: 4.6381 D(x): 0.8694 D(G(z)): 0.0344 / 0.0164 [196/200][450/462] Loss_D: 0.4516 Loss_G: 4.4001 D(x): 0.8356 D(G(z)): 0.0430 / 0.0165 [197/200][0/462] Loss_D: 0.5683 Loss_G: 3.6977 D(x): 0.7347 D(G(z)): 0.0241 / 0.0350 [197/200][50/462] Loss_D: 0.4042 Loss_G: 5.6045 D(x): 0.9341 D(G(z)): 0.0216 / 0.0058 [197/200][100/462] Loss_D: 0.4312 Loss_G: 4.2969 D(x): 0.8336 D(G(z)): 0.0324 / 0.0187 [197/200][150/462] Loss_D: 0.4172 Loss_G: 4.8956 D(x): 0.8328 D(G(z)): 0.0390 / 0.0114 [197/200][200/462] Loss_D: 0.4520 Loss_G: 4.8326 D(x): 0.9489 D(G(z)): 0.0342 / 0.0115 [197/200][250/462] Loss_D: 0.3859 Loss_G: 4.4846 D(x): 0.8721 D(G(z)): 0.0193 / 0.0149 [197/200][300/462] Loss_D: 0.5995 Loss_G: 3.5806 D(x): 0.6626 D(G(z)): 0.0436 / 0.0431 [197/200][350/462] Loss_D: 0.4181 Loss_G: 4.5928 D(x): 0.7993 D(G(z)): 0.0118 / 0.0143 [197/200][400/462] Loss_D: 0.3851 Loss_G: 4.9095 D(x): 0.8545 D(G(z)): 0.0101 / 0.0129 [197/200][450/462] Loss_D: 0.4561 Loss_G: 4.5553 D(x): 0.8930 D(G(z)): 0.0340 / 0.0126 [198/200][0/462] Loss_D: 0.3908 Loss_G: 4.9620 D(x): 0.8833 D(G(z)): 0.0199 / 0.0112 [198/200][50/462] Loss_D: 0.4746 Loss_G: 4.6692 D(x): 0.9617 D(G(z)): 0.0552 / 0.0119 [198/200][100/462] Loss_D: 0.3940 Loss_G: 5.0568 D(x): 0.8627 D(G(z)): 0.0167 / 0.0097 [198/200][150/462] Loss_D: 0.4319 Loss_G: 5.6370 D(x): 0.9682 D(G(z)): 0.0174 / 0.0052 [198/200][200/462] Loss_D: 0.4377 Loss_G: 5.2043 D(x): 0.9151 D(G(z)): 0.0491 / 0.0131 [198/200][250/462] Loss_D: 0.4035 Loss_G: 5.0493 D(x): 0.9279 D(G(z)): 0.0216 / 0.0107 [198/200][300/462] Loss_D: 0.4549 Loss_G: 5.0427 D(x): 0.9275 D(G(z)): 0.0420 / 0.0104 [198/200][350/462] Loss_D: 0.4330 Loss_G: 3.9328 D(x): 0.7891 D(G(z)): 0.0220 / 0.0325 [198/200][400/462] Loss_D: 0.4337 Loss_G: 4.0729 D(x): 0.8731 D(G(z)): 0.0416 / 0.0203 [198/200][450/462] Loss_D: 0.3629 Loss_G: 6.0366 D(x): 0.9046 D(G(z)): 0.0075 / 0.0042 [199/200][0/462] Loss_D: 0.3706 Loss_G: 5.0136 D(x): 0.8645 D(G(z)): 0.0093 / 0.0099 [199/200][50/462] Loss_D: 0.4870 Loss_G: 4.9224 D(x): 0.9298 D(G(z)): 0.0810 / 0.0100 [199/200][100/462] Loss_D: 0.4193 Loss_G: 5.3195 D(x): 0.9189 D(G(z)): 0.0437 / 0.0070 [199/200][150/462] Loss_D: 0.4034 Loss_G: 5.6929 D(x): 0.9362 D(G(z)): 0.0257 / 0.0065 [199/200][200/462] Loss_D: 0.5333 Loss_G: 4.2341 D(x): 0.8329 D(G(z)): 0.0906 / 0.0186 [199/200][250/462] Loss_D: 0.4142 Loss_G: 4.5558 D(x): 0.9415 D(G(z)): 0.0455 / 0.0175 [199/200][300/462] Loss_D: 0.4483 Loss_G: 5.1966 D(x): 0.9649 D(G(z)): 0.0424 / 0.0080 [199/200][350/462] Loss_D: 0.5321 Loss_G: 3.8531 D(x): 0.6920 D(G(z)): 0.0184 / 0.0348 [199/200][400/462] Loss_D: 0.4067 Loss_G: 5.4376 D(x): 0.8521 D(G(z)): 0.0112 / 0.0098 [199/200][450/462] Loss_D: 0.3963 Loss_G: 4.6203 D(x): 0.8583 D(G(z)): 0.0249 / 0.0151 [200/200][0/462] Loss_D: 0.3959 Loss_G: 4.6327 D(x): 0.9071 D(G(z)): 0.0187 / 0.0129 [200/200][50/462] Loss_D: 0.3982 Loss_G: 3.9742 D(x): 0.8605 D(G(z)): 0.0343 / 0.0244 [200/200][100/462] Loss_D: 0.4114 Loss_G: 4.4839 D(x): 0.8912 D(G(z)): 0.0307 / 0.0153 [200/200][150/462] Loss_D: 0.5038 Loss_G: 4.3629 D(x): 0.8955 D(G(z)): 0.1122 / 0.0160 [200/200][200/462] Loss_D: 0.4182 Loss_G: 4.6384 D(x): 0.9155 D(G(z)): 0.0276 / 0.0149 [200/200][250/462] Loss_D: 0.4402 Loss_G: 4.1156 D(x): 0.7772 D(G(z)): 0.0338 / 0.0260 [200/200][300/462] Loss_D: 0.4497 Loss_G: 3.9811 D(x): 0.9158 D(G(z)): 0.0396 / 0.0221 [200/200][350/462] Loss_D: 0.3709 Loss_G: 4.8445 D(x): 0.8520 D(G(z)): 0.0108 / 0.0112 [200/200][400/462] Loss_D: 0.5052 Loss_G: 4.0256 D(x): 0.7272 D(G(z)): 0.0128 / 0.0332 [200/200][450/462] Loss_D: 0.4756 Loss_G: 4.4365 D(x): 0.9545 D(G(z)): 0.0662 / 0.0188 . . Let&#39;s change the fixed noise batch a few times and see what we can produce. . gan_trainer.fixed_noise = torch.randn(3, 100, 1, 1, device=dev) f, ax = gan_trainer.plot_fixed_batch() plt.show() . gan_trainer.fixed_noise = torch.randn(3, 100, 1, 1, device=dev) f, ax = gan_trainer.plot_fixed_batch() plt.show() . gan_trainer.fixed_noise = torch.randn(3, 100, 1, 1, device=dev) f, ax = gan_trainer.plot_fixed_batch() plt.show() . gan_trainer.fixed_noise = torch.randn(3, 100, 1, 1, device=dev) f, ax = gan_trainer.plot_fixed_batch() plt.show() . Finally, let&#39;s save the generator. . torch.save(gan_trainer.generator, &#39;drive/MyDrive/Colab Notebooks/model_weights/gan_generator.pth&#39;) . How to make a GIF from your generated images . Let&#39;s use the Pillow library to save the generated images. We first get the image file names with the pathlib glob method and open these into a list of images. . We can then save a gif by appending all the images to the first image in the .save method of the PIL.Image class. . img_path = Path(&#39;drive/MyDrive/Colab Notebooks/GAN_images/&#39;) images = img_path.glob(&#39;4_*.png&#39;) images = list(images) ims = [Image.open(im) for im in images] first_im = ims[0] first_im.save(&#39;GAN_training.gif&#39;, save_all=True, append_images=ims[1:], optimize=False, duration=80, loop=0) . . Conclusion . So here we have trained a DCGAN to create pet faces. As you can see it&#39;s not the greatest but we have only trained for 200 epochs. I find it kind of remarkable that this even works at all though, it seems strange that you can go from random noise to something that even barely resembles a cat or dog. . There are several things we could do to improve this and potentially something to look at in a later post: . Spectral Normalization - a weight normalization technique that makes training more stable | Self Attention - an attention mechanism that helps produce more consistent images | .",
            "url": "https://jc639.github.io/blog/pytorch/deep-learning/neural-nets/gan/2021/05/20/Pets_DCGAN.html",
            "relUrl": "/pytorch/deep-learning/neural-nets/gan/2021/05/20/Pets_DCGAN.html",
            "date": " • May 20, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Getting pet faces!",
            "content": "The Oxford-IIIT Pet Dataset has 37 different breeds of cats and dogs, and comes with bounding box location for the face in each picture for around half of the dataset. I want to play around with DCGAN in a later post so I could just use those already in the train set for the DCGAN, but that&#39;s no fun! Let&#39;s also do some training and get the bounding box of those unlabelled images as well to maximise numbers to feed into the GAN. Here I am mostly using fast.ai to get this done, easily and quickly. . First, as I am writing this on Google Colab, let&#39;s install the library. (You can run this too if you click the Colab button) . !pip install fastai -q --upgrade . |████████████████████████████████| 194kB 7.9MB/s |████████████████████████████████| 12.8MB 221kB/s |████████████████████████████████| 776.8MB 23kB/s |████████████████████████████████| 61kB 8.3MB/s ERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you&#39;ll have torch 1.7.1 which is incompatible. . And get the pets dataset using URLS and untar_data. . from fastai.data.external import URLs, untar_data from fastai.vision.all import * import re path = untar_data(URLs.PETS) . untar_data returns a Pathlib object of where all the relevant files were downloaded to. The folder xmls/ contains XML files with amongst other things the information for the bounding boxes. The images are contained in the images/ folder. . xmls_path = list(path.glob(&#39;**/xmls/&#39;))[0] image_path = list(path.glob(&#39;**/images&#39;))[0] image_path . Path(&#39;/root/.fastai/data/oxford-iiit-pet/images&#39;) . XML is a markup language and similar to HTML has a tag format like &lt;tag&gt; .... &lt; tag&gt;. Each image has a corresponding XML file and contained in this is the filename of the image, the xmin, xmax, ymin and ymax of the bounding boxes. The functions below help us to read the file and to get the necessary information out. I add a little extra around the bounding box, where image size permits, as the bounding boxes were quite tight to the face. . def tag_search(tag, string, as_int=False): re_exp = r&#39;&lt;{}&gt;([A-Za-z0-9._]+)&lt;/{}&gt;&#39;.format(tag, tag) grp = re.search(re_exp, string).group(1) if as_int: return int(grp) return grp def read_file(file, add_val=10): with open(file, &#39;r&#39;) as f: lines = f.readlines() if len(lines) &gt; 1: raise Exception(&#39;Should only be a single line&#39;) line = lines[0] filename = tag_search(&#39;filename&#39;, line) xmin = tag_search(&#39;xmin&#39;, line, as_int=True) xmax = tag_search(&#39;xmax&#39;, line, as_int=True) ymin = tag_search(&#39;ymin&#39;, line, as_int=True) ymax = tag_search(&#39;ymax&#39;, line, as_int=True) height = tag_search(&#39;height&#39;, line, as_int=True) width = tag_search(&#39;width&#39;, line, as_int=True) return filename, max(xmin - add_val, 0), min(xmax + add_val, width), max(ymin - add_val, 0), min(ymax + add_val, height) def get_items(path): return [[read_file(file)] for file in path.ls()] items = get_items(xmls_path) . Let&#39;s have a look at an example to see if this works. . example = items[502][0] fname = example[0] left = example[1] upper = example[3] right = example[2] lower = example[4] img = Image.open(image_path/fname) img.crop((left, upper, right, lower)) . Pretty cute. Let&#39;s iterate through these and save the cropped photos in a folder. Here I have mounted my GDrive in Colab so I can stored these crops there. . drive_path = Path(&#39;drive/MyDrive/Colab Notebooks/cropped_images/&#39;) drive_path.mkdir(exist_ok=True) for it in items: item = it[0] fname = item[0] left = item[1] upper = item[3] right = item[2] lower = item[4] img = Image.open(image_path/fname) cropped_img = img.crop((left, upper, right, lower)) cropped_img.save(drive_path/fname) print(item) . (&#39;British_Shorthair_190.jpg&#39;, 142, 474, 0, 270) . Now we need to make some getter methods to get X - the image, and y - the bounding box points so we can set up training a neural network to do the bounding box prediction for the remaining unlabelled images. . These will work on the tuples of each item. For y we give a list of points that correspond to left, upper, right, lower. These will get interpreted as the x,y coordinates for the top left (first two points) and bottom right (last two points). . def get_x(tup): return image_path/tup[0][0] def get_y(tup): return [tup[0][1], tup[0][3], tup[0][2], tup[0][4]] . Now use these in a fastai DataBlock to be able to easily load the Image and the Regression points. . dblock = DataBlock( blocks=(ImageBlock, PointBlock), get_items=get_items, get_x=get_x, get_y=get_y, item_tfms=Resize(96), splitter=RandomSplitter(0.1) ) . The get_items works on a Pathlib object so lets pass in the xmls path. . dls = dblock.dataloaders(xmls_path) . We can use .show_batch on fastai dataloaders to show a batch. Here you can see the images, and the corresponding points (very faint red dots) around the heads of the animals. . dls.show_batch() . Now let&#39;s do some learning. We can use the cnn_leaner from fastai to create a convolutional neural network to do this computer vision task. If you are wondering where these functions and resnet34 came from, fastai imports all these when you do fastai.vision.all import * (I am not a massive fan of this wildcard import style, but it is recommended in the fastai docs as how to work with their library) . learn = cnn_learner(dls, resnet34) . Downloading: &#34;https://download.pytorch.org/models/resnet34-333f7ec4.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth . . Learning rate finder - want to choose a point where the loss is rapidly decreasing but some way before it rapidly shoots up again. . learn.lr_find() . SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.02754228748381138) . Time to train! For the number of freeze_epochs we are training just the new head of the resnet, the rest of the the pre-trained parameters are frozen for these epochs. . learn.fine_tune(10, freeze_epochs=3, base_lr=1e-2) . epoch train_loss valid_loss time . 0 | 3.094963 | 0.855605 | 00:21 | . 1 | 1.448034 | 0.181688 | 00:20 | . 2 | 0.643637 | 0.141903 | 00:20 | . epoch train_loss valid_loss time . 0 | 0.160587 | 0.074748 | 00:21 | . 1 | 0.112519 | 0.081809 | 00:21 | . 2 | 0.131000 | 8.762899 | 00:21 | . 3 | 0.160365 | 0.053658 | 00:21 | . 4 | 0.136672 | 0.086606 | 00:21 | . 5 | 0.110147 | 0.051879 | 00:21 | . 6 | 0.078456 | 0.030404 | 00:21 | . 7 | 0.061008 | 0.024650 | 00:21 | . 8 | 0.046319 | 0.021720 | 00:21 | . 9 | 0.040444 | 0.020882 | 00:21 | . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=9.12010818865383e-08, lr_steep=4.786300905834651e-06) . learn.fit_one_cycle(3, slice(1e-7, 1e-6)) . epoch train_loss valid_loss time . 0 | 0.035879 | 0.020362 | 00:21 | . 1 | 0.035305 | 0.020359 | 00:21 | . 2 | 0.034114 | 0.020699 | 00:21 | . Seems good enough, let&#39;s see how it does. learn.show_results() shows the results of a applying the model to a batch of the validation dataset by default. Train loss is a tiny bit above valid loss - so maybe we could try training for longer, but the results look good enough to me. . learn.show_results() . Now let&#39;s do a prediction of a single image, and figure out how to crop with that. . all_images = set([f.name for f in image_path.ls()]) train_imgs = set([item[0][0] for item in items]) test_imgs = [img for img in list(all_images.difference(train_imgs)) if img.endswith(&#39;.jpg&#39;)] fname = test_imgs[0] img = PILImage.create(image_path/fname) _, points, _ = learn.predict(img) . img . def get_crop_points(points, w, h): # points are expressed in -1 to 1 range image, but can be outside the image dims. prop = ((points + 1) / 2).tolist() left = max(int(w * prop[0]), 0) upper = max(int(h * prop[1]), 0) right = min(int(w * prop[2]), w) lower = min(int(h * prop[3]), h) return left, upper, right, lower . w, h = img.size . img.crop(get_crop_points(points, w, h)) . Finally let&#39;s get predictions for all the images that didn&#39;t have the bounding boxes. . for img_file in test_imgs: img = PILImage.create(image_path/img_file) _, points, _ = learn.predict(img) w, h = img.size crop_pts = get_crop_points(points, w, h) cropped_img = img.crop(crop_pts) cropped_img.save(drive_path/f&#39;test_crop_{img_file}&#39;) . Done! . test_crop_file = [img for img in os.listdir(&#39;drive/MyDrive/Colab Notebooks/cropped_images/&#39;) if &#39;test_crop&#39; in img][100] img = PILImage.create(drive_path/test_crop_file) img . img = PILImage.create(image_path/test_crop_file.replace(&#39;test_crop_&#39;, &#39;&#39;)) img .",
            "url": "https://jc639.github.io/blog/fastai/neural-nets/bounding-box/2021/04/29/pets_bounding_box.html",
            "relUrl": "/fastai/neural-nets/bounding-box/2021/04/29/pets_bounding_box.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "A PyTorch Refresher",
            "content": "PyTorch is a great library for deep learning for many reasons; including but not limited to: . It&#39;s easy to learn and use. | Very well documented with many great tutorials, official and unofficial. | Very popular, with a strong community and officially backed by Facebook. | Neat and ordered API. It&#39;s great for getting used to Python classes. | . It&#39;s been a little while since I dipped my toes into the PyTorch water, so I want to take this opportunity to do a complete refresher covering the basics of PyTorch. Maybe you will find it useful too. If you want to run this notebook too you can click the Colab button, be sure to set the runtime to GPU. There is nothing that revolutionary in this post but we will go from an image dataset to a trained image classification model. . We need three things to do deep learning: . Data - input data and output labels/targets. | A model - something that takes in the inputs and produces a probability or value that can be compared to the targets. | A loss function - something that takes our predictions and the actual targets and tells us how well the model is doing. | Data . As PyTorch is a machine learning library we are going to need some data, something that we can input into the model with corresponding labels that we can attempt to predict. I am going to use some of the functionality of the fastai library to quickly get a dataset we can play with: Imagenette . Imagenette is a much smaller subset of the famous Imagenet dataset and consists of 10 classes of images: . tench | English springer | cassette player | chain saw | church | French horn | garbage truck | gas pump | golf ball | parachute | . Let&#39;s have a look at some of the data. . from fastai.data.external import untar_data, URLs path = untar_data(URLs.IMAGENETTE_160) . This downloads the data and returns a fastai verion of a pathlib object. Here I can call ls on this object to list the files and directories in the path where the data was downloaded. Among these you can see that we get a /train and /val directory. . path.ls() . (#4) [Path(&#39;/root/.fastai/data/imagenette2-160/.DS_Store&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/val&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/noisy_imagenette.csv&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train&#39;)] . Let&#39;s have a look in the /train directory to see what we have. Here we can see there are 11 items in this list, one folder for each class and an extra .DS_Store file (we don&#39;t need this). . train_dir = [p for p in path.ls() if p.name == &#39;train&#39;][0] val_dir = [p for p in path.ls() if p.name == &#39;val&#39;][0] train_dir.ls() . (#11) [Path(&#39;/root/.fastai/data/imagenette2-160/train/n03445777&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03394916&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/.DS_Store&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n02102040&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03000684&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n01440764&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03888257&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03417042&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03425413&#39;),Path(&#39;/root/.fastai/data/imagenette2-160/train/n03028079&#39;)...] . Let&#39;s check that assumption that the images of a given class are separated into specific folders and have a peek at some images in these folders. Here I am using another utility function from fastai show_image that just helps with showing images with matplotlib. . from fastai.vision.all import show_image from PIL import Image import matplotlib.pyplot as plt train_folder = train_dir.ls()[1] print(f&#39;Train folder name={train_folder.name}&#39;) first_img, second_img = train_folder.ls()[:2] f, ax = plt.subplots(1, 2) img1 = Image.open(first_img) img2 = Image.open(second_img) _ = show_image(img1, ctx=ax[0]) _ = show_image(img2, ctx=ax[1]) . Train folder name=n03394916 . It certainly looks like the classes are given by the directory name of the folder that contains the images. We want to confirm that the same classes are in the same named folder in the val/. Let&#39;s have a look at that. . val_folder = val_dir.ls()[1] print(f&#39;Folder name={val_folder.name}&#39;) first_img, second_img = val_folder.ls()[:2] f, ax = plt.subplots(1, 2) img1 = Image.open(first_img) img2 = Image.open(second_img) _ = show_image(img1, ctx=ax[0]) _ = show_image(img2, ctx=ax[1]) . Folder name=n03394916 . That&#39;s some fine tootin&#39;. Great it looks like the second folder of both train/ and val/ contain French horns, and both have the same directory name in their respective locations. . Tensors . Right, that&#39;s all well and good but how do we use this data and what are images anyway? Well, digital images can be represented as arrays so if you have a colour image you have a 3-dimensional array that has the shape height x width x channels where red pixel intensity is represented in one channel, green in the next channel and blue in the final channel for RGB images. . Let&#39;s look at this more concretely. Below we import NumPy and transform the loaded img1 from earlier into an array. NumPy knows how to do this with a PIL Image. . import numpy as np arr = np.array(img1) print(&#39;The image array has the shape: &#39;, arr.shape) print(&#39;The top 10x10px in the red channel have the values: n&#39;, arr[:10, :10, 0]) . The image array has the shape: (160, 185, 3) The top 10x10px in the red channel have the values: [[233 233 233 233 233 233 233 233 234 234] [234 234 234 234 234 234 234 234 234 234] [234 234 234 234 234 234 234 234 234 234] [235 235 235 235 235 235 235 235 234 234] [235 235 235 235 235 235 235 235 234 234] [234 234 234 234 234 234 234 234 234 234] [234 234 234 234 234 234 234 234 234 234] [233 233 233 233 233 233 233 233 234 234] [233 234 234 234 234 233 232 231 235 234] [233 234 234 234 234 233 232 231 234 233]] . . As you can see the image is 160px by 213px and has 3 channels. Let&#39;s show this image, and each channel isolated. Here I have created a simple function isolate_channel which sets all pixel values in all channels except the selected one to 0, and plot_img to help with showing these. . def isolate_channel(arr, chan=0): &quot;&quot;&quot;Sets all channels which are not chan to 0 intensity&quot;&quot;&quot; chans = [i for i in range(3) if i != chan] img_copy = arr.copy() img_copy[:, :, chans] = 0 return img_copy def plot_img(arr, ax, title): &quot;&quot;&quot;Plots image with given axes and title&quot;&quot;&quot; ax.imshow(arr) ax.axis(&#39;off&#39;) ax.set_title(title) f, ax = plt.subplots(2, 2) ax = ax.flatten() plot_img(arr, ax=ax[0], title=&#39;Original Image&#39;) for i, title in enumerate([&#39;Red Channel&#39;, &#39;Green Channel&#39;, &#39;Blue Channel&#39;]): plot_img(isolate_channel(arr, i), ax=ax[i+1], title=title) f.set_size_inches(8, 4) plt.tight_layout() . So how does this relate to tensors? Well, tensors are essentially the same thing, if you are familiar with NumPy&#39;s vectors and arrays then you will know how to work with tensors. Tensors tend to be described by rank with the following relation to other data structures: . Rank Structure . 0 | Scalar | . 1 | Vector | . 2 | Matrix | . 3 | 3-Dimensional Array | . Let&#39;s convert our NumPy array to a torch tensor. See how its the same to index into as the NumPy array and has the same size/shape . import torch ten = torch.tensor(arr) print(&#39;Top 10x10px of the red channel: n&#39;, ten[:10, :10, 0]) print(&#39;Tensor shape is: &#39;, ten.size(), &#39; | &#39;, ten.shape) . Top 10x10px of the red channel: tensor([[233, 233, 233, 233, 233, 233, 233, 233, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [235, 235, 235, 235, 235, 235, 235, 235, 234, 234], [235, 235, 235, 235, 235, 235, 235, 235, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [234, 234, 234, 234, 234, 234, 234, 234, 234, 234], [233, 233, 233, 233, 233, 233, 233, 233, 234, 234], [233, 234, 234, 234, 234, 233, 232, 231, 235, 234], [233, 234, 234, 234, 234, 233, 232, 231, 234, 233]], dtype=torch.uint8) Tensor shape is: torch.Size([160, 185, 3]) | torch.Size([160, 185, 3]) . The cool thing about torch tensors is that they are aware of the actions (matrix multiplication, additions, etc) that happen to them and any chain of events subsequently so that gradients can be calculated automatically. Remember the gradient of a function $f$ just tells you what you need to change about $x$ to make $f(x)$ lower (or higher if you do the opposite). . This is not that useful for your input data represented as a tensor; you don&#39;t want to use any gradients to change your input data. But you can store the parameters of your models as tensors, and you do want to change these during training to make your model better. This you can do by using the gradients after every forward pass. See here for a really good overview. . Let&#39;s see this in action. Starting with an easy one, if you are familiar with the power rule you will know that the gradient of the square function $x^2$ is $2x$. Below we have a tensor [1., 2., 3.] which we have asked PyTorch to calculate gradients by using requires_grad=True. We pass this tensor through a simple square function which returns the square of each element in the tensor. . def square(x): return x**2 ten = torch.tensor([1., 2. , 3.], requires_grad=True) print(&#39;Tensor is: &#39;, ten) out = square(ten) print(&#39;The result of squaring the tensor is: &#39;, out) out = out.sum() out.backward() print(&#39; nComputing gradients:&#39;) print(ten.grad) . Tensor is: tensor([1., 2., 3.], requires_grad=True) The result of squaring the tensor is: tensor([1., 4., 9.], grad_fn=&lt;PowBackward0&gt;) Computing gradients: tensor([2., 4., 6.]) . As you can see calling .backward() on the summed output of the square() function automatically calculates the gradients for the tensor ten. . Why is this useful then? Well, generally for a supervised machine learning task we want: . Input data | A model to convert our input data to predictions. The model has parameters that we want to update by computing their gradients after each forward pass | A loss function that takes in the predictions and the target that we want the prediction to match. The lower the value returned the better we are doing. Calling backward on the output of the loss function allows us to get gradients so that the parameters of the model can be modified in a manner that should hopefully make the loss value returned lower on the next pass of the data. | . Let&#39;s make a small contrived example with that setup. We will have some input data, that we will put through a &quot;model&quot; and then compute the loss using the output of that &quot;model&quot;. With that loss, we can compute the gradients and modify the weights of our &quot;model&quot; with them. For this we have the following: . Input data which is always an array of [1, 2, 3, 4] | A model with some randomly initialized parameters. In this case just a tensor of shape 4x1 | A loss function. In this case, we will just use $x^2$ as it is a differentiable function, and we can nicely plot the loss value at a range of $x$ points. In reality, we want a loss function that takes in model outputs and targets. The output of the loss function should be lower when the model is better at predicting the given target and should also be a smooth and differentiable function. | . from matplotlib.animation import FuncAnimation from IPython.display import HTML inp = torch.tensor([1., 2., 3., 4.]) weights = torch.tensor([[0.5945], [0.8308], [0.1952], [0.9057]], requires_grad=True) def loss_func(x): return x**2 def one_forward_pass(inp=inp, weights=weights): out = inp @ weights loss = loss_func(out) loss.backward() weights.data = weights.data - 0.001*weights.grad weights.grad = None return out, weights, loss . def animate(i): o, w, loss = one_forward_pass() scatter.set_offsets([o.item(), loss.item()]) text.set_text(f&#39;Forward Pass {i + 1} nLoss={loss.item():.2f} nWeights={w.data.squeeze()}&#39;) return scatter, anim = FuncAnimation(fig, animate, frames=25, interval=500, blit=True) HTML(anim.to_jshtml()) . &lt;/input&gt; Once Loop Reflect If you follow the red dot you can see after each iteration the parameters are changed by using their gradients and the subsequent loss value is a bit lower. You can see how we are moving down the slope towards the minimum value of the loss which is when the &quot;model&quot; outputs 0. Our input data is staying the same on each iteration but the change in parameters is helping us to get there. This is gradient descent - using gradients to minimize the output of a function. . So we can see how we can use tensors and automatic gradient calculation to optimise functions, so we could just use torch.tensor to get out data into shape for modelling, construct a model, calculate the loss and compute gradients to do some deep learning. You could do that but then you are missing out on a lot of the functionality Pytorch has to offer to make our lives easier. . Datasets . First, up Datasets, and by this, I don&#39;t just mean the images we already have. Pytorch has an abstract Dataset class which you can use to represent your data and get the collection of input and targets of whatever items you wish to train with. Here I am demonstrating this with the images from Imagenette, but it could be text, sound, tabular data; anything really. . Datasets must inherit from the parent class torch.utils.data.Dataset and implement two methods: . __get__ so that we can index into a dataset and the item at the $ith$ index | __len__ so that calling len on a Dataset returns the number of items in it. | . I am going to use some of the transforms from the torchvision library, so it&#39;s worth pointing out here that when working with images that torchvision already has useful functions for creating datasets, so creating a custom Dataset here is unnecessary but is useful for understanding how to implement your own Dataset class if you ever want to. I finding implementing your own Dataset is also a good way to get to know your data. The same is true of torchtext if you are working with text data. . from torch.utils.data import Dataset class ImageDataset: &quot;&quot;&quot;Custom Dataset class for working with images&quot;&quot;&quot; def __init__(self, files, label_func=None, transforms=None): &quot;&quot;&quot;Pass in a list of image files and a way to label them :files (list): list of image file filepaths :label_func (callable): A function to get the label from image name :transforms (callable): Optional transforms to apply to the images &quot;&quot;&quot; if label_func is None: raise Exception(&#39;label_func is None. A way to label the files must be passed&#39;) self.files = files self.label_func = label_func self.transforms = transforms def __len__(self): return len(self.files) def __getitem__(self, i): img_name = self.files[i] label = self.label_func(img_name) image = Image.open(img_name).convert(&#39;RGB&#39;) if self.transforms: image = self.transforms(image) return {&#39;image&#39;: image, &#39;label&#39;: label} . To use this Dataset class we need to pass in a list of Pathlib files, and a way to label them. We can use the .glob method of the Pathlib to find all files that match the .JPEG pattern like below: . train_files = [i for i in train_dir.glob(&#39;**/*.JPEG&#39;)] train_files[0] . Path(&#39;/root/.fastai/data/imagenette2-160/train/n03445777/n03445777_6465.JPEG&#39;) . To label the image we need to know the set of unique labels of which there are 10 in total, and have a dictionary that converts the string to an index, and then sets the corresponding index in a tensor to 1 for that image, label pair. . unique_labels = [d.name for d in train_dir.ls() if d.is_dir()] label_dict = {lab: i for i, lab in enumerate(unique_labels)} def get_label(filename): label_ten = torch.zeros((10, 1)) label_str = filename.parent.name label_ten[label_dict[label_str]] = 1.0 return label_ten get_label(train_files[0]) . tensor([[1.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.]]) . Let&#39;s see this in action. I am passing the list of train image files and the get_label as the label function to convert the label to a tensor. Indexing into the train_ds returns the image and label at the i-th index as a dictionary. . train_ds = ImageDataset(files=train_files, label_func=get_label) first_example = train_ds[0] first_example[&#39;label&#39;] first_example[&#39;image&#39;] . Nice! Seems to be working, but let&#39;s have a look at a couple of image sizes. . print(&#39;Image is size &#39;, train_ds[178][&#39;image&#39;].size) print(&#39;Image is size &#39;, train_ds[-150][&#39;image&#39;].size) . Image is size (181, 160) Image is size (160, 204) . Data Augmentation . As you can see these images are different sizes. For computer vision models to work the input tensors have to be of the same shape (height x width). There are several different ways we can do this. The simplest is to just reshape every image to a square, 124 x 124 for instance, regardless of the original size. The problem with this is that we will be squishing and stretching the image in ways that will distort the shape of anything contained in the image. . This simple technique also misses out on a key thing we can do to maximise what we can get out of the training data - namely data augmentation. Data augmentation is a technique where we apply transforms to the input such that each time we get the i-th example from our ImageDataset there is some randomness in the transform and the image we get back is not always exactly the same. In this way, it effectively increases the size of the training set. . For the training dataset, I am going to make use of some transforms from the torchvision library to do this. . from torchvision.transforms import Resize, RandomCrop, RandomHorizontalFlip, Compose # resizes smallest edge to the given size - this should already be 160, but to be safe resize = Resize(size=160) # takes a random crop of size (h, w) random_crop = RandomCrop(size=(124, 124)) # randomly flip the image on the along y axis with probablity = p random_flip = RandomHorizontalFlip(p=0.5) # compose these all together train_transforms = Compose( [ resize, random_crop, random_flip ] ) train_ds = ImageDataset(files=train_files, label_func=get_label, transforms=train_transforms) . Let&#39;s get an example from this dataset now and have a look. . train_ds[3456][&#39;image&#39;] . And again... As you can see these are now not exactly the same now that we have thrown some randomness into the mix. . train_ds[3456][&#39;image&#39;] . Currently what we get back from ImageDataset is a PIL image and a tensor representing the labels but for training we want both the image and the labels to be converted to a tensor. For this, we just need to add in a ToTensor transform, which will do this and convert the pixel values to between 0-1. Notice how when we call shape it is slightly different to what we get when the image array shape is displayed with numpy. Pytorch expects images to have shape Channel, Height, Width as opposed to Height, Width, Channel that we get with numpy. . Finally, we add a normalisation transform to get tensors in the range -1 to 1. Here I am just going to use the Imagenet stats for the mean and standard deviation of each channel, as Imagenette comes from Imagenet data. What you do here depends on what you are using for your model as well. If you are going to use transfer learning and the pre-trained model that you start from has learnt from Imagenet you will also want to use Imagenet stats. If you are starting with a new model, and a new dataset then you want to compute the channels means and standard deviation on your training set images and pass these stats to the normalisation transform. . from torchvision.transforms import ToTensor, Normalize to_tensor = ToTensor() # normalisation [Red channel mean, B mean, G mean], [R std dev, G std dev, B std dev] norm = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) train_transforms = Compose( [ resize, random_crop, random_flip, to_tensor, norm ] ) train_dataset = ImageDataset(files=train_files, label_func=get_label, transforms=train_transforms) first_example = train_dataset[0] print(first_example[&#39;image&#39;][0, :10, :10])# 10 x 10 pixels from the first channel (red) print(first_example[&#39;label&#39;]) print(first_example[&#39;image&#39;].shape) . tensor([[-0.8335, -0.5253, -0.3027, -0.4739, -0.2513, -0.1143, -0.1828, -0.1657, -0.0801, -0.1314], [-0.1657, -0.0972, -0.2684, -0.3027, -0.2171, -0.1143, -0.0972, -0.2342, -0.4054, -0.4568], [-0.1999, -0.1828, -0.2342, -0.1486, -0.1828, -0.1828, -0.2171, -0.4568, -0.7479, -0.7993], [-0.5767, -0.5424, -0.1657, -0.1657, -0.1314, -0.1657, -0.3369, -0.5253, -0.6109, -0.5767], [-0.3198, -0.4054, 0.0569, -0.1999, 0.0056, -0.0116, -0.2684, -0.3541, -0.1314, 0.0227], [-0.5253, -0.2513, 0.0227, -0.0629, -0.0629, 0.0741, -0.0801, -0.3541, -0.0458, 0.3652], [-0.6281, -0.4739, -0.4397, -0.5424, -0.4226, -0.1657, -0.0629, -0.2171, -0.1999, 0.0569], [-0.3369, -0.3198, -0.4911, -0.1999, -0.3541, -0.4054, -0.0972, 0.2282, 0.1426, -0.0458], [-0.3541, -0.2342, -0.2342, -0.0458, -0.1999, -0.5596, -0.4739, -0.0629, -0.1314, -0.4226], [-0.8164, -0.5082, -0.2171, -0.3541, -0.0287, -0.1828, -0.5082, -0.5253, -0.5596, -0.5424]]) tensor([[1.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.]]) torch.Size([3, 124, 124]) . Great so now we have our training dataset in our ImageDataset class that we can index into and get the image as a tensor with random cropping and mirroring, and the label as a tensor. . We now need the same for validation images. Luckily this is quite easy we just reuse the same custom class with a few key differences. We don&#39;t want our transforms to be random here. This is the validation set and we want the same image/tensor to be returned each time we pass through this dataset, otherwise, the accuracy metrics won&#39;t be consistent. To achieve this we will resize the smallest edge to the same size as the output of the training dataset, and then take a central square crop of the same size each time. . from torchvision.transforms import CenterCrop val_resize = Resize(124) val_crop = CenterCrop(124) valid_transforms = Compose( [ val_resize, val_crop ] ) val_files = [i for i in val_dir.glob(&#39;**/*.JPEG&#39;)] val_dataset = ImageDataset(files=val_files, label_func=get_label, transforms=valid_transforms) . Let&#39;s have a look at the first image in the validation set. . val_dataset[0][&#39;image&#39;] . And again. No randomness here, it&#39;s the same again. . img = val_dataset[0][&#39;image&#39;] print(img.shape) img . (124, 124) . This looks good so now we again want to make sure the image is converted to tensor and is normalised, so let&#39;s add those to the transforms. . val_to_tensor = ToTensor() # its important we use the same normalisation stats as the train, i.e don&#39;t recompute for validation set ever. val_normalise = Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) valid_transforms = Compose( [ val_resize, val_crop, val_to_tensor, val_normalise ] ) val_dataset = ImageDataset(files=val_files, label_func=get_label, transforms=valid_transforms) val_dataset[-1000] . {&#39;image&#39;: tensor([[[ 0.4851, 0.5878, 0.7762, ..., 1.8722, 1.8550, 1.8550], [-0.1314, 0.0398, 0.2624, ..., 2.2489, 2.2489, 2.2318], [-0.0972, 0.1254, 0.2967, ..., 2.2489, 2.2489, 2.2489], ..., [-1.4329, -1.0390, -1.0562, ..., 0.6221, 0.6563, 0.6563], [-1.6727, -1.4843, -1.0904, ..., 0.5707, 0.6049, 0.6049], [-0.4911, -0.3883, -0.2856, ..., 1.0502, 1.0844, 1.0844]], [[ 0.1527, 0.2402, 0.3978, ..., 2.0434, 2.0434, 2.0259], [-0.4776, -0.3375, -0.1450, ..., 2.4286, 2.4286, 2.4111], [-0.4426, -0.2675, -0.1099, ..., 2.4111, 2.3936, 2.3936], ..., [-1.6331, -1.2654, -1.2829, ..., 0.1001, 0.1352, 0.1527], [-1.8782, -1.7031, -1.3354, ..., 0.0651, 0.1001, 0.1001], [-0.6877, -0.6001, -0.5126, ..., 0.5728, 0.6078, 0.6078]], [[ 0.0779, 0.1476, 0.2871, ..., 2.3088, 2.2914, 2.2566], [-0.5844, -0.4275, -0.2532, ..., 2.6226, 2.6226, 2.6051], [-0.5495, -0.3753, -0.2358, ..., 2.5877, 2.5703, 2.5529], ..., [-1.5779, -1.2467, -1.2467, ..., -0.0441, -0.0092, 0.0082], [-1.7870, -1.6476, -1.3164, ..., -0.0615, -0.0267, -0.0267], [-0.6367, -0.5844, -0.4973, ..., 0.4439, 0.4788, 0.4788]]]), &#39;label&#39;: tensor([[0.], [0.], [0.], [0.], [0.], [0.], [0.], [1.], [0.], [0.]])} . Dataloaders . Datasets get our image, label at the i-th index for us but to be able to train efficiently we want to stack several items together into batches. That&#39;s where the DataLoader class from torch.utils.data comes in. . We can pass our datasets into this class and quite easily get something that returns our image tensors batched together. The default dataloader will most often do the job for us but we can inherit from this class and create a more custom DataLoader if needed. See a more custom DataLoader I have created in the past to get an idea of how we can do this. . The key parameters here are dataset - the dataset we have previously implemented, batch_size - how many items we want to stack together into a batch and shuffle - whether to randomise the order of items before batching them up. . One thing you might want to change is how the DataLoader collates your items into batches and you can do that by passing in a function with the collate_fn parameter. Here this isn&#39;t necessary as batching these images and labels together is quite easy. You might want to do this if you are implementing padding in a text DataLoader though. See here and here for examples of where I have done this before. . from torch.utils.data import DataLoader train_dl = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True) . A DataLoader returns an iterable so that we can iterate through batches. We can use next and iter to grab one batch for inspection. Let&#39;s check the size of the image batch and the label batch. As you can see images are batched together with 64 in total and all have 3 channels, and a height and width of 124 pixels. . The label batch again has 64 items, each one with a shape of 10 rows by 1 column relating to the 10 possible options for each label. Only one row of the 10 will be equal to 1 and the rest as 0s as this is a single label classification task. . batch = next(iter(train_dl)) print(&#39;Images have been batched into a batch of size&#39;, batch[&#39;image&#39;].shape) print(&#39;Labels have been batched into a batch of size&#39;, batch[&#39;label&#39;].shape) batch[&#39;label&#39;][0] . Images have been batched into a batch of size torch.Size([64, 3, 124, 124]) Labels have been batched into a batch of size torch.Size([64, 10, 1]) . tensor([[1.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.], [0.]]) . Again with the validation DataLoader, it&#39;s simple enough to do the same thing as you do with the training set. One thing we can do differently is to use a higher batch size. With the validation data, we won&#39;t be tracking forward passes and accumulating gradients so larger batches won&#39;t have as large a footprint on GPU usage compared to the same batch size for the training set. It&#39;s also not necessary to shuffle the validation data. . val_dl = DataLoader(dataset=val_dataset, batch_size=124) . Models . With our ImageDataset now passed into a DataLoader class, we have a nice way to iterate through batches of the training data and the validation data. . We now need a way to go from the input data - the batch[&#39;image&#39;] - to predictions that can be compared to the correct label - the batch[&#39;label&#39;]. For this, we need a model, and here we only have 3 constraints: . The model must be able to take in input tensors of the size we have specified (3 x 124 x 124). | Have weights that can be updated during training to lower the output of the loss function, and hopefully make the model better at prediction. | Produce outputs that have the same shape as the labels (10 x 1) and reflect the probabilities of a given image being that label. | With those constraints met the possibilities for a model are endless and are only limited by your compute resource and imagination. Thankfully for image recognition, there are several well known good neural network architectures. . First, though let&#39;s start with a simple feedforward network so we can see what is going on easily. Pytorch again provides torch.nn which has lots of different useful building blocks for creating neural network architecture. This simple feedforward network will use linear layers and Relu to take a flatten tensor of the image ((3*124*124), 1) to a 10 x 1 output. We will have 1 hidden layer. . One way to define a model in Pytorch is to define a class that inherits from nn.Module and call super in our __init__ and then also implement a forward method for your class. The forward method is what we use on every forward pass, unsurprisingly, and is what happens when you use an instantiated model like so model(..). If you know anything about python and dunder methods (double underscored methods) you will recognise this as very similar to implementing a __call__ method. . from torch import nn class SimpleNN(nn.Module): &quot;&quot;&quot;Simple Feed Forward Neural Net&quot;&quot;&quot; def __init__(self, n_inp, n_hidden, n_out): super().__init__() self.n_inp = n_inp self.n_hidden = n_hidden self.n_out = n_out self.in_to_hid = nn.Linear(n_inp, n_hidden) self.relu = nn.ReLU() self.hid_to_out = nn.Linear(n_hidden, n_out) self.softmax = nn.Softmax(dim=1) def forward(self, xb): xb = self.in_to_hid(xb) xb = self.relu(xb) xb = self.hid_to_out(xb) out = self.softmax(xb) return out . This is a really simple neural network. It goes from input to a hidden layer to output layer with these tensors of size: . input = (batch_size, n_inp) | input to hidden layer weight tensor = (n_inp, n_hidden) | hidden to output layer weight tensor = (n_hidden, n_out) | . We are essentially just doing a chain of matrix multiplication where matrix multiplication results in this shape: (m, n) x (n, o) = (m, o) . So with (batch_size, n_inp) x (n_inp, n_hidden) = (batch_size, n_hidden) . And then (batch_size, n_hidden) x (n_hidden, n_out) = (batch_size, n_out) . Right, so let&#39;s see this in action. . simple_net = SimpleNN(n_inp=3*124*124, n_hidden=512, n_out=10) one_batch = batch[&#39;image&#39;] # let&#39;s take just 3 images from this batch one_batch = one_batch[:3, :, :, :] # flatten each image tensor to something of shape=3x124x124 # this is just so we can input this tensor to a feedforward network print(&#39;Original batch size: &#39;, one_batch.shape) one_batch = one_batch.flatten(start_dim=1) print(&#39;Batch size after flatten: &#39;, one_batch.shape) output_of_model = simple_net(one_batch) print(&#39;Output size: &#39;, output_of_model.shape) output_of_model . Original batch size: torch.Size([3, 3, 124, 124]) Batch size after flatten: torch.Size([3, 46128]) Output size: torch.Size([3, 10]) . tensor([[0.1282, 0.1080, 0.1170, 0.0974, 0.0630, 0.1192, 0.0882, 0.0977, 0.0802, 0.1012], [0.1584, 0.0784, 0.1103, 0.1360, 0.0633, 0.0694, 0.0926, 0.0811, 0.1181, 0.0925], [0.0918, 0.0954, 0.1246, 0.0961, 0.0794, 0.0887, 0.0970, 0.1036, 0.1157, 0.1078]], grad_fn=&lt;SoftmaxBackward&gt;) . Models know what their parameters are and set the requires_grad to True by default. Here we should have four tensors that are the parameters of the model - each linear layer has a weight tensor and a bias tensor. It&#39;s important to note looking at the following output that nn.Linear transposes the weight tensor before doing the linear transform. . params = list(simple_net.parameters()) print(&#39;SimpleNN has&#39;, len(params), &#39;tensor parameters&#39;) for i, param in enumerate(params, start=1): print(f&#39;Parameter #{i}&#39;) print(&#39;Parameter is of type&#39;, param.dtype) print(&#39;Parameter has a shape&#39;, param.shape) print(f&#39;Parameter requires_grad={param.requires_grad} n&#39;) . SimpleNN has 4 tensor parameters Parameter #1 Parameter is of type torch.float32 Parameter has a shape torch.Size([512, 46128]) Parameter requires_grad=True Parameter #2 Parameter is of type torch.float32 Parameter has a shape torch.Size([512]) Parameter requires_grad=True Parameter #3 Parameter is of type torch.float32 Parameter has a shape torch.Size([10, 512]) Parameter requires_grad=True Parameter #4 Parameter is of type torch.float32 Parameter has a shape torch.Size([10]) Parameter requires_grad=True . So we have a model that can handle image input data. But this model is a bit boring for a computer vision task, let&#39;s use a small ResNet for this. ResNets are a type of deep computer vision neural net architecture that do very well on image classification tasks. Computer vision architectures differ from standard feedforward networks through their use of convolution layers which are great for image inputs as they are kernels that shift over the input features and we don&#39;t have to flatten the image to a long array, we can keep it its original shape for the input step. To get more of an idea about convolution layers see &quot;A guide to convolution arithmetic for deep learning&quot; . As the pre-trained ResNet that we can get from torchvision.models was trained on Imagenet we will make sure we set pretrained to False. In reality, if you have new data, using a pre-trained model to start with is a good idea - your model already knows how to do one task and won&#39;t take so much to adapt to a new one! . Similarly, the model from torchvision.models final layer creates an output that fits the shape of 1000 classes in Imagenet, but as we are going to learn from Imagenette data we only want output for 10 classes. Let&#39;s use this network and modify the output. . from torchvision.models import resnet18 class ModifiedResNet(nn.Module): def __init__(self, n_out): super().__init__() self.n_out = n_out self.body = nn.Sequential(*list(resnet18(pretrained=False).children())[:-1]) self.flatten = nn.Flatten(start_dim=1) self.fc = nn.Linear(512, n_out) def forward(self, xb): xb = self.body(xb) # need to flatten to get rid of 1x1 from the shape (bs x 512 x 1 x 1) xb = self.flatten(xb) return self.fc(xb) model = ModifiedResNet(n_out=10) model . ModifiedResNet( (body): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (8): AdaptiveAvgPool2d(output_size=(1, 1)) ) (flatten): Flatten(start_dim=1, end_dim=-1) (fc): Linear(in_features=512, out_features=10, bias=True) ) . . Loss . So to reiterate we have a way of batching our data and a model to go from input -&gt; prediction but now we need a way to check these predictions, and using this function to optimise the weights of the models. . Again torch.nn has these loss functions that we need. Here we are classifying with 10 possible labels, so we want a loss function that is lower when the probability output for the given class is high, i.e. the model is confident and correct about the class. And a high loss when the model is confidently wrong. For this we can use nn.CrossEntropyLoss which works for this multi-classification task. . Let&#39;s have look at this with a small 4 &quot;class&quot; example. In the first example here the 3rd label is the correct label and the &quot;model&quot; output has predicted that quite confidently. As you can see the loss is very small for this - there is not much the model can do to better predict this. In the second example, the &quot;model&quot; is confident that the label should be the 2nd label but in reality, it is the 3rd label. The loss is high - the model could stand to make improvements and do better. . output = torch.tensor([[0.5, 0.1, 12, 0.3]]) target = torch.tensor([[0., 0., 1., 0.]]) print(&#39;&quot;Model&quot; output probabilities:&#39;, output.softmax(dim=1)) print(&#39;Actual label:&#39;, target) loss_func = nn.CrossEntropyLoss() loss = loss_func(output, target.argmax(dim=1)) print(&#39;Model is confident and correct, loss=&#39;, loss, &#39; n&#39;) output = torch.tensor([[0.5, 12, 0.1, 0.3]]) target = torch.tensor([[0., 0., 1., 0.]]) print(&#39;&quot;Model&quot; output probabilities:&#39;, output.softmax(dim=1)) print(&#39;Actual label:&#39;, target) loss = loss_func(output, target.argmax(dim=1)) print(&#39;Model is confident and incorrect, loss=&#39;, loss, &#39; n&#39;) . &#34;Model&#34; output probabilities: tensor([[1.0130e-05, 6.7902e-06, 9.9997e-01, 8.2936e-06]]) Actual label: tensor([[0., 0., 1., 0.]]) Model is confident and correct, loss= tensor(2.5272e-05) &#34;Model&#34; output probabilities: tensor([[1.0130e-05, 9.9997e-01, 6.7902e-06, 8.2936e-06]]) Actual label: tensor([[0., 0., 1., 0.]]) Model is confident and incorrect, loss= tensor(11.9000) . For the loss function we provide the output of the model as a tensor which has the size (batch_size x number of classes) and the desired target label for each item in the batch so that they have the shape (batch_size,). We can use .argmax on a tensor to get which is index is the highest value, as we do with the target tensor above in the loss function. . With the output of this function, we can call loss.backward() to compute gradients for any tensor used in the calculation of the output probability, and use those gradients to update those tensors to hopefully lower the loss. . Optimizers and Learning Rate Schedulers . The final part of the puzzle before we can get to training are optimizers from torch.optim. When we come to update the parameters of our model we could call loss.backward() and iterate through our parameters to update according to some fixed learning rate like so: . for batch in train_dl: xb, targets = batch[&#39;image&#39;], batch[&#39;label&#39;] preds = model(xb) loss = loss_func(preds, targets) loss.backward() for param in model.parameters(): param.data = param.data - learning_rate * param.grad param.grad = None . But this is a bit unwieldy. PyTorch has lots of optimizers you can use to update model parameters and has the added benefit that you don&#39;t need to implement things such as momentum, or weight decay (L2 regularisation) as these are already there for you to use. To use an optimizer you can do this: . from torch.optim import Adam model = SomeModel() optimizer = Adam(model.parameters(), lr=0.003) ... for batch in train_dl: xb, targets = xb, targets = batch[&#39;image&#39;], batch[&#39;label&#39;] preds = model(xb) loss = loss_func(preds, targets) loss.backward() optimizer.step() optimizer.zero_grad() . As well as using optimizers it is a good idea to use learning rate schedulers. It is a common technique to use dynamic learning rates that change during training rather than using a flat learning rate throughout. PyTorch provides these through torch.optim.lr_scheduler. You can use these in the following way: . from torch.optim import Adam from torch.optim.lr_scheduler import OneCycleLR ... model = SomeModel() optimizer = Adam(model.parameters(), lr=0.003) scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=10) ... for epoch in range(10): for batch in train_dl: xb, targets = xb, targets = batch[&#39;image&#39;], batch[&#39;label&#39;] preds = model(xb) loss = loss_func(preds, targets) loss.backward() optimizer.step() optimizer.zero_grad() scheduler.step() . The advantage of using a scheduler like OneCycleLR is that we can start with a relatively low learning rate and making small updates to parameters before ramping up to a high learning rate about a third of the way in training allowing us to make bigger updates to the parameters. Finally, we slowly descend back to smaller learning rates and make less drastic updates when the model should have already learnt. This schedule was demonstrated in &quot;Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates&quot; where it was shown that this could lead to faster training. . Let&#39;s examine the schedule a bit more below: . from torch.optim import SGD from torch.optim.lr_scheduler import OneCycleLR opt = SGD(simple_net.parameters(), lr=0.003) sched = OneCycleLR(opt, max_lr=0.1, epochs=3, steps_per_epoch=100) step = [] lr = [] for i in range(3*100): step.append(i) lr.append(sched.get_last_lr()) opt.step() sched.step() f, ax = plt.subplots(1, 1) ax.plot(step, lr) _ = ax.set_ylabel(&#39;Learning Rate&#39;, fontsize=14) _ = ax.set_xlabel(&#39;Step&#39;, fontsize=14) _ = ax.tick_params(&#39;both&#39;, labelsize=14) ymin, ymax = ax.get_ylim() ax.vlines(x=100, ymin=ymin, ymax=ymax, linestyle=&#39;dashed&#39;, color=&#39;black&#39;) ax.vlines(x=200, ymin=ymin, ymax=ymax, linestyle=&#39;dashed&#39;, color=&#39;black&#39;) _ = ax.text(x=101, y=ymin+((ymax - ymin)/2), s=&#39;First Epoch&#39;, rotation=-90, fontsize=12, color=&#39;red&#39;) _ = ax.text(x=201, y=ymin+((ymax - ymin)/2), s=&#39;Second Epoch&#39;, rotation=-90, fontsize=12, color=&#39;red&#39;) . Training . Phew! We have made it to training. So what does PyTorch have up its sleeve to help us train? Well, nothing actually! We are free to implement the training loop however we want with the tools we have already seen. In fact, a training loop is very similar to the loops shown above in the Optimizer section. . Often the logic of the training loop doesn&#39;t change from application to application so it seems overkill to reimplement it each time. Luckily some libraries can help us out by reducing the boilerplate and provide useful utilities: . PyTorch Lightning | fast.ai | . I am a big fan of fast.ai and have found their courses to be really good for getting into and learning deep learning concepts. By using fast.ai this problem can be reduced to around 5 lines of code or so. I have yet to try out PyTorch Lightning, so maybe I will do that for a future post. Here, however, we want to see how we can do this with pure PyTorch. . Let&#39;s first set up a few things by re-initializing the ResNet18 model and . from torch.optim import Adam N_EPOCHS = 20 BASE_LR = 0.005 # if GPU is avaiable we can use .to(DEVICE) to put tensors on the GPU DEVICE = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) model = ModifiedResNet(n_out=10).to(DEVICE) opt = Adam(model.parameters(), lr=BASE_LR) sched = OneCycleLR(opt, max_lr=0.01, steps_per_epoch=len(train_dl), epochs=N_EPOCHS) loss_func = nn.CrossEntropyLoss() . Now let&#39;s write the actual training loop. Here its pretty simple: . We iterate through the batches of the training dataloader | Forward pass of the images through the model | Compare to actual labels to compute the loss | Call loss.backward() to compute gradients, and use those to update the parameter of the models | Once iterated through the whole training dataloader iterate through the validation dataloader and use these batches to calculate loss (not for updating model, just as a metric) and accuracy. | Repeat the process for N epochs. | Several things to note. First is the use of model.train() and model.eval() - this sets certain elements of the model such as batch norm layers to track metrics during training but not during evaluation. The second thing is the use of torch.no_grad() in the validation loop which as the name implies prevents computation of gradients within this context block. Validation is just for computing metrics so gradients aren&#39;t needed during this step. . training_losses = [] validation_losses = [] validation_accuracies = [] for epoch in range(N_EPOCHS): train_loss = 0 model.train() for i, batch in enumerate(train_dl): if i == 0: print(f&#39;Epoch {epoch} - beginning training&#39;) xb, label = batch[&#39;image&#39;].to(DEVICE), batch[&#39;label&#39;].to(DEVICE) preds = model(xb) loss = loss_func(preds, label.argmax(axis=1).flatten()) train_loss += loss.item() loss.backward() opt.step() opt.zero_grad() sched.step() if i % 10 == 0: print(f&#39;Training - Epoch {epoch} - {100*(i/len(train_dl)):.2f}%&#39;) model.eval() val_loss, val_acc = 0, 0 with torch.no_grad(): for i, batch in enumerate(val_dl): if i == 0: print(f&#39;Epoch {epoch} - beginning validation&#39;) xb, label = batch[&#39;image&#39;].to(DEVICE), batch[&#39;label&#39;].to(DEVICE) preds = model(xb) loss = loss_func(preds, label.argmax(axis=1).flatten()) val_loss += loss.item() val_acc += (preds.argmax(1) == label.argmax(axis=1).flatten()).sum().item() if i % 10 == 0: print(f&#39;Validation - Epoch {epoch} - {100*(i/len(val_dl)):.2f}%&#39;) train_loss = train_loss / len(train_dl) training_losses.append(train_loss) val_loss = val_loss / len(val_dl) validation_losses.append(val_loss) val_acc = val_acc / len(val_dl.dataset) validation_accuracies.append(val_acc) print(f&#39;Epoch {epoch}: Train loss={train_loss:.3f} | Val Loss={val_loss:.3f} | Val Acc={val_acc*100:.3f}&#39;) . Epoch 0 - beginning training Training - Epoch 0 - 0.00% Training - Epoch 0 - 6.76% Training - Epoch 0 - 13.51% Training - Epoch 0 - 20.27% Training - Epoch 0 - 27.03% Training - Epoch 0 - 33.78% Training - Epoch 0 - 40.54% Training - Epoch 0 - 47.30% Training - Epoch 0 - 54.05% Training - Epoch 0 - 60.81% Training - Epoch 0 - 67.57% Training - Epoch 0 - 74.32% Training - Epoch 0 - 81.08% Training - Epoch 0 - 87.84% Training - Epoch 0 - 94.59% Epoch 0 - beginning validation Validation - Epoch 0 - 0.00% Validation - Epoch 0 - 31.25% Validation - Epoch 0 - 62.50% Validation - Epoch 0 - 93.75% Epoch 0: Train loss=1.614 | Val Loss=1.781 | Val Acc=47.643 Epoch 1 - beginning training Training - Epoch 1 - 0.00% Training - Epoch 1 - 6.76% Training - Epoch 1 - 13.51% Training - Epoch 1 - 20.27% Training - Epoch 1 - 27.03% Training - Epoch 1 - 33.78% Training - Epoch 1 - 40.54% Training - Epoch 1 - 47.30% Training - Epoch 1 - 54.05% Training - Epoch 1 - 60.81% Training - Epoch 1 - 67.57% Training - Epoch 1 - 74.32% Training - Epoch 1 - 81.08% Training - Epoch 1 - 87.84% Training - Epoch 1 - 94.59% Epoch 1 - beginning validation Validation - Epoch 1 - 0.00% Validation - Epoch 1 - 31.25% Validation - Epoch 1 - 62.50% Validation - Epoch 1 - 93.75% Epoch 1: Train loss=1.393 | Val Loss=1.756 | Val Acc=48.866 Epoch 2 - beginning training Training - Epoch 2 - 0.00% Training - Epoch 2 - 6.76% Training - Epoch 2 - 13.51% Training - Epoch 2 - 20.27% Training - Epoch 2 - 27.03% Training - Epoch 2 - 33.78% Training - Epoch 2 - 40.54% Training - Epoch 2 - 47.30% Training - Epoch 2 - 54.05% Training - Epoch 2 - 60.81% Training - Epoch 2 - 67.57% Training - Epoch 2 - 74.32% Training - Epoch 2 - 81.08% Training - Epoch 2 - 87.84% Training - Epoch 2 - 94.59% Epoch 2 - beginning validation Validation - Epoch 2 - 0.00% Validation - Epoch 2 - 31.25% Validation - Epoch 2 - 62.50% Validation - Epoch 2 - 93.75% Epoch 2: Train loss=1.347 | Val Loss=1.407 | Val Acc=54.318 Epoch 3 - beginning training Training - Epoch 3 - 0.00% Training - Epoch 3 - 6.76% Training - Epoch 3 - 13.51% Training - Epoch 3 - 20.27% Training - Epoch 3 - 27.03% Training - Epoch 3 - 33.78% Training - Epoch 3 - 40.54% Training - Epoch 3 - 47.30% Training - Epoch 3 - 54.05% Training - Epoch 3 - 60.81% Training - Epoch 3 - 67.57% Training - Epoch 3 - 74.32% Training - Epoch 3 - 81.08% Training - Epoch 3 - 87.84% Training - Epoch 3 - 94.59% Epoch 3 - beginning validation Validation - Epoch 3 - 0.00% Validation - Epoch 3 - 31.25% Validation - Epoch 3 - 62.50% Validation - Epoch 3 - 93.75% Epoch 3: Train loss=1.282 | Val Loss=1.821 | Val Acc=43.287 Epoch 4 - beginning training Training - Epoch 4 - 0.00% Training - Epoch 4 - 6.76% Training - Epoch 4 - 13.51% Training - Epoch 4 - 20.27% Training - Epoch 4 - 27.03% Training - Epoch 4 - 33.78% Training - Epoch 4 - 40.54% Training - Epoch 4 - 47.30% Training - Epoch 4 - 54.05% Training - Epoch 4 - 60.81% Training - Epoch 4 - 67.57% Training - Epoch 4 - 74.32% Training - Epoch 4 - 81.08% Training - Epoch 4 - 87.84% Training - Epoch 4 - 94.59% Epoch 4 - beginning validation Validation - Epoch 4 - 0.00% Validation - Epoch 4 - 31.25% Validation - Epoch 4 - 62.50% Validation - Epoch 4 - 93.75% Epoch 4: Train loss=1.158 | Val Loss=1.273 | Val Acc=59.414 Epoch 5 - beginning training Training - Epoch 5 - 0.00% Training - Epoch 5 - 6.76% Training - Epoch 5 - 13.51% Training - Epoch 5 - 20.27% Training - Epoch 5 - 27.03% Training - Epoch 5 - 33.78% Training - Epoch 5 - 40.54% Training - Epoch 5 - 47.30% Training - Epoch 5 - 54.05% Training - Epoch 5 - 60.81% Training - Epoch 5 - 67.57% Training - Epoch 5 - 74.32% Training - Epoch 5 - 81.08% Training - Epoch 5 - 87.84% Training - Epoch 5 - 94.59% Epoch 5 - beginning validation Validation - Epoch 5 - 0.00% Validation - Epoch 5 - 31.25% Validation - Epoch 5 - 62.50% Validation - Epoch 5 - 93.75% Epoch 5: Train loss=1.048 | Val Loss=1.448 | Val Acc=54.599 Epoch 6 - beginning training Training - Epoch 6 - 0.00% Training - Epoch 6 - 6.76% Training - Epoch 6 - 13.51% Training - Epoch 6 - 20.27% Training - Epoch 6 - 27.03% Training - Epoch 6 - 33.78% Training - Epoch 6 - 40.54% Training - Epoch 6 - 47.30% Training - Epoch 6 - 54.05% Training - Epoch 6 - 60.81% Training - Epoch 6 - 67.57% Training - Epoch 6 - 74.32% Training - Epoch 6 - 81.08% Training - Epoch 6 - 87.84% Training - Epoch 6 - 94.59% Epoch 6 - beginning validation Validation - Epoch 6 - 0.00% Validation - Epoch 6 - 31.25% Validation - Epoch 6 - 62.50% Validation - Epoch 6 - 93.75% Epoch 6: Train loss=0.973 | Val Loss=0.996 | Val Acc=67.618 Epoch 7 - beginning training Training - Epoch 7 - 0.00% Training - Epoch 7 - 6.76% Training - Epoch 7 - 13.51% Training - Epoch 7 - 20.27% Training - Epoch 7 - 27.03% Training - Epoch 7 - 33.78% Training - Epoch 7 - 40.54% Training - Epoch 7 - 47.30% Training - Epoch 7 - 54.05% Training - Epoch 7 - 60.81% Training - Epoch 7 - 67.57% Training - Epoch 7 - 74.32% Training - Epoch 7 - 81.08% Training - Epoch 7 - 87.84% Training - Epoch 7 - 94.59% Epoch 7 - beginning validation Validation - Epoch 7 - 0.00% Validation - Epoch 7 - 31.25% Validation - Epoch 7 - 62.50% Validation - Epoch 7 - 93.75% Epoch 7: Train loss=0.902 | Val Loss=0.931 | Val Acc=71.159 Epoch 8 - beginning training Training - Epoch 8 - 0.00% Training - Epoch 8 - 6.76% Training - Epoch 8 - 13.51% Training - Epoch 8 - 20.27% Training - Epoch 8 - 27.03% Training - Epoch 8 - 33.78% Training - Epoch 8 - 40.54% Training - Epoch 8 - 47.30% Training - Epoch 8 - 54.05% Training - Epoch 8 - 60.81% Training - Epoch 8 - 67.57% Training - Epoch 8 - 74.32% Training - Epoch 8 - 81.08% Training - Epoch 8 - 87.84% Training - Epoch 8 - 94.59% Epoch 8 - beginning validation Validation - Epoch 8 - 0.00% Validation - Epoch 8 - 31.25% Validation - Epoch 8 - 62.50% Validation - Epoch 8 - 93.75% Epoch 8: Train loss=0.824 | Val Loss=0.987 | Val Acc=69.427 Epoch 9 - beginning training Training - Epoch 9 - 0.00% Training - Epoch 9 - 6.76% Training - Epoch 9 - 13.51% Training - Epoch 9 - 20.27% Training - Epoch 9 - 27.03% Training - Epoch 9 - 33.78% Training - Epoch 9 - 40.54% Training - Epoch 9 - 47.30% Training - Epoch 9 - 54.05% Training - Epoch 9 - 60.81% Training - Epoch 9 - 67.57% Training - Epoch 9 - 74.32% Training - Epoch 9 - 81.08% Training - Epoch 9 - 87.84% Training - Epoch 9 - 94.59% Epoch 9 - beginning validation Validation - Epoch 9 - 0.00% Validation - Epoch 9 - 31.25% Validation - Epoch 9 - 62.50% Validation - Epoch 9 - 93.75% Epoch 9: Train loss=0.772 | Val Loss=0.807 | Val Acc=74.904 Epoch 10 - beginning training Training - Epoch 10 - 0.00% Training - Epoch 10 - 6.76% Training - Epoch 10 - 13.51% Training - Epoch 10 - 20.27% Training - Epoch 10 - 27.03% Training - Epoch 10 - 33.78% Training - Epoch 10 - 40.54% Training - Epoch 10 - 47.30% Training - Epoch 10 - 54.05% Training - Epoch 10 - 60.81% Training - Epoch 10 - 67.57% Training - Epoch 10 - 74.32% Training - Epoch 10 - 81.08% Training - Epoch 10 - 87.84% Training - Epoch 10 - 94.59% Epoch 10 - beginning validation Validation - Epoch 10 - 0.00% Validation - Epoch 10 - 31.25% Validation - Epoch 10 - 62.50% Validation - Epoch 10 - 93.75% Epoch 10: Train loss=0.714 | Val Loss=0.835 | Val Acc=73.987 Epoch 11 - beginning training Training - Epoch 11 - 0.00% Training - Epoch 11 - 6.76% Training - Epoch 11 - 13.51% Training - Epoch 11 - 20.27% Training - Epoch 11 - 27.03% Training - Epoch 11 - 33.78% Training - Epoch 11 - 40.54% Training - Epoch 11 - 47.30% Training - Epoch 11 - 54.05% Training - Epoch 11 - 60.81% Training - Epoch 11 - 67.57% Training - Epoch 11 - 74.32% Training - Epoch 11 - 81.08% Training - Epoch 11 - 87.84% Training - Epoch 11 - 94.59% Epoch 11 - beginning validation Validation - Epoch 11 - 0.00% Validation - Epoch 11 - 31.25% Validation - Epoch 11 - 62.50% Validation - Epoch 11 - 93.75% Epoch 11: Train loss=0.660 | Val Loss=0.865 | Val Acc=73.376 Epoch 12 - beginning training Training - Epoch 12 - 0.00% Training - Epoch 12 - 6.76% Training - Epoch 12 - 13.51% Training - Epoch 12 - 20.27% Training - Epoch 12 - 27.03% Training - Epoch 12 - 33.78% Training - Epoch 12 - 40.54% Training - Epoch 12 - 47.30% Training - Epoch 12 - 54.05% Training - Epoch 12 - 60.81% Training - Epoch 12 - 67.57% Training - Epoch 12 - 74.32% Training - Epoch 12 - 81.08% Training - Epoch 12 - 87.84% Training - Epoch 12 - 94.59% Epoch 12 - beginning validation Validation - Epoch 12 - 0.00% Validation - Epoch 12 - 31.25% Validation - Epoch 12 - 62.50% Validation - Epoch 12 - 93.75% Epoch 12: Train loss=0.625 | Val Loss=0.701 | Val Acc=77.860 Epoch 13 - beginning training Training - Epoch 13 - 0.00% Training - Epoch 13 - 6.76% Training - Epoch 13 - 13.51% Training - Epoch 13 - 20.27% Training - Epoch 13 - 27.03% Training - Epoch 13 - 33.78% Training - Epoch 13 - 40.54% Training - Epoch 13 - 47.30% Training - Epoch 13 - 54.05% Training - Epoch 13 - 60.81% Training - Epoch 13 - 67.57% Training - Epoch 13 - 74.32% Training - Epoch 13 - 81.08% Training - Epoch 13 - 87.84% Training - Epoch 13 - 94.59% Epoch 13 - beginning validation Validation - Epoch 13 - 0.00% Validation - Epoch 13 - 31.25% Validation - Epoch 13 - 62.50% Validation - Epoch 13 - 93.75% Epoch 13: Train loss=0.560 | Val Loss=0.610 | Val Acc=80.510 Epoch 14 - beginning training Training - Epoch 14 - 0.00% Training - Epoch 14 - 6.76% Training - Epoch 14 - 13.51% Training - Epoch 14 - 20.27% Training - Epoch 14 - 27.03% Training - Epoch 14 - 33.78% Training - Epoch 14 - 40.54% Training - Epoch 14 - 47.30% Training - Epoch 14 - 54.05% Training - Epoch 14 - 60.81% Training - Epoch 14 - 67.57% Training - Epoch 14 - 74.32% Training - Epoch 14 - 81.08% Training - Epoch 14 - 87.84% Training - Epoch 14 - 94.59% Epoch 14 - beginning validation Validation - Epoch 14 - 0.00% Validation - Epoch 14 - 31.25% Validation - Epoch 14 - 62.50% Validation - Epoch 14 - 93.75% Epoch 14: Train loss=0.510 | Val Loss=0.628 | Val Acc=79.847 Epoch 15 - beginning training Training - Epoch 15 - 0.00% Training - Epoch 15 - 6.76% Training - Epoch 15 - 13.51% Training - Epoch 15 - 20.27% Training - Epoch 15 - 27.03% Training - Epoch 15 - 33.78% Training - Epoch 15 - 40.54% Training - Epoch 15 - 47.30% Training - Epoch 15 - 54.05% Training - Epoch 15 - 60.81% Training - Epoch 15 - 67.57% Training - Epoch 15 - 74.32% Training - Epoch 15 - 81.08% Training - Epoch 15 - 87.84% Training - Epoch 15 - 94.59% Epoch 15 - beginning validation Validation - Epoch 15 - 0.00% Validation - Epoch 15 - 31.25% Validation - Epoch 15 - 62.50% Validation - Epoch 15 - 93.75% Epoch 15: Train loss=0.464 | Val Loss=0.571 | Val Acc=81.962 Epoch 16 - beginning training Training - Epoch 16 - 0.00% Training - Epoch 16 - 6.76% Training - Epoch 16 - 13.51% Training - Epoch 16 - 20.27% Training - Epoch 16 - 27.03% Training - Epoch 16 - 33.78% Training - Epoch 16 - 40.54% Training - Epoch 16 - 47.30% Training - Epoch 16 - 54.05% Training - Epoch 16 - 60.81% Training - Epoch 16 - 67.57% Training - Epoch 16 - 74.32% Training - Epoch 16 - 81.08% Training - Epoch 16 - 87.84% Training - Epoch 16 - 94.59% Epoch 16 - beginning validation Validation - Epoch 16 - 0.00% Validation - Epoch 16 - 31.25% Validation - Epoch 16 - 62.50% Validation - Epoch 16 - 93.75% Epoch 16: Train loss=0.413 | Val Loss=0.571 | Val Acc=81.962 Epoch 17 - beginning training Training - Epoch 17 - 0.00% Training - Epoch 17 - 6.76% Training - Epoch 17 - 13.51% Training - Epoch 17 - 20.27% Training - Epoch 17 - 27.03% Training - Epoch 17 - 33.78% Training - Epoch 17 - 40.54% Training - Epoch 17 - 47.30% Training - Epoch 17 - 54.05% Training - Epoch 17 - 60.81% Training - Epoch 17 - 67.57% Training - Epoch 17 - 74.32% Training - Epoch 17 - 81.08% Training - Epoch 17 - 87.84% Training - Epoch 17 - 94.59% Epoch 17 - beginning validation Validation - Epoch 17 - 0.00% Validation - Epoch 17 - 31.25% Validation - Epoch 17 - 62.50% Validation - Epoch 17 - 93.75% Epoch 17: Train loss=0.373 | Val Loss=0.523 | Val Acc=83.669 Epoch 18 - beginning training Training - Epoch 18 - 0.00% Training - Epoch 18 - 6.76% Training - Epoch 18 - 13.51% Training - Epoch 18 - 20.27% Training - Epoch 18 - 27.03% Training - Epoch 18 - 33.78% Training - Epoch 18 - 40.54% Training - Epoch 18 - 47.30% Training - Epoch 18 - 54.05% Training - Epoch 18 - 60.81% Training - Epoch 18 - 67.57% Training - Epoch 18 - 74.32% Training - Epoch 18 - 81.08% Training - Epoch 18 - 87.84% Training - Epoch 18 - 94.59% Epoch 18 - beginning validation Validation - Epoch 18 - 0.00% Validation - Epoch 18 - 31.25% Validation - Epoch 18 - 62.50% Validation - Epoch 18 - 93.75% Epoch 18: Train loss=0.345 | Val Loss=0.524 | Val Acc=83.669 Epoch 19 - beginning training Training - Epoch 19 - 0.00% Training - Epoch 19 - 6.76% Training - Epoch 19 - 13.51% Training - Epoch 19 - 20.27% Training - Epoch 19 - 27.03% Training - Epoch 19 - 33.78% Training - Epoch 19 - 40.54% Training - Epoch 19 - 47.30% Training - Epoch 19 - 54.05% Training - Epoch 19 - 60.81% Training - Epoch 19 - 67.57% Training - Epoch 19 - 74.32% Training - Epoch 19 - 81.08% Training - Epoch 19 - 87.84% Training - Epoch 19 - 94.59% Epoch 19 - beginning validation Validation - Epoch 19 - 0.00% Validation - Epoch 19 - 31.25% Validation - Epoch 19 - 62.50% Validation - Epoch 19 - 93.75% Epoch 19: Train loss=0.324 | Val Loss=0.517 | Val Acc=83.618 . . Below are the training and validation losses, alongside the accuracy - here we can see we are getting to around 83% accuracy in around 20 epochs. We can play around with training for longer, changing the hyperparameters (learning rate, regularisation) or using a bigger model to see if we can improve upon this. It is definitely possible to get to +90% accuracy with 200 epochs from looking at the Imagenette leaderboard. . If you decide to check out the libraries that can help with training, one thing that fast.ai gives us for training is a learning rate finder which helps up to pick a good value for learning rates. . Prediction . We have our model and now it&#39;s been trained so let&#39;s use it for its actual purpose - prediction! . I have grabbed a label conversion dictionary from a fast.ai tutorial for the Imagenette labels so that we can output the actual name of the item rather than the name of the directory. . lbl_dict = dict( n01440764=&#39;tench&#39;, n02102040=&#39;English springer&#39;, n02979186=&#39;cassette player&#39;, n03000684=&#39;chain saw&#39;, n03028079=&#39;church&#39;, n03394916=&#39;French horn&#39;, n03417042=&#39;garbage truck&#39;, n03425413=&#39;gas pump&#39;, n03445777=&#39;golf ball&#39;, n03888257=&#39;parachute&#39; ) . Before prediction, we will set all parameters in the model .requires_grad to False so that gradients aren&#39;t tracked and make a little predict function. For this, we want to pass in an image and apply the same valid transforms from the validation dataloader so that we can take a centre crop and convert it to a tensor. . for p in model.parameters(): p.requires_grad = False def predict(img, model, transforms): model.eval() with torch.no_grad(): # need to add a leading dimension to make a batch of 1 out = model(transforms(img).unsqueeze(0).to(DEVICE)) probs = out.softmax(1) lbl = lbl_dict[unique_labels[probs.argmax().item()]] return lbl, probs.max().item() fname = val_files[2452] img = Image.open(fname).convert(&#39;RGB&#39;) show_image(img) predict(img, model, valid_transforms) . (&#39;garbage truck&#39;, 0.9994460940361023) . fname = val_files[899] img = Image.open(fname).convert(&#39;RGB&#39;) show_image(img) predict(img, model, valid_transforms) . (&#39;English springer&#39;, 0.9999816417694092) . Conclusion . In summary then to use PyTorch we need: . Data, and a way of getting it into tensors with Dataset and DataLoader classes. You should have some training data for learning and validation data for computing metrics such as accuracy. Ideally, you should also reserve some more data as a final test set, which you will only check accuracy against once you have finished - you have selected the best model, best hyperparameter. | A model to take input data and produce output (probabilities for classification, actual number values for regression). The model will have parameters which through learning we will change to make better predictions. | Losses and optimizers to update the model parameters with. Loss should compare the output with known labels, and through gradient descent, we aim to make the output of the loss function lower. Optimizers to help us with weight updating methods during training. | . References . As mentioned at the beginning of this post PyTorch is a well-documented library with lots of good tutorials and examples. This post covers a lot of the basics that are also covered in the &quot;Getting Started&quot; tutorial of the official docs and is a good place to also check out for a similar approach. . I also found helpful: . WRITING CUSTOM DATASETS, DATALOADERS AND TRANSFORMS | Fast.ai docs about Imagenette | .",
            "url": "https://jc639.github.io/blog/pytorch/deep-learning/neural-nets/2021/04/02/Pytorch_refresher.html",
            "relUrl": "/pytorch/deep-learning/neural-nets/2021/04/02/Pytorch_refresher.html",
            "date": " • Apr 2, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Github Actions are Awesome!",
            "content": "Github Actions are a way to automate your workflow so you can build continuous-integration/continuous development (CI/CD) workflows. You can run tests, check your library or package can build, generate and commit new files all on various triggers such as pushes, pull requests or even schedules. It&#39;s really awesome for automating lots of different tasks and with scripts the possibilities are endless. It&#39;s even how this website gets built. . Fastpages . This website is built with the amazing fastpages, which alongside nbdev is great for a gentle introduction to Github Actions. . Fastpages allows me to write this post in a Jupyter notebook, and then when I push these notebooks to the blog repository an action takes place that converts them to markdown files and builds the Jekyll site that you are looking at now. . I think this is really neat but I thought it would also be cool to use this workflow to generate an ever-changing header for the front page that is based on the blog posts that have been published recently. . . An action workflow . Fastpages comes with a CI YAML file that looks something like this. I will break this down bit by bit to explain and then show how I modified it so that a new header is generated both when pushing new notebooks and on a schedule: . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: - name: Check if secret exists if: github.event_name == &#39;push&#39; run: | if [ -z &quot;$deploy_key&quot; ] then echo &quot;You do not have a secret named SSH_DEPLOY_KEY. This means you did not follow the setup instructions carefully. Please try setting up your repo again with the right secrets.&quot; exit 1; fi env: deploy_key: $ - name: Copy Repository Contents uses: actions/checkout@main with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;jekyll build -V --strict_front_matter --trace&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site . The first bit sets up when the Action file gets run. The following gives the Action a name and specifies it should run on a push to master branch, or pull requests. The workflow_dispatch: allows manual triggering of the workflow from the actions tab or REST API on Github. . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: . Next, we set up the job and what it runs on - here it is the latest ubuntu image. What this means is that each time this runs we get a self-contained environment in which to carry out the steps of the workflow. You can see below that we can sprinkle our action files with conditional if: ... statements allowing control flow in our actions. . jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest . Now it&#39;s time to define the actual steps that we want to execute. The key part here is that we give the step a name and then either have a run: or uses: key for the given step. If we have a run: key we can execute statements as if we were in the bash terminal (this being a ubuntu image) such as cd .. or ls etc. . The use: allows us to use specific actions created by others - of which there are many on the GitHub Actions marketplace. You can pass arguments by using the with: key to these prespecified actions. . The first step below just checks to see if we have set up a deploy key, something which should be done when we first set up a blog with fastpages: . - name: Check if secret exists if: github.event_name == &#39;push&#39; run: | if [ -z &quot;$deploy_key&quot; ] then echo &quot;You do not have a secret named SSH_DEPLOY_KEY. This means you did not follow the setup instructions carefully. Please try setting up your repo again with the right secrets.&quot; exit 1; fi env: deploy_key: $ . Next, it uses a marketplace action to checkout our blog repository and then uses a local action directory to convert our notebooks to markdown files. This conversion puts the markdown in the _posts/ directory of the blog repository. . - name: Copy Repository Contents uses: actions/checkout@main with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files . The final few steps build and deploy the Jekyll blog. First, the _site/ directory is cleared and then rebuilt using a Docker container image. The CNAME step is only pertinent if you have a custom domain name, which this blog doesn&#39;t so can be ignored. . Finally the last action peaceiris/actions-gh-pages@v3 takes the fresh _site/ and deploys this to a Github Page, which allows hosting of static sites. Easy right? . - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;jekyll build -V --strict_front_matter --trace&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : - name: Deploy if: github.event_name == &#39;push&#39; uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site . How to make the custom header image . To make the custom header image I wrote a little script of which the main execution function is shown below in the collapsable code fold. It&#39;s not too important to understand it for this explanation, just that it is simple python script that produces a matplotlib plot. The main thing is that it reads the markdown posts in the _posts/ folder to get the titles and dates published, and then it can construct the plot with a list of the titles and time since today&#39;s date expressed in days. . As blog posts only ever get put in the _posts/ directory at site build time and the resulting markdown is not committed to the repository I realised I can slot this script into the current ci.yaml workflow. . def line_plot(post_titles: list, deltas: list, cutoff=-30): &quot;&quot;&quot;Creates the line plot in the XKCD style. Args: post_titles (list): list of post titles deltas (list): list of time since comparison time cutoff (int, optional): cutoff point. Defaults to -30. Returns: tuple : plt.fig, plt.ax &quot;&quot;&quot; with plt.xkcd(): f, ax = plt.subplots(1, 1) x_vals = [i for i in range(cutoff, 0, 1)] y_counts = Counter(deltas) y_vals = [y_counts[x] for x in x_vals] ax.plot(x_vals, y_vals) ax.spines[&#39;top&#39;].set_visible(False) ax.spines[&#39;right&#39;].set_visible(False) ax.set_title(&#39;WELCOME TO THE BLOG!&#39;, fontweight=&#39;bold&#39;, y=1.05) max_y_val = max(y_vals) ax.set_ylim(top=max_y_val+0.3+0.2*len(y_counts)) ax.set_yticks(range(0, max_y_val+1)) ax.set_xlabel(&#39;Days ago...&#39;) ax.set_ylabel(&#39;Number of posts&#39;) arrowprops = dict( arrowstyle=&quot;-&gt;&quot;, connectionstyle=&quot;angle3,angleA=0,angleB=90&quot;) titles_arr = np.array(post_titles) deltas_arr = np.array(deltas) x_offset = 2 y_offset = max_y_val + 0.2*len(y_counts) + 0.2 for x, count in y_counts.items(): post_titles = titles_arr[deltas_arr == x] ax.annotate(&#39; n+ n&#39;.join(post_titles), xy=(x, count), xytext=(x+x_offset, y_offset), arrowprops=arrowprops) y_offset -= 0.2 ax.text(x=1.2, y=0.6, s=&#39;Days since posting...&#39;, transform=ax.transAxes) bbox_props = dict(boxstyle=&quot;round&quot;, fc=&quot;white&quot;, ec=&quot;black&quot;) if len(deltas) &gt; 0: last_post_days = str(abs(deltas[-1]) - 1) else: last_post_days = &#39;+&#39; + str(abs(cutoff)) ax.text(1.3, 0.4, last_post_days, bbox=bbox_props, transform=ax.transAxes, fontsize=24) exclam = &#39;&quot;Nice!&quot;&#39; if int(last_post_days) &lt; 14 else &#39;&quot;UH OH!&quot;&#39; ax.text(1.28, 0.1, exclam, transform=ax.transAxes, rotation=25) f.set_size_inches(12, 2.5) return f, ax . . Here are the changes I made to make that work. . Firstly I had to add another trigger to the action. It&#39;s fine that it runs on push as I want new blog posts to be added to the header as they are published. But I also want it to update every day so that the time since publishing updates daily for each post. Handily, GitHub actions have a schedule trigger where we can set a schedule for the action to occur using Cron expressions. . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: schedule: - cron: &quot;0 1 * * *&quot; . This Cron expression means the action will happen every day at 1 am. Check this handy website if you need to write Cron expression https://crontab.guru/. . Next, I slot the script and associated setup in between - name: convert notebooks and word docs to posts and the - name: setup directories for Jekyll build steps. . To use the script we need to have python and the required libraries installed. Luckily this is again quite easy using the actions/setup-python action followed by a run: pip install -r requirements.txt step. These are the - name: setup python and - name: Install dependencies steps, respectively. . The next step installs the humor sans font that is required by plt.xkcd and then finally we run the script in the - name: make-header step. . - name: convert notebooks and word docs to posts uses: ./_action_files - name: setup python uses: actions/setup-python@v2 with: python-version: 3.7 - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Install humor sans font run: | sudo apt-get update -y sudo apt-get install -y fonts-humor-sans rm -rf ~/.cache/matplotlib - name: make-header run: | python scripts/make_header.py - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . . The very last thing we have to change is the conditional if: in the final - name: Deploy step to also run this when the github.event_name is equal to &#39;schedule&#39;. Viola! . - name: Deploy if: ( github.event_name == &#39;push&#39; ) || ( github.event_name == &#39;schedule&#39; ) uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site . Here is the new ci.yaml workflow file in full: . name: CI on: push: branches: - master # need to filter here so we only deploy when there is a push to master # no filters on pull requests, so intentionally left blank pull_request: workflow_dispatch: schedule: - cron: &quot;0 1 * * *&quot; jobs: build-site: if: ( github.event.commits[0].message != &#39;Initial commit&#39; ) || github.run_number &gt; 1 runs-on: ubuntu-latest steps: - name: Check if secret exists if: ( github.event_name == &#39;push&#39; ) || ( github.event_name == &#39;schedule&#39; ) run: | if [ -z &quot;$deploy_key&quot; ] then echo &quot;You do not have a secret named SSH_DEPLOY_KEY. This means you did not follow the setup instructions carefully. Please try setting up your repo again with the right secrets.&quot; exit 1; fi env: deploy_key: $ - name: Copy Repository Contents uses: actions/checkout@main with: persist-credentials: false - name: convert notebooks and word docs to posts uses: ./_action_files - name: setup python uses: actions/setup-python@v2 with: python-version: 3.7 - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements.txt - name: Install humor sans font run: | sudo apt-get update -y sudo apt-get install -y fonts-humor-sans rm -rf ~/.cache/matplotlib - name: make-header run: | python scripts/make_header.py - name: setup directories for Jekyll build run: | rm -rf _site sudo chmod -R 777 . - name: Jekyll build uses: docker://fastai/fastpages-jekyll with: args: bash -c &quot;jekyll build -V --strict_front_matter --trace&quot; env: JEKYLL_ENV: &#39;production&#39; - name: copy CNAME file into _site if CNAME exists run: | sudo chmod -R 777 _site/ cp CNAME _site/ 2&gt;/dev/null || : - name: Deploy if: ( github.event_name == &#39;push&#39; ) || ( github.event_name == &#39;schedule&#39; ) uses: peaceiris/actions-gh-pages@v3 with: deploy_key: $ publish_dir: ./_site .",
            "url": "https://jc639.github.io/blog/github/ci/actions/2021/03/06/github_actions.html",
            "relUrl": "/github/ci/actions/2021/03/06/github_actions.html",
            "date": " • Mar 6, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Hello",
            "content": "Testing 1, 2, 3... Is anyone out there? Hello to anyone reading this and welcome to this blog. I intend to write here semi-regularly covering various topics in the data science space, as well as deep learning. . I am a data scientist at an FTSE 100 company, and do lots of various things covering a wide range of data science techniques but not so much deep learning. . I really enjoy deep learning and as you can see in the past I have done some projects in the past using deep learning techniques, such as: . handwriting recognition | character RNN | album cover genre image recognition | . But I want to do more stuff regularly and plan to use this blog to do that. The writing here will be a consolidation of any learning and is mainly for me, but if anyone else finds it useful that is great! .",
            "url": "https://jc639.github.io/blog/2021/02/27/Hello.html",
            "relUrl": "/2021/02/27/Hello.html",
            "date": " • Feb 27, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am a data scientist. This blog represents my personal opinions and musings on data science. . I decided to start blogging after reading some of the fastai deep learning book, where they suggest writing can help you solidify your learnings. Two reasons given for starting a blog stood out to me: . “It’s like a resume but better!” | “Helps you learn” | . I have a strong interest in deep learning and I want to use this blog to help with learning but also document those skills that might not be evident from my CV. If someone finds something useful that I have written as part of that, then that’s great! .",
          "url": "https://jc639.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jc639.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}